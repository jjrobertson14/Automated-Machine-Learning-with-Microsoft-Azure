{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMIT MSG\n",
    "# \n",
    "# [Forecasting] - First forecasting notebook working\n",
    "# TODO! pin versions of new pip dependencies of OJ Sales Forecasting notebook\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Datastore, Environment\n",
    "from azureml.core import Experiment\n",
    "import azureml.interpret\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Forecasting Specific\n",
    "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Workspace object from Azure\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "# You can find tenant id under azure active directory->properties\n",
    "tenant_id = '198c7d8c-e010-45ce-a018-ec2d9a33f58f'\n",
    "\n",
    "# Authenticate and get Workspace object\n",
    "ia = InteractiveLoginAuthentication(tenant_id=tenant_id)\n",
    "ws_name = 'automlbook'\n",
    "subscription_id = '4d278f3d-b4fd-4fa2-86b6-d34b96bc888f'\n",
    "resource_group = 'Foxy_Resources'\n",
    "ws = Workspace.get(name=ws_name,\n",
    "                   subscription_id=subscription_id,\n",
    "                   resource_group=resource_group,\n",
    "                   auth=ia)\n",
    "print(\"After authenticating to the workspace with Interactive Authentication:\", \n",
    "        \"ws.name: \" + ws.name, \n",
    "        \"ws.resource_group: \" + ws.resource_group, \n",
    "        \"ws.location: \" + ws.location, \n",
    "        \"ws.subscription_id: \" + ws.subscription_id, \n",
    "sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Commmented example code to Auth with Service Principal)\n",
    "# (Purely to Validate this will work inside script...) \n",
    "\n",
    "# TODO? maybe? I could build a custom image with BuildKit, with \"Secret mount type\", and then have Azure get it out of Docker Repository\n",
    "# Store authentication strings\n",
    "# Store username and password to Service Principal in order to authenticate within the python script\n",
    "# CRITICAL: You must have a file like this at resources/custom_env_vars_for_script_inside_docker_container (from project root)...\n",
    "#       # Set AzureML Service Principle ID and Password\n",
    "#       AML_PRINCIPAL_ID=\"<Principal ID, AKA clientId>\"\n",
    "#       AML_PRINCIPAL_PASS=\"<Principal Password, AKA clientSecret>\"\n",
    "\n",
    "# Authenticate with the Service Principal in order to get the Workspace object\n",
    "# from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "# sp = ServicePrincipalAuthentication(tenant_id=tenant_id,\n",
    "#                                     service_principal_id=kv.get_secret(name=\"localDockerAmlPrincipalId\"), # clientId of service principal\n",
    "#                                     service_principal_password=kv.get_secret(name=\"localDockerAmlPrincipalPass\")) # clientSecret of service principal\n",
    "# ws = Workspace.get(name=ws_name,\n",
    "#                    subscription_id=subscription_id,\n",
    "#                    resource_group=resource_group,\n",
    "#                    auth=sp)\n",
    "# print(\"After re-authenticating to the workspace with Service Principal\", \n",
    "#         \"ws.name: \" + ws.name, \n",
    "#         \"ws.resource_group: \" + ws.resource_group, \n",
    "#         \"ws.location: \" + ws.location, \n",
    "#         \"ws.subscription_id: \" + ws.subscription_id, \n",
    "# sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Print statements to list available environments)\n",
    "# envs = Environment.list(workspace=ws)\n",
    "\n",
    "# for env in envs:\n",
    "    # if env.startswith(\"AzureML\"):\n",
    "        # print(\"Name\",env)\n",
    "        # if None != envs[env].python.conda_dependencies:\n",
    "            # print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datastore, try getting datastore via Workspace object\n",
    "datastore = Datastore.get_default(ws)\n",
    "datastore_name = 'workspaceblobstore'\n",
    "datastore = Datastore.get(ws, datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Only Uncomment and Run Once to register the Tabular OJSales Data) \n",
    "# Download Data Files to convert to a Tabular Dataframe we can more easily use\n",
    "\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# \n",
    "# Open Dataset containing our Orange Juice sales data (requires pip install azureml-opendatasets)\n",
    "# from azureml.opendatasets import OjSalesSimulated\n",
    "\n",
    "# Create a dataset from the datastore of the Workspace\n",
    "# oj_sales_files = OjSalesSimulated.get_file_dataset() \n",
    "# oj_sales = oj_sales_files.take(10) \n",
    "\n",
    "# IMPORTANT TIP! \n",
    "#  There are over 4,000 files in the OJ Sales Simulated Data Azure Open Dataset. \n",
    "#  Pulling all of them can result in extended training times.\n",
    "\n",
    "# Create folder to hold OJ_Sales files\n",
    "#\n",
    "# In order to use file datasets, you first need to download them to your local Jupyter environment. \n",
    "# Then, you can read them in as a pandas dataframe by concatenating the files. \n",
    "# Download the 10 files to your newly created OJ_Sales folder with the following code: \n",
    "\n",
    "# folder_name = \"OJ_Sales\" \n",
    "# os.makedirs(folder_name, exist_ok=True)\n",
    "# oj_sales.download(folder_name, overwrite=True)\n",
    "\n",
    "# OJ_file_path = Path(folder_name).rglob('*.csv') \n",
    "# OJ_files = [x for x in OJ_file_path] \n",
    "# df = pd.concat((pd.read_csv(f) for f in OJ_files))\n",
    "\n",
    "# print(df.head(20))\n",
    "\n",
    "# Dataset.Tabular.register_pandas_dataframe(df, datastore, \"automlbook OJSales Forecassting Training A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the datastore of the Workspace\n",
    "dataset_name = 'automlbook Iris Multiclass Training A'\n",
    "# dataset = Dataset.get_by_name(ws, dataset_name)\n",
    "dataset = Dataset.get_by_name(ws, dataset_name, version = 'latest')\n",
    "dataset_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "target_column_name = 'species'\n",
    "\n",
    "# Show a sample of the data in the dataset\n",
    "print(dataset.take(10).to_pandas_dataframe())\n",
    "\n",
    "# Turn Dataset into Pandas Dataframe, it is to be preprocessed\n",
    "df = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify number of classes of target column\n",
    "num_classes = df[target_column_name].nunique()\n",
    "\n",
    "expected_classes = 3\n",
    "if (num_classes != expected_classes):\n",
    "    raise Exception(target_column_name + \" target field expected to have this many classes: \", expected_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a weight_column to have AutoML give different weight to missed predictions of certain rows of data.\n",
    "#     - A common use is to weigh missed classifications differently (based on class)\n",
    "weight_column_name = 'weight_column'\n",
    "df[weight_column_name] = np.where(True, 1, 0)\n",
    "df[weight_column_name] = np.where(df[target_column_name] == 'Iris-setosa', 1, df[weight_column_name])\n",
    "df[weight_column_name] = np.where(df[target_column_name] == 'Iris-versicolor', 100, df[weight_column_name])\n",
    "df[weight_column_name] = np.where(df[target_column_name] == 'Iris-virginica', 100, df[weight_column_name])\n",
    "#\n",
    "# EXAMPLE: Assigning weight (to incorrectly predicting) of 1 to survivors and 0.1 to victims\n",
    "#     - (so a missed prediction of a person surviving has weight 1)\n",
    "#     - (and so a missed prediction of a person not surviving has weight 0.1)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (COMMENTED OUT) Bin an age column into four different age groups. (Under 15, 15-35, 35-60, and over 60)\n",
    "# df['AgeUnder15'] = np.where(df.Age < 15, 1, 0) \n",
    "# df['Age15to34'] = np.where((df.Age > 14) & (df.Age < 35), 1, 0) \n",
    "# df['Age35to60'] = np.where((df.Age > 34) & (df.Age < 61), 1, 0) \n",
    "# df['AgeOver60'] = np.where(df.Age > 60, 1, 0)\n",
    "\n",
    "# df = df.drop(['Age'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Dataframe to get one for Numeric columns and one for Categorical columns\n",
    "df_column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "##### BEGIN Create dataframe with numeric columns so that it contains all numbers that are preprocessed...\n",
    "df_numeric_column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "# Copy df, keeping only float columns\n",
    "df_float_column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "df_float_columns = []\n",
    "if (len(df_float_column_names) != 0):\n",
    "    df_float_columns = pd.DataFrame(df[df_float_column_names], dtype=np.float, columns=df_float_column_names)\n",
    "\n",
    "# Copy df, keeping only int columns\n",
    "df_int_column_names = []\n",
    "df_int_columns = []\n",
    "if (len(df_int_column_names) != 0):\n",
    "    df_int_columns = pd.DataFrame(df[df_int_column_names], columns=df_int_column_names)\n",
    "\n",
    "# Concatenate the numeric DataFrames\n",
    "if (len(df_int_columns) > 0 and len(df_float_columns) > 0):\n",
    "    df_numeric_columns = pd.concat([df_int_columns, df_float_columns], axis=1)\n",
    "elif (len(df_int_columns) > 0):\n",
    "    df_numeric_columns = df_int_columns\n",
    "elif (len(df_float_columns) > 0):\n",
    "    df_numeric_columns = df_float_columns\n",
    "    \n",
    "print('concatenated df_numeric_columns: ', df_numeric_columns)\n",
    "\n",
    "##### BEGIN Create dataframe with categorical columns so that it contains all categorical data that is preprocessed...\n",
    "df_categorical_column_names = []\n",
    "print(\"df_categorical_column_names.count is \" + str(len(df_categorical_column_names)))\n",
    "df_categorical_columns = []\n",
    "# Copy df, keeping only categorical columns\n",
    "if (len(df_categorical_column_names) != 0):\n",
    "    df_categorical_columns = pd.DataFrame(df[df_categorical_column_names], dtype=np.str, columns=df_categorical_column_names)\n",
    "    print('df_categorical_columns: ', df_categorical_columns)\n",
    "\n",
    "feature_column_names = [*df_numeric_column_names, *df_categorical_column_names]\n",
    "print(\"feature_column_names: \", feature_column_names)\n",
    "\n",
    "if (len(feature_column_names) == 0 or (len(df_categorical_columns) == 0 and len(df_numeric_columns) == 0)):\n",
    "    raise Exception(\"There either are no feature columns names, or no feature columns found by those names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the numeric DF with the categorical DF\n",
    "# print(\"df[target_column_name] is \", df[target_column_name])\n",
    "# print(\"df_numeric_columns is \", df_numeric_columns)\n",
    "# print(\"df_numeric_columns.columns is \", df_numeric_columns.columns)\n",
    "# print(\"df_categorical_columns is \", df_categorical_columns)\n",
    "# print(\"df_categorical_columns.columns is \", df_categorical_columns.columns)\n",
    "\n",
    "# Concatenate dfs to get DataFrame of all columns to submit to the training script\n",
    "if (len(df_numeric_columns) > 0 and len(df_categorical_columns) > 0):\n",
    "    dfs = [df[target_column_name], df[weight_column_name], df_numeric_columns, df_categorical_columns]\n",
    "elif (len(df_numeric_columns) > 0):\n",
    "    dfs = [df[target_column_name], df[weight_column_name], df_numeric_columns]\n",
    "elif (len(df_categorical_columns) > 0):\n",
    "    dfs = [df[target_column_name], df[weight_column_name], df_categorical_columns]\n",
    "else:\n",
    "    raise Exception(\"There are no feature columns available in the lists of them provided.\")\n",
    "# print(\"dfs is\" + str(dfs))\n",
    "# print('Before concatenation to dfTyped, df[\\'target_column_name\\']: ', df[target_column_name])\n",
    "# print('Before concatenation to dfTyped, df_numeric_columns: ', df_numeric_columns)\n",
    "dfTyped = pd.concat(dfs, axis=1)\n",
    "print('dfTyped: ', dfTyped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pre-transformation Data Frame into feature/target columns\n",
    "# df_weight_column = pd.DataFrame(dfTyped[weight_column_name], columns=[weight_column_name])\n",
    "df_weight_column = dfTyped[weight_column_name].to_frame()\n",
    "df_x = dfTyped.drop([target_column_name, weight_column_name], axis=1)\n",
    "df_y = dfTyped[target_column_name].to_frame()\n",
    "# print(\"See df_x\", df_x)\n",
    "print(\"See df_y\", df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor to preprocess numeric and categorical columns (with Transfomer API via ColumnTransformer)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', one_hot_encoder)])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, df_numeric_column_names),\n",
    "        ('cat', categorical_transformer, df_categorical_column_names)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Validate the Pipeline will function once passed to the script)\n",
    "\n",
    "# Get the preprocessed Data Frame columns in a list\n",
    "if (len(df_categorical_columns) != 0): # There are categorical columns\n",
    "    one_hot_encoder.fit(df_categorical_columns)\n",
    "    # Get new One Hot Encoded column names\n",
    "    encoded_categorical_column_names = one_hot_encoder.get_feature_names(df_categorical_column_names)\n",
    "    encoded_feature_names = [*df_numeric_column_names, *encoded_categorical_column_names]\n",
    "else: # There are no categorical columns\n",
    "    encoded_categorical_column_names = []\n",
    "    encoded_feature_names = [*df_numeric_column_names]\n",
    "\n",
    "\n",
    "print(str(encoded_feature_names))\n",
    "\n",
    "preprocessor.fit(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessor to the feature columns (then concat the weight_column Dataframe to it)\n",
    "df_x = preprocessor.transform(df_x)\n",
    "df_x = pd.DataFrame(df_x, columns=encoded_feature_names)\n",
    "df_x = pd.concat([df_x, df_weight_column], axis=1)\n",
    "print('df_x right before registration, with weight_column concatenated.\\n', df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Pandas Dataframe of base df_x and df_y\n",
    "Dataset.Tabular.register_pandas_dataframe(df_x, datastore, \"Iris MultiClass Feature Column Data for train_test_split usage (Docker Environment)\")\n",
    "Dataset.Tabular.register_pandas_dataframe(df_y, datastore, \"Iris MultiClass Target Column Data for train_test_split usage (Docker Environment)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test data, register the resulting Datasets with Azure\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "# What you need to pass to train_test_split...\n",
    "# ... I need X and Y dataframe, X just with target missing, Y just with target column present\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "print(test_data)\n",
    "\n",
    "# print(\"See y_test\", y_test)\n",
    "# print(\"See y_test.columns.tolist()\", str(y_test.columns.tolist()))\n",
    "# print(\"See y_test.values.tolist() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.tolist())\n",
    "# print(\"See y_test.values.ravel() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.ravel())\n",
    "# print(\"See y_test.values.tolist().flatten() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.flatten())\n",
    "\n",
    "# Register the split (by whether target column) training and test datasets\n",
    "X_train_registered_name = \"Iris MultiClass Feature Column Data for training (Docker Environment)\"\n",
    "X_test_registered_name = \"Iris MultiClass Feature Column Data for testing (Docker Environment)\"\n",
    "y_train_registered_name = \"Iris MultiClass Target Column Data for training (Docker Environment)\"\n",
    "y_test_registered_name = \"Iris MultiClass Target Column Data for testing (Docker Environment)\"\n",
    "# Dataset.Tabular.register_pandas_dataframe(X_train, datastore, X_train_registered_name)\n",
    "Dataset.Tabular.register_pandas_dataframe(X_test, datastore, X_test_registered_name)\n",
    "# Dataset.Tabular.register_pandas_dataframe(y_train, datastore, y_train_registered_name)\n",
    "Dataset.Tabular.register_pandas_dataframe(y_test, datastore, y_test_registered_name)\n",
    "\n",
    "# Register the combined (feature and target columns) training and test datasets\n",
    "train_data_registered_name = \"Iris MultiClass Training Data (Docker Environment)\"\n",
    "test_data_registered_name = \"Iris MultiClass Training Test Data (Docker Environment)\"\n",
    "Dataset.Tabular.register_pandas_dataframe(train_data, datastore, train_data_registered_name)\n",
    "Dataset.Tabular.register_pandas_dataframe(test_data, datastore, test_data_registered_name)\n",
    "\n",
    "trainTestDataSetNames = [X_train_registered_name, X_test_registered_name, y_train_registered_name, y_test_registered_name, train_data_registered_name, test_data_registered_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names and create TabularExplainer with them\n",
    "features=[*df_numeric_column_names, *encoded_categorical_column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Experiment script arguments list into a string like '[\"a\",\"b\"]'\n",
    "\n",
    "# Encode numeric column names list\n",
    "temp_column_names = df_numeric_column_names.copy()\n",
    "for x in range(len(temp_column_names)):\n",
    "        temp_column_names[x] = '\"{}\"'.format(temp_column_names[x])\n",
    "numericFeatureNamesEncoded = \"[{}]\".format(\",\".join(temp_column_names))\n",
    "# print(\"numericFeatureNamesEncoded:\", numericFeatureNamesEncoded)\n",
    "\n",
    "# Encode categoric column names list\n",
    "temp_column_names = encoded_categorical_column_names.copy()\n",
    "for x in range(len(temp_column_names)):\n",
    "        temp_column_names[x] = '\"{}\"'.format(temp_column_names[x])\n",
    "categoricFeatureNamesEncoded = \"[{}]\".format(\",\".join(temp_column_names))\n",
    "# print(\"categoricFeatureNamesEncoded:\", categoricFeatureNamesEncoded)\n",
    "\n",
    "# Encode split dataset names list\n",
    "for x in range(len(trainTestDataSetNames)):\n",
    "        trainTestDataSetNames[x] = '\"{}\"'.format(trainTestDataSetNames[x])\n",
    "trainTestDataSetNamesEncoded = \"[{}]\".format(\",\".join(trainTestDataSetNames))\n",
    "print(\"splitDatasetNamesEncoded:\", trainTestDataSetNamesEncoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Dockerfile, Set Local Docker Environment up (with System Managed Dependencies, via Conda)\n",
    "#\n",
    "# Learn about Environment and how to use a Docker Environment here:\n",
    "#       https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments\n",
    "#       https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment(class)?view=azure-ml-py\n",
    "#       ! IMPORTANT: https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/environment/\n",
    "#\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "\n",
    "docker_env = Environment(\"docker-env\")\n",
    "# Editing a run configuration property on-fly.\n",
    "docker_env.python.user_managed_dependencies = False\n",
    "# Use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param.\n",
    "#           https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfig.dockerconfiguration?view=azure-ml-py\n",
    "docker_config = DockerConfiguration(use_docker=True)\n",
    "print(\"initial base image from base docker-env Environment: \", docker_env.docker.base_image)\n",
    "\n",
    "# Specify docker steps as a string. \n",
    "dockerfile = r\"\"\"\n",
    "FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220314.v1\n",
    "\n",
    "ARG DEBIAN_FRONTEND=noninteractive\n",
    "ARG ACCEPT_EULA=Y\n",
    "RUN ls -altr\n",
    "RUN ls -altr .dockerenv\n",
    "RUN ls -altr dev\n",
    "RUN ls -altr home\n",
    "RUN ls -altr run\n",
    "RUN apt-get update -y && apt-get upgrade -y &&\\\n",
    "    apt-get install -y build-essential \\\n",
    "                       cmake \\\n",
    "                       curl \\\n",
    "                       gfortran \\\n",
    "                       git \\\n",
    "                       jupyter \\\n",
    "                       libatlas-base-dev \\\n",
    "                       libblas-dev \\\n",
    "                       libbz2-dev \\\n",
    "                       libffi-dev \\\n",
    "                       libgdbm-dev \\\n",
    "                       liblapack-dev \\\n",
    "                       liblzma-dev \\\n",
    "                       libncurses5-dev \\\n",
    "                       libncursesw5-dev \\\n",
    "                       libreadline-dev \\\n",
    "                       libsqlite3-dev \\\n",
    "                       libssl-dev \\\n",
    "                       libxml2-dev \\\n",
    "                       libxmlsec1-dev \\\n",
    "                       llvm \\\n",
    "                       lzma \\\n",
    "                       lzma-dev \\\n",
    "                       make \\\n",
    "                       tcl-dev \\\n",
    "                       tk-dev \\\n",
    "                       wget \\\n",
    "                       xz-utils \\\n",
    "                       zlib1g-dev\n",
    "\n",
    "RUN conda -V\n",
    "RUN echo \"Hello from custom container!\" > ~/hello.txt\n",
    "# RUN export PIP_LOG=\"/tmp/pip_log.txt\" && touch ${PIP_LOG} && tail -f ${PIP_LOG} & conda env create -f \"conda.yml\" && killall tail && rm ${PIP_LOG}\n",
    "\"\"\"\n",
    "\n",
    "# NOTE: you can pass Dockerfile string to docker build command via stdin like this:\n",
    "#\n",
    "# sudo docker build -t myimage:latest -<<EOF\n",
    "# FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220314.v1\n",
    "# RUN echo \"hello world\"\n",
    "# EOF\n",
    "\n",
    "\n",
    "# Set base image to None, because the image is defined by dockerfile.\n",
    "docker_env.docker.base_image = None\n",
    "# Use the Dockerfile string and build image based on it with this code from above (move it down here)\n",
    "docker_env.docker.base_dockerfile = dockerfile\n",
    "\n",
    "#       For help, try reading this\n",
    "#                    - https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/environment/#custom-docker-image--dockerfile\n",
    "#       May also try using other base Docker images from these container registries\n",
    "#                - https://github.com/microsoft/containerregistry\n",
    "#                - https://github.com/Azure/AzureML-Containers\n",
    "# env.docker.base_image = '<image-name>'\n",
    "# env.docker.base_image_registry.address = '<container-registry-address>'\n",
    "# env.docker.base_image_registry.username = '<acr-username>'\n",
    "# env.docker.base_image_registry.password = os.environ.get(\"CONTAINER_PASSWORD\")\n",
    "#\n",
    "# TODO? use this link to get username and password from Azure KeyVault:\n",
    "#           https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/environment/#use-keyvault-to-pass-secrets\n",
    "# TODO? use this code to set username and password:\n",
    "# # Retrieve username and password from the workspace key vault\n",
    "#       env.docker.base_image_registry.username = ws.get_default_keyvault().get_secret(\"username\")  \n",
    "#       env.docker.base_image_registry.password = ws.get_default_keyvault().get_secret(\"password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify conda and pip dependencies for (Docker) environment\n",
    "# DONE get 'azureml-train-automl' installed, the docker build times out for some reason when I added that to pip_packages\n",
    "#           Help: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-environments\n",
    "\n",
    "conda_packages = ['pip'\n",
    "                 ]\n",
    "\n",
    "pip_packages =   ['azure-core==1.24.1',\n",
    "                  'azure-storage-queue==12.3.0',\n",
    "                  'cffi==1.15.0',\n",
    "                  'cryptography==37.0.3',\n",
    "                  'dataclasses==0.6',\n",
    "                  'google-auth==2.8.0',\n",
    "                  'opencensus==0.9.0',\n",
    "                  'opencensus-ext-azure==1.1.4',\n",
    "                  'portalocker==2.4.0',\n",
    "                  'toolz==0.11.2',\n",
    "                  'azureml.train.automl==1.43.0',\n",
    "                  'azureml.interpret==1.43.0',\n",
    "                  'python-dotenv==0.20.0',\n",
    "                  'rsa==4.8',\n",
    "                 ]\n",
    "\n",
    "condaDependencies = CondaDependencies.create(conda_packages=conda_packages, pip_packages=pip_packages)\n",
    "docker_env.python.conda_dependencies = condaDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Docker Environment and build the Docker image locally\n",
    "registered_docker_env = docker_env.register(ws)\n",
    "print(registered_docker_env)\n",
    "# Need to enable non-root docker user usage of docker for this local build of the image to work, see guide:\n",
    "#           (Actually, this was not enough it seems, I am getting no feedback here, hm.)\n",
    "#           (Then I tried: sudo chmod 777 /var/run/docker.sock, I think 770 is enough because of the docker group owning /var/run/docker.sock at 660 initially)\n",
    "      # (https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user)\n",
    "\n",
    "registered_docker_env.save_to_directory('environment_out', overwrite=True)\n",
    "# If this fails, make sure docker service is running\n",
    "registered_docker_env.build_local(ws, useDocker=True, pushImageToWorkspaceAcr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to run AutoML Training Experiment in Docker Environment (With Docker running on local device)\n",
    "# TODO? maybe turn off this featurization to leave previous featurization or to enhance featurization yourself\n",
    "\n",
    "from azureml.core import ScriptRunConfig\n",
    "import datetime\n",
    "\n",
    "# Set ScriptRunConfig arguments\n",
    "compute_target = 'local'\n",
    "source_directory = './scripts'\n",
    "script_name = 'ojSalesDockerForecastingTrainingAutoML.py'\n",
    "suffix = 'local-' + str(datetime.datetime.now())\n",
    "suffix = suffix.replace(' ', '_') # Clean up datetimestamp\n",
    "suffix = suffix.replace(':', '-') \n",
    "out_model_file_name = 'AutoMLMultiClassification_Iris_BestModel_Docker_{}.pkl'.format(suffix)\n",
    "# set output file name like 'AutoMLMultiClassification_Iris_BestModel_Docker_2022-04-17 21:40:36.114550.pkl'\n",
    "expiriment_name_prefix = \"Iris_Docker_MultiClassification_Training\"\n",
    "automlconfig_experiment_name = expiriment_name_prefix + '_AutoML'\n",
    "scriptrunconfig_experiment_name = expiriment_name_prefix + '_ScriptRunConfig'\n",
    "\n",
    "script_arguments = [\n",
    "\"--tenant-id\", tenant_id,\n",
    "\"--ws-name\", ws_name,\n",
    "\"--subscription-id\", subscription_id,\n",
    "\"--resource-group\", resource_group,\n",
    "\"--datastore-name\", datastore_name,\n",
    "\"--out-model-file-name\", out_model_file_name,\n",
    "\"--target-column-name\", target_column_name,\n",
    "\"--numeric-feature-names\", numericFeatureNamesEncoded,\n",
    "# Pass list encoded as a comma-separated string, containing the raw feature names\n",
    "# like '[\"a\",\"b\"]'\n",
    "\"--categoric-feature-names\", categoricFeatureNamesEncoded,\n",
    "# Pass list encoded as a comma-separated string, containing the name of each dataset (X_train, X_test, y_train, y_test)\n",
    "# like '[\"a\",\"b\"]'\n",
    "\"--x-train-test-y-train-test-combined-train-test\", trainTestDataSetNamesEncoded,\n",
    "\"--num_classes\", num_classes,\n",
    "\"--weight_column_name\", weight_column_name,\n",
    "\"--automlconfig_experiment_name\", automlconfig_experiment_name\n",
    "]\n",
    "print(\"ScriptRunConfig arguments: \", script_arguments)\n",
    "scriptRunConfig = ScriptRunConfig(\n",
    "        source_directory=source_directory,\n",
    "        script=script_name,\n",
    "        arguments=script_arguments,\n",
    "        environment=registered_docker_env,\n",
    "        docker_runtime_config=docker_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit Experiment Run to Docker environment\n",
    "#\n",
    "# (see more on use of Docker environment: \n",
    "#   https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-local/train-on-local.ipynb\n",
    "# )\n",
    "import subprocess\n",
    "# TODO? Add minconda bin to path within docker container? \"/home/johna/miniconda3/bin:\"\n",
    "\n",
    "# import getpass\n",
    "# If you need, you can get a password from user input (Notebook pauses to show a prompt here)\n",
    "# password = getpass.getpass()\n",
    "\n",
    "# Check if Docker is installed and Linux containers are enabled\n",
    "if subprocess.run(\"docker -v\", shell=True).returncode == 0:\n",
    "    subprocess.run(\"service docker status\", shell=True)\n",
    "\n",
    "\n",
    "    # This StackOverflow page will help you run the docker commands with sudo if that is necessary for you: \n",
    "    # https://askubuntu.com/questions/155791/how-do-i-sudo-a-command-in-a-script-without-being-asked-for-a-password\n",
    "    #\n",
    "    # (NOTE: These snippets from the link will allow you to have the docker commands run with sudo)\n",
    "    #\n",
    "    # Create script with the command to run with sudo, for example:\n",
    "    # docker system info\n",
    "    #\n",
    "    # Run sudo chown and chmod commands to grant access of the file to root \n",
    "    # sudo chown root:root ~/docker_system_info.sh\n",
    "    # sudo chmod 700 ~/docker_system_info.sh\n",
    "    #\n",
    "    # Run sudo visudo and insert a line below the line `%sudo   ALL=(ALL:ALL) ALL`\n",
    "    # [username]  ALL=(ALL) NOPASSWD: /home/[username]/docker_system_info.sh\n",
    "    #\n",
    "    # Then call the python script with your python subprocess command\n",
    "    # p = subprocess.run('sudo ~/docker_system_info.sh', shell=True)\n",
    "\n",
    "    # out_docker_system_info = subprocess.run(\"docker system info\", shell=True)\n",
    "    out_docker_system_info = subprocess.check_output('~/docker_system_info.sh', shell=True).decode('ascii')\n",
    "    # out_docker_system_info = subprocess.check_output('sudo su && ~/docker_system_info.sh', shell=True).decode('ascii')\n",
    "    print(out_docker_system_info)\n",
    "        #           [Install Ubuntu](https://docs.docker.com/engine/install/ubuntu/)\n",
    "        #           [Uninstall Docker Engine](https://docs.docker.com/engine/install/ubuntu/#uninstall-docker-engine)\n",
    "        #           WARNING! When I ran this command there is a failure to uninstall and purge all docker engine apt-get packages `sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-compose-plugin`\n",
    "        #                   AND TO RESOLVE (EXTRA CAPS) !!!THIS!!! THING, I DID THE FOLLOWING...\n",
    "        #\n",
    "        # Get /usr/bin/docker: Permission denied? (was not enough for me)\n",
    "        #       See (https://adamtheautomator.com/docker-permission-denied/)\n",
    "        #           (perhaps I need apt dependencies added to Dockerfile string I passed(??))\n",
    "        #       Along with  (https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/environment/)\n",
    "        # TODO? update for my laptop: still not getting any logs past \"nvidia-docker is installed on the target. Using nvidia-docker for docker operations\"\n",
    "        # \n",
    "        # NOTE - NEXT... Try using custom image from your own Docker image repository\n",
    "        #                   (perhaps allow docker to run by johna user normally, without any black magic)\n",
    "        # \n",
    "        # LAST DITCH EFFORTS - If you are stuck, uninstall and install docker, then install different Linux version for WSL, then try reinstall WSL\n",
    "        #\n",
    "        # NOTE - How to get past error: (perhaps uninstall nvidia-docker)\n",
    "        #           nvidia-docker is installed on the target. Using nvidia-docker for docker operations.\n",
    "        # \n",
    "        # You may want to follow this guide to install the Docker engine into Ubuntu:\n",
    "        #           (https://docs.docker.com/engine/install/ubuntu/)\n",
    "        #   Post install steps:\n",
    "        #           (https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user)\n",
    "        #   Alternative way to install:\n",
    "        #            https://github.com/docker/docker-install\n",
    "        #\n",
    "        # Run Experiment in Docker environment\n",
    "        # (NOTE: If you get any errors, in AMLS go to Jobs -> Click Experiment then Run from list -> Look for \"Environment\", \n",
    "        #           There should be a hyperlink to a page for the Environment used for that run!\n",
    "        #           There should be a Docker Build Log you can access\n",
    "        #           You should be able to trigger a build of the Docker image from the Environment's main page\n",
    "        # )\n",
    "    experiment = Experiment(workspace=ws, name=scriptrunconfig_experiment_name)\n",
    "    # NOTE: I previously got an error message including \"GPU\", because of a --gpu flag used the instructions at this link to get past that:\n",
    "    #           (https://docs.nvidia.com/cuda/wsl-user-guide/index.html)\n",
    "    # NOTE: If script is failing at authentication, follow this link for help:\n",
    "    #           (https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication)\n",
    "\n",
    "    # ScriptRunConfig usage to create a Run\n",
    "    ScriptRunConfig_run = experiment.submit(scriptRunConfig)\n",
    "    RunDetails(ScriptRunConfig_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the engineered feature names\n",
    "# (While Experiment script runs, Validate the Engineered Feature Explanation will function inside the script)\n",
    "\n",
    "# Split training features into numeric and categoric dataframes\n",
    "# numeric_X_test = pd.DataFrame(X_test[df_numeric_column_names], dtype=np.str, columns=df_numeric_column_names)\n",
    "# categoric_X_test = pd.DataFrame(X_test[df_categorical_column_names], dtype=np.str, columns=df_categorical_column_names)\n",
    "# # Fit and Run the numeric and categoric ColumnTransformers on the split dataframes to perform feature engineering\n",
    "# preprocessor.transformers[0][1].fit(numeric_X_test)\n",
    "# preprocessor.transformers[1][1].steps[0][1].fit(categoric_X_test)\n",
    "# numeric_X_test_preprocessed = preprocessor.transformers[0][1].transform(numeric_X_test)\n",
    "# numeric_X_test_preprocessed = pd.DataFrame(numeric_X_test_preprocessed, dtype=np.float, columns=df_numeric_column_names)\n",
    "# categoric_X_test_preprocessed = preprocessor.transformers[1][1].steps[0][1].transform(categoric_X_test)\n",
    "# # Fit OneHotEncoder\n",
    "# preprocessor.transformers[1][1].steps[1][1].fit(categoric_X_test_preprocessed)\n",
    "# # Get new One Hot Encoded column names\n",
    "# print(categoric_X_test_preprocessed)\n",
    "# print(df_categorical_column_names)\n",
    "if (0 != len(df_categorical_column_names)):\n",
    "    df_encoded_categorical_column_names = preprocessor.transformers[1][1].steps[1][1].get_feature_names(df_categorical_column_names)\n",
    "else:\n",
    "    df_encoded_categorical_column_names = []\n",
    "# print(\"df_encoded_categorical_column_names\", df_encoded_categorical_column_names)\n",
    "# # Transform categoric, null-imputed features with fitted OneHotEncoder\n",
    "# categoric_X_test_preprocessed = preprocessor.transformers[1][1].steps[1][1].transform(categoric_X_test_preprocessed)\n",
    "# # Turn preprocessed categoric features into a DataFrame\n",
    "# categoric_X_test_preprocessed = pd.DataFrame(categoric_X_test_preprocessed, dtype=np.float64, columns=df_encoded_categorical_column_names)\n",
    "\n",
    "# # Combine the numeric DF with the categorical DF to submit to the AutoML training experiment\n",
    "# X_test_preprocessed_list = [numeric_X_test_preprocessed, categoric_X_test_preprocessed]\n",
    "# X_test_preprocessed = pd.concat(X_test_preprocessed_list, axis=1)\n",
    "\n",
    "\n",
    "# Save engineered features' names to create TabularExplainer with them\n",
    "engineeredFeatures=[*df_numeric_column_names, *df_encoded_categorical_column_names]\n",
    "print(engineeredFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO Somehow wait until script run completion ... (Manually terminate this cell because exception keeps ocurring) Wait for completion of script\n",
    "# from azureml.core import Model\n",
    "# import sklearn\n",
    "# import time\n",
    "# while True:\n",
    "#   try:\n",
    "#     ScriptRunConfig_run.wait_for_completion()\n",
    "#     break\n",
    "#   except:\n",
    "#       print (\"encountered exception waiting for completion of ScriptRunConfig_run, waiting and trying again...\") \n",
    "#       time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO? Get the model metrics somehow and plot output Best Model metrics\n",
    "\n",
    "# Get all metris logged in the run\n",
    "metrics = ScriptRunConfig_run.get_metrics()\n",
    "print(\"metrics: \", metrics)\n",
    "\n",
    "# Get the metrics that were logged from the run of the training script\n",
    "# print(\"metrics['accuracy']: \" + str(metrics['accuracy']))\n",
    "# print(\"metrics['f1']: \" + str(metrics['f1']))\n",
    "\n",
    "# %matplotlib inline\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(metrics['accuracy'], metrics['f1'], marker='o')\n",
    "# plt.ylabel(\"f1\")\n",
    "# plt.xlabel(\"accuracy\")\n",
    "#\n",
    "# You can also list all the files that are associated with this run record\n",
    "#\n",
    "print(\"Here are the files associated with the Azure AutoML Run: \", ScriptRunConfig_run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Experiment Run that was ran, Get Global Explanations (Downloaded with the Experiment Run object!) # TODO? Download model explanations\n",
    "# IMPORTANT: this step will fail until you copy the explanation_id values from the printed list of explanations\n",
    "# from azureml.interpret import ExplanationClient\n",
    "\n",
    "# client = ExplanationClient.from_run(ScriptRunConfig_run)\n",
    "\n",
    "# Get model explanation data\n",
    "# TODO? Get the explanations, for now this client.list_model_explanations() is returning empty\n",
    "# print(client.list_model_explanations())\n",
    "# IMPORTANT: this step will fail until you copy the explanation_id values from the printed list of explanations\n",
    "# engineered_global_explanation_test = client.download_model_explanation(explanation_id='7e81718e-4481-4dcb-a269-0db977df3436')\n",
    "# raw_explanation = client.download_model_explanation(explanation_id='86861485-00ae-42f0-8bab-3ae41556c6a9')\n",
    "\n",
    "\n",
    "# Or only get the top k (e.g., 4) most important features with their importance values\n",
    "# explanation = client.download_model_explanation(top_k=4)\n",
    "\n",
    "# global_importance_values = engineered_global_explanation_test.get_ranked_global_values()\n",
    "# global_importance_names = engineered_global_explanation_test.get_ranked_global_names()\n",
    "# print('global importance values: {}'.format(global_importance_values))\n",
    "# print('global importance names: {}'.format(global_importance_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO? Stop the failures here and download the best model from the AutoML run that's inside the script (then visualize the explanation)\n",
    "# Download the Model from AzureML and Visualize Explanations with it \n",
    "from raiwidgets import ExplanationDashboard\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "import jinja2\n",
    "\n",
    "# print('Model.list(ws)', Model.list(ws))\n",
    "\n",
    "# Download the Model from Azure\n",
    "\n",
    "# Use Model.download and joblib.load()\n",
    "# remote_model_obj = Model(ws, out_model_file_name)\n",
    "# print('Name:', remote_model_obj.name)\n",
    "# print('Version:', remote_model_obj.version)\n",
    "# remote_model_path = remote_model_obj.download(exist_ok = True)\n",
    "# downloaded_model = joblib.load(remote_model_path)\n",
    "\n",
    "# BEGIN Access \"Local Explanations\", uncomment these lines if you want to do that here...\n",
    "# (Local Explanation meaning \"of individual predictions\") \n",
    "# from interpret.ext.blackbox import TabularExplainer\n",
    "# \"features\" and \"classes\" fields are optional\n",
    "# explainer = TabularExplainer(downloaded_model,\n",
    "                            #  X_test,\n",
    "                            #  features=features)\n",
    "\n",
    "# Get explanation for the first few data points in the test set\n",
    "# local_explanation = explainer.explain_local(X_test[0:5])\n",
    "# Sorted feature importance values and feature names\n",
    "# sorted_local_importance_names = local_explanation.get_ranked_local_names()\n",
    "# print('sorted_local_importance_names: ', sorted_local_importance_names)\n",
    "# print('len(sorted_local_importance_names): ', len(sorted_local_importance_names))\n",
    "# sorted_local_importance_values = local_explanation.get_ranked_local_values()\n",
    "# print('sorted_local_importance_values: ', sorted_local_importance_values)\n",
    "# print('len(sorted_local_importance_values): ', len(sorted_local_importance_values))\n",
    "# COOL THING TO DO: Sometime could get local explanation of specific data points uploaded, downloaded, and visualized as well...\n",
    "# END Access \"Local Explanations\"\n",
    "\n",
    "# Visualize explanations\n",
    "# Be sure to pass dataset=(test feature columns Dataframe) and true_y=(test predicted column Dataframe)\n",
    "#       1) getting the raiwidgets thing working\n",
    "#       2) see README at https://github.com/interpretml/interpret\n",
    "# ExplanationDashboard(engineered_global_explanation_test, downloaded_model, dataset=X_test, true_y=y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46ae9456b2737841401e3d3c7acdf327031e52ebe64544d23b06b17169cc2049"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit ('3.7.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
