{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set (User Managed) Local Environment up\n",
    "from azureml.core import Environment\n",
    "\n",
    "# Editing a run configuration property on-fly.\n",
    "user_managed_env = Environment(\"user-managed-env\")\n",
    "\n",
    "user_managed_env.python.user_managed_dependencies = True\n",
    "\n",
    "# You can choose a specific Python environment by pointing to a Python path \n",
    "#user_managed_env.python.interpreter_path = '/home/johndoe/miniconda3/envs/myenv/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "# Get the Workspace object from Azure\n",
    "# You can find tenant id under azure active directory->properties\n",
    "tenant_id = '198c7d8c-e010-45ce-a018-ec2d9a33f58f'\n",
    "ia = InteractiveLoginAuthentication(tenant_id=tenant_id)\n",
    "ws_name = 'automlbook'\n",
    "subscription_id = '4d278f3d-b4fd-4fa2-86b6-d34b96bc888f'\n",
    "resource_group = 'Foxy_Resources'\n",
    "ws = Workspace.get(name=ws_name,\n",
    "                   subscription_id=subscription_id,\n",
    "                   resource_group=resource_group,\n",
    "                   auth=ia)\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datastore, try getting datastore via Workspace object\n",
    "datastore = Datastore.get_default(ws)\n",
    "datastore_name = 'workspaceblobstore'\n",
    "datastore = Datastore.get(ws, datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the datastore of the Workspace\n",
    "dataset_name = 'automlbook Titanic Training Data A'\n",
    "# dataset = Dataset.get_by_name(ws, dataset_name)\n",
    "dataset = Dataset.get_by_name(ws, dataset_name, version = 'latest')\n",
    "dataset_columns = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "\n",
    "# Show a sample of the data in the dataset\n",
    "dataset.take(10).to_pandas_dataframe()\n",
    "\n",
    "# Turn Dataset into Pandas Dataframe, it is to be preprocessed\n",
    "df = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess numeric columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_column_names = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "df_numeric_column_names = ['Age', 'Fare']\n",
    "\n",
    "# BEGIN Create dataframe with numeric columns so that it contains all numbers that are preprocessed...\n",
    "\n",
    "# For int column Age, Impute NaN numeric values, and Remove outliers\n",
    "print('Before Removing outliers or Imputing null values, df[Age]: ', df['Age'])\n",
    "ageMedian = np.nanmedian(df['Age'])\n",
    "print('ageMedian: ', ageMedian)\n",
    "df['Age'] = np.where(np.isnan(df['Age']), ageMedian, df['Age'])\n",
    "print('Before Removing outliers and after Imputing null values, df[Age]: ', df['Age'])\n",
    "\n",
    "# Calculate 3STD and Mean for Age\n",
    "ageThreeSD = np.std(df['Age']) * 3\n",
    "ageMean = np.mean(df['Age'])\n",
    "ageOutlierThreshold = round(ageThreeSD + ageMean)\n",
    "print('Age Outlier Threshold: ', ageOutlierThreshold)\n",
    "\n",
    "# Remove Outliers by replacing all values above Threshold (3STD + Mean) with Threshold Value\n",
    "df['Age'] = df['Age'].mask(df['Age'] > ageOutlierThreshold, ageOutlierThreshold)\n",
    "print('After Removing outliers and Imputing null values, df[Age]: ', df['Age'])\n",
    "\n",
    "# Copy df, keeping only Age column, set type of this df copy to float\n",
    "df_age_column = pd.DataFrame(df['Age'], columns=['Age'])\n",
    "\n",
    "\n",
    "# Copy df, keeping only float numeric columns, set type of this df copy to float\n",
    "df_float_column_names = ['Fare']\n",
    "print('df_float_column_names: ', df_float_column_names)\n",
    "df_float_columns = pd.DataFrame(df[df_float_column_names], dtype=np.float, columns=df_float_column_names)\n",
    "\n",
    "# Concatenate the numeric Data Frames to scale them\n",
    "print('Before concatenation to df_numeric_columns, df[Age]: ', df['Age'])\n",
    "print('Before concatenation to df_numeric_columns, df_age_column: ', df_age_column)\n",
    "df_numeric_columns = pd.concat([df_age_column, df_float_columns], keys=df_numeric_column_names, axis=1)\n",
    "print('concatenated df_numeric_columns: ', df_numeric_columns)\n",
    "\n",
    "\n",
    "# Use StandardScaler or MinMaxScaler on Numeric/Non-Categorical columns split\n",
    "scaler = StandardScaler().fit(df_numeric_columns)\n",
    "print('scaler.mean_: ', scaler.mean_)\n",
    "print('scaler.scale: ', scaler.scale_)\n",
    "\n",
    "df_scaled_numeric_columns =  pd.DataFrame(scaler.transform(df_numeric_columns), columns=df_numeric_column_names)\n",
    "print('df_scaled_numeric_columns: ', df_scaled_numeric_columns)\n",
    "# Scaled data should have zero mean and unit variance, check with these prints:\n",
    "print('df_scaled_numeric_columns.mean(axis=0): ', df_scaled_numeric_columns.mean(axis=0))\n",
    "print('df_scaled_numeric_columns.std(axis=0)', df_scaled_numeric_columns.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess categorical columns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# BEGIN Create dataframe with categorical columns so that it contains all categorical data that is preprocessed...\n",
    "\n",
    "# Copy df, keeping only categorical columns, and one-hot encode them\n",
    "df_categorical_column_names_raw = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin', 'Embarked']\n",
    "print('df_categorical_column_names_raw: ', df_categorical_column_names_raw)\n",
    "df_categorical_columns = pd.DataFrame(df[df_categorical_column_names_raw], dtype=np.str, columns=df_categorical_column_names_raw)\n",
    "print('df_categorical_columns: ', df_categorical_columns)\n",
    "encoder = OneHotEncoder(drop='first', handle_unknown='error', sparse=False, ).fit(df_categorical_columns)\n",
    "print('encoder.categories: ', encoder.categories)\n",
    "df_encoded_categorical_columns = pd.DataFrame(encoder.transform(df_categorical_columns))\n",
    "df_encoded_categorical_columns.columns = encoder.get_feature_names(df_categorical_column_names_raw)\n",
    "print('df_encoded_categorical_columns: ', df_encoded_categorical_columns)\n",
    "#   By default, the values each feature can take is inferred automatically from the dataset and can be found in the categories_ attribute:\n",
    "\n",
    "\n",
    "# Combine the numeric DF with the categorical DF\n",
    "dfs = [df['Survived'], df_scaled_numeric_columns, df_encoded_categorical_columns]\n",
    "print('Before concatenation to dfTyped, df[\\'Survived\\']: ', df['Survived'])\n",
    "print('Before concatenation to dfTyped, df[Age]: ', df['Age'])\n",
    "print('Before concatenation to dfTyped, df_numeric_columns: ', df_numeric_columns)\n",
    "print('Before concatenation to dfTyped, df_scaled_numeric_columns: ', df_scaled_numeric_columns)\n",
    "dfTyped = pd.concat(dfs, axis=1)\n",
    "print('dfTyped: ', dfTyped)\n",
    "print('dfTyped[Age]: ', dfTyped['Age'])\n",
    "\n",
    "        # - With sklearn.preprocessing, preprocess your Dataframes before training model in the Python Script\n",
    "        #     - [Guide at SciKit Learn site](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "        #     - Use OneHotEncoder\n",
    "        #     - Use StandardScaler or  MinMaxScaler while you're at it\n",
    "        #     - Don't worry about any other preprocessing to just get the training working\n",
    "        #     - Strategy:\n",
    "        #         - d Split dataframe into Numeric/Non-Categorial and Non-Numeric/Categorial columns\n",
    "        #             - ! Use StandardScaler or MinMaxScaler on Numeric/Non-Categorical columns split\n",
    "        #             - d Use OneHotEncoder on Non-Numeric/Categorical columns split\n",
    "\n",
    "# Initial Data Frame is now preprocessed in dfPreprocessed\n",
    "dfPreprocessed = dfTyped\n",
    "print('dfPreprocessed: ', dfPreprocessed)\n",
    "\n",
    "# Split DataFrame for training now that it is pre-processed\n",
    "target_column_name = 'Survived'\n",
    "df_x = dfPreprocessed.drop([target_column_name], axis=1)\n",
    "df_y = dfPreprocessed.filter([target_column_name], axis=1)\n",
    "print(\"See df_x\", df_x)\n",
    "print(\"See df_y\", df_y)\n",
    "# Register Pandas Dataframe of base df_x and df_y\n",
    "Dataset.Tabular.register_pandas_dataframe(df_x, datastore, \"Titanic Feature Column Data for train_test_split usage\")\n",
    "Dataset.Tabular.register_pandas_dataframe(df_y, datastore, \"Titanic Target Column Data for train_test_split usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names to pass to Experiment runner\n",
    "features=[*df_numeric_column_names, *df_encoded_categorical_columns.columns]\n",
    "# Encode features (names) list into a string like '[\"a\",\"b\"]'\n",
    "for x in range(len(features)):\n",
    "        features[x] = '\"{}\"'.format(features[x])\n",
    "featuresEncoded = \"[{}]\".format(\",\".join(features))\n",
    "print(featuresEncoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For splitting of data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "# What you need to pass to train_test_split...\n",
    "# ... I need X and Y dataframe, X just with target missing, Y just with target column present\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Register the splits\n",
    "Dataset.Tabular.register_pandas_dataframe(X_train, datastore, \"Titanic Feature Column Data for training\")\n",
    "Dataset.Tabular.register_pandas_dataframe(X_test, datastore, \"Titanic Feature Column Data for testing\")\n",
    "Dataset.Tabular.register_pandas_dataframe(y_train, datastore, \"Titanic Target Column Data for training\")\n",
    "Dataset.Tabular.register_pandas_dataframe(y_test, datastore, \"Titanic Target Column Data for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training Experiment locally\n",
    "from azureml.core import ScriptRunConfig\n",
    "import datetime\n",
    "\n",
    "# Experiment\n",
    "experiment_name = 'Local_Training_AzureML'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "# Define Compute Cluster to use\n",
    "compute_target = 'local'\n",
    "source_directory = './scripts'\n",
    "script_name = 'localTrainingAzureML.py'\n",
    "dataset_name = 'automlbook Titanic Training Data A'\n",
    "# set output file name like 'DecisionTreeClassifier_Titanic_local-2022-04-17 21:40:36.114550.pkl'\n",
    "suffix = 'local-' + str(datetime.datetime.now())\n",
    "suffix = suffix.replace(' ', '_') # Clean up datetimestamp\n",
    "suffix = suffix.replace(':', '-') \n",
    "out_model_file_name = 'DecisionTreeClassifier_Titanic_{}.pkl'.format(suffix)\n",
    "# set output file name like 'DecisionTreeClassifier_Titanic_local-2022-04-17 21:40:36.114550.pkl'\n",
    "\n",
    "script_arguments = [\n",
    "\"--tenant-id\", tenant_id,\n",
    "\"--ws-name\", ws_name,\n",
    "\"--subscription-id\", subscription_id,\n",
    "\"--resource-group\", resource_group,\n",
    "\"--datastore-name\", datastore_name,\n",
    "\"--out-model-file-name\", out_model_file_name,\n",
    "\"--features\", featuresEncoded\n",
    "]\n",
    "scriptRunConfig = ScriptRunConfig(\n",
    "        source_directory=source_directory,\n",
    "        script=script_name,\n",
    "        arguments=script_arguments,\n",
    "        environment=user_managed_env,\n",
    "        compute_target=compute_target)\n",
    "        \n",
    "AzureML_run = experiment.submit(scriptRunConfig)\n",
    "RunDetails(AzureML_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Model from the AzureML_run\n",
    "description = \"Best Local AzureML Regression Run using Titanic Sample Data.\"\n",
    "tags = {\n",
    "  \"project\" : \"Local Training AzureML\", \n",
    "  \"creator\": \"fox\", \n",
    "  \"task\": \"classification\", \n",
    "  \"dataset\": \"automlbook Titanic Training Data A\", \n",
    "  \"metric\": \"normalized_root_mean_squared_error\"\n",
    "}\n",
    "\n",
    "# Attempt to register model once output model file is available\n",
    "from azureml.core import Model\n",
    "import sklearn\n",
    "import time\n",
    "while True:\n",
    "  try:\n",
    "    AzureML_run.register_model(model_path='./outputs', model_name=out_model_file_name, description=description, tags=tags,\n",
    "                            model_framework=Model.Framework.SCIKITLEARN, # Framework used to create the model.\n",
    "                            model_framework_version=sklearn.__version__)  # Version of scikit-learn used to create the model.)\n",
    "    break\n",
    "  except:\n",
    "      print (\"encountered exception registering model output file, waiting and trying again...\") \n",
    "      time.sleep(60)\n",
    "# set output file name like 'DecisionTreeClassifier_Titanic_local-2022-04-17 21:40:36.114550.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the Model with downloaded Explanation\n",
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run(AzureML_run)\n",
    "\n",
    "# get model explanation data\n",
    "global_explanation = client.download_model_explanation()\n",
    "# or only get the top k (e.g., 4) most important features with their importance values\n",
    "# explanation = client.download_model_explanation(top_k=4)\n",
    "\n",
    "global_importance_values = global_explanation.get_ranked_global_values()\n",
    "global_importance_names = global_explanation.get_ranked_global_names()\n",
    "print('global importance values: {}'.format(global_importance_values))\n",
    "print('global importance names: {}'.format(global_importance_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explanations\n",
    "from raiwidgets import ExplanationDashboard\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "\n",
    "# print('Model.list(ws)', Model.list(ws))\n",
    "\n",
    "# Try 1: just Model constructor and joblib.load()\n",
    "# downloaded_model = Model(ws, out_model_file_name)\n",
    "# joblib.load(downloaded_model)\n",
    "\n",
    "# Try 2: Use Model.get_model_path and joblib.load()\n",
    "# remote_model_path = Model.get_model_path(out_model_file_name, _workspace=ws)\n",
    "# downloaded_model = joblib.load(remote_model_path)\n",
    "\n",
    "# Try 3: Use Model.download and joblib.load()\n",
    "remote_model_obj = Model(ws, out_model_file_name)\n",
    "print('Name:', remote_model_obj.name)\n",
    "print('Version:', remote_model_obj.version)\n",
    "remote_model_path = remote_model_obj.download(exist_ok = True)\n",
    "downloaded_model = joblib.load(remote_model_path)\n",
    "\n",
    "# TODO? get local explanation uploaded, downloaded, and visualized as well, \n",
    "#       for individual feature performance, etc...\n",
    "\n",
    "# Be sure to pass dataset=(test feature columns Dataframe) and true_y=(test predicted column Dataframe)\n",
    "ExplanationDashboard(global_explanation, downloaded_model, dataset=X_test, true_y=y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46ae9456b2737841401e3d3c7acdf327031e52ebe64544d23b06b17169cc2049"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit ('3.7.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
