{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Datastore, Environment\n",
    "from azureml.core import Experiment\n",
    "import azureml.interpret\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Workspace object from Azure\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "# You can find tenant id under azure active directory->properties\n",
    "tenant_id = '198c7d8c-e010-45ce-a018-ec2d9a33f58f'\n",
    "\n",
    "# Authenticate and get Workspace object\n",
    "ia = InteractiveLoginAuthentication(tenant_id=tenant_id)\n",
    "ws_name = 'automlbook'\n",
    "subscription_id = '4d278f3d-b4fd-4fa2-86b6-d34b96bc888f'\n",
    "resource_group = 'Foxy_Resources'\n",
    "ws = Workspace.get(name=ws_name,\n",
    "                   subscription_id=subscription_id,\n",
    "                   resource_group=resource_group,\n",
    "                   auth=ia)\n",
    "print(\"After authenticating to the workspace with Interactive Authentication:\", \n",
    "        \"ws.name: \" + ws.name, \n",
    "        \"ws.resource_group: \" + ws.resource_group, \n",
    "        \"ws.location: \" + ws.location, \n",
    "        \"ws.subscription_id: \" + ws.subscription_id, \n",
    "sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Commmented example code to Auth with Service Principal)\n",
    "# (Purely to Validate this will work inside script...) \n",
    "\n",
    "# TODO? maybe? I could build a custom image with BuildKit, with \"Secret mount type\", and then have Azure get it out of Docker Repository\n",
    "# Store authentication strings\n",
    "# Store username and password to Service Principal in order to authenticate within the python script\n",
    "# CRITICAL: You must have a file like this at resources/custom_env_vars_for_script_inside_docker_container (from project root)...\n",
    "#       # Set AzureML Service Principle ID and Password\n",
    "#       AML_PRINCIPAL_ID=\"<Principal ID, AKA clientId>\"\n",
    "#       AML_PRINCIPAL_PASS=\"<Principal Password, AKA clientSecret>\"\n",
    "\n",
    "# Authenticate with the Service Principal in order to get the Workspace object\n",
    "# from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "# sp = ServicePrincipalAuthentication(tenant_id=tenant_id,\n",
    "#                                     service_principal_id=kv.get_secret(name=\"localDockerAmlPrincipalId\"), # clientId of service principal\n",
    "#                                     service_principal_password=kv.get_secret(name=\"localDockerAmlPrincipalPass\")) # clientSecret of service principal\n",
    "# ws = Workspace.get(name=ws_name,\n",
    "#                    subscription_id=subscription_id,\n",
    "#                    resource_group=resource_group,\n",
    "#                    auth=sp)\n",
    "# print(\"After re-authenticating to the workspace with Service Principal\", \n",
    "#         \"ws.name: \" + ws.name, \n",
    "#         \"ws.resource_group: \" + ws.resource_group, \n",
    "#         \"ws.location: \" + ws.location, \n",
    "#         \"ws.subscription_id: \" + ws.subscription_id, \n",
    "# sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Print statements to list available environments)\n",
    "# envs = Environment.list(workspace=ws)\n",
    "\n",
    "# for env in envs:\n",
    "    # if env.startswith(\"AzureML\"):\n",
    "        # print(\"Name\",env)\n",
    "        # if None != envs[env].python.conda_dependencies:\n",
    "            # print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datastore, try getting datastore via Workspace object\n",
    "datastore = Datastore.get_default(ws)\n",
    "datastore_name = 'workspaceblobstore'\n",
    "datastore = Datastore.get(ws, datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the datastore of the Workspace\n",
    "dataset_name = 'Diabetes Sample Full Transform'\n",
    "# TODO? perhaps use instead (and perform following two transformations): 'automlbook Diabetes Sample A'\n",
    "#       dfRaw['AGE'].mask(dfRaw.AGE > AgeMean, AgeMean)\n",
    "#       dfRaw['BMI'] = np.where(dfRaw['BMI'] > 30, 1, 0)\n",
    "# dataset = Dataset.get_by_name(ws, dataset_name)\n",
    "dataset = Dataset.get_by_name(ws, dataset_name, version = 'latest')\n",
    "dataset_columns = ['AGE', 'SEX', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'Y']\n",
    "\n",
    "# Show a sample of the data in the dataset\n",
    "dataset.take(10).to_pandas_dataframe()\n",
    "\n",
    "# Turn Dataset into Pandas Dataframe, it is to be preprocessed\n",
    "df = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Dataframe to get one for Numeric columns and one for Categorical columns\n",
    "\n",
    "df_column_names = ['AGE', 'SEX', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']\n",
    "\n",
    "##### BEGIN Create dataframe with numeric columns so that it contains all numbers that are preprocessed...\n",
    "df_numeric_column_names = ['AGE', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']\n",
    "# Copy df, keeping only float numeric columns, set type of this df copy to float\n",
    "df_float_column_names = ['BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']\n",
    "df_float_columns = pd.DataFrame(df[df_float_column_names], dtype=np.float, columns=df_float_column_names)\n",
    "# # Copy df, keeping only integer Age column to leave as an integer\n",
    "df_integer_column = pd.DataFrame(df['AGE'], columns=['AGE'])\n",
    "# Concatenate the numeric DataFrames\n",
    "df_numeric_columns = pd.concat([df_integer_column, df_float_columns], axis=1)\n",
    "\n",
    "\n",
    "##### BEGIN Create dataframe with categorical columns so that it contains all categorical data that is preprocessed...\n",
    "df_categorical_column_names = ['SEX']\n",
    "# Copy df, keeping only categorical columns\n",
    "df_categorical_columns = pd.DataFrame(df[df_categorical_column_names], dtype=np.str, columns=df_categorical_column_names)\n",
    "\n",
    "print('concatenated df_numeric_columns: ', df_numeric_columns)\n",
    "print('df_categorical_columns: ', df_categorical_columns)\n",
    "\n",
    "feature_column_names = [*df_numeric_column_names, *df_categorical_column_names]\n",
    "print(feature_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the numeric DF with the categorical DF\n",
    "# print(\"df['Y'] is \", df['Y'])\n",
    "# print(\"df_numeric_columns is \", df_numeric_columns)\n",
    "# print(\"df_numeric_columns.columns is \", df_numeric_columns.columns)\n",
    "# print(\"df_categorical_columns is \", df_categorical_columns)\n",
    "# print(\"df_categorical_columns.columns is \", df_categorical_columns.columns)\n",
    "\n",
    "# Concatenate dfs to get DataFrame of all columns to submit to the training script\n",
    "dfs = [df['Y'], df_numeric_columns, df_categorical_columns]\n",
    "# print(\"dfs is\" + str(dfs))\n",
    "# print('Before concatenation to dfTyped, df[\\'Y\\']: ', df['Y'])\n",
    "# print('Before concatenation to dfTyped, df_numeric_columns: ', df_numeric_columns)\n",
    "dfTyped = pd.concat(dfs, axis=1)\n",
    "print('dfTyped: ', dfTyped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pre-transformation Data Frame into feature/target columns\n",
    "target_column_name = 'Y'\n",
    "df_x = dfTyped.drop([target_column_name], axis=1)\n",
    "df_y = dfTyped[target_column_name].to_frame()\n",
    "# print(\"See df_x\", df_x)\n",
    "print(\"See df_y\", df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor to preprocess numeric and categorical columns (with Transfomer API via ColumnTransformer)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', one_hot_encoder)])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, df_numeric_column_names),\n",
    "        ('cat', categorical_transformer, df_categorical_column_names)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Validate the Pipeline will function once passed to the script)\n",
    "\n",
    "# Get the preprocessed Data Frame columns in a list\n",
    "one_hot_encoder.fit(df_categorical_columns)\n",
    "# Get new One Hot Encoded column names\n",
    "encoded_categorical_column_names = one_hot_encoder.get_feature_names(df_categorical_column_names)\n",
    "encoded_feature_names = [*df_numeric_column_names, *encoded_categorical_column_names]\n",
    "print(str(encoded_feature_names))\n",
    "\n",
    "preprocessor.fit(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Apply the preprocessor to the columns\n",
    "print(df_x)\n",
    "df_x = preprocessor.transform(df_x)\n",
    "df_x = pd.DataFrame(df_x, columns=encoded_feature_names)\n",
    "print(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Pandas Dataframe of base df_x and df_y\n",
    "Dataset.Tabular.register_pandas_dataframe(df_x, datastore, \"Diabetes Feature Column Data for train_test_split usage (Docker Environment)\")\n",
    "Dataset.Tabular.register_pandas_dataframe(df_y, datastore, \"Diabetes Target Column Data for train_test_split usage (Docker Environment)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove me # Append regressor to preprocessing pipeline (Then we have a full prediction pipeline)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor()\n",
    "\n",
    "regressor_pipeline = Pipeline(steps=[('regressor', regressor)])\n",
    "# The regressor_pipeline is later de-pickled and used in the Experiment Run's python script\n",
    "\n",
    "# Now we have a full prediction pipeline.\n",
    "print(regressor_pipeline.__dict__)\n",
    "# Note that you may access regressor_pipeline ColumnTransformers as follows...\n",
    "# print(regressor_pipeline['preprocessor'].transformers[1][1][1].get_feature_names(df_categorical_column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove me # Pickle the SciKit Learn Pipeline to pass it to the script\n",
    "# Link about pickling:\n",
    "# https://codefather.tech/blog/python-pickle/#:~:text=%20Python%20Pickle%3A%20Serialize%20Your%20Objects%20%20,The%20pickle%20module%20also%20allows%20to...%20More%20\n",
    "import pickle\n",
    "\n",
    "pickled_pipeline = pickle.dumps(regressor_pipeline)\n",
    "# print(pickle.loads(pickled_pipeline))\n",
    "\n",
    "import os\n",
    "os.makedirs('./scripts/resources', exist_ok=True)\n",
    "with open('./scripts/resources/regressor_pipeline.pickle', 'wb') as file:\n",
    "    pickle.dump(regressor_pipeline, file)\n",
    "\n",
    "# You may copy this to the script and Unpickle the SciKit Pipline (that performs Transformation and Model Training)\n",
    "# with open('regressor_pipeline.pickle', 'rb') as file:\n",
    "#     unpickled_pipeline = pickle.load(file)\n",
    "#     print(unpickled_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test data, register the resulting Datasets with Azure\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "# What you need to pass to train_test_split...\n",
    "# ... I need X and Y dataframe, X just with target missing, Y just with target column present\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "print(test_data)\n",
    "\n",
    "# print(\"See y_test\", y_test)\n",
    "# print(\"See y_test.columns.tolist()\", str(y_test.columns.tolist()))\n",
    "# print(\"See y_test.values.tolist() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.tolist())\n",
    "# print(\"See y_test.values.ravel() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.ravel())\n",
    "# print(\"See y_test.values.tolist().flatten() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.flatten())\n",
    "\n",
    "# Register the split (by whether target column) training and test datasets\n",
    "X_train_registered_name = \"Diabetes Feature Column Data for training (Docker Environment)\"\n",
    "X_test_registered_name = \"Diabetes Feature Column Data for testing (Docker Environment)\"\n",
    "y_train_registered_name = \"Diabetes Target Column Data for training (Docker Environment)\"\n",
    "y_test_registered_name = \"Diabetes Target Column Data for testing (Docker Environment)\"\n",
    "# Dataset.Tabular.register_pandas_dataframe(X_train, datastore, X_train_registered_name)\n",
    "Dataset.Tabular.register_pandas_dataframe(X_test, datastore, X_test_registered_name)\n",
    "# Dataset.Tabular.register_pandas_dataframe(y_train, datastore, y_train_registered_name)\n",
    "Dataset.Tabular.register_pandas_dataframe(y_test, datastore, y_test_registered_name)\n",
    "\n",
    "# Register the combined (feature and target columns) training and test datasets\n",
    "train_data_registered_name = \"Diabetes Training Data (Docker Environment)\"\n",
    "test_data_registered_name = \"Diabetes Training Test Data (Docker Environment)\"\n",
    "Dataset.Tabular.register_pandas_dataframe(train_data, datastore, train_data_registered_name)\n",
    "Dataset.Tabular.register_pandas_dataframe(test_data, datastore, test_data_registered_name)\n",
    "\n",
    "trainTestDataSetNames = [X_train_registered_name, X_test_registered_name, y_train_registered_name, y_test_registered_name, train_data_registered_name, test_data_registered_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names and create TabularExplainer with them\n",
    "features=[*df_numeric_column_names, *encoded_categorical_column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Experiment script arguments list into a string like '[\"a\",\"b\"]'\n",
    "\n",
    "# Encode numeric column names list\n",
    "temp_column_names = df_numeric_column_names.copy()\n",
    "for x in range(len(temp_column_names)):\n",
    "        temp_column_names[x] = '\"{}\"'.format(temp_column_names[x])\n",
    "numericFeatureNamesEncoded = \"[{}]\".format(\",\".join(temp_column_names))\n",
    "# print(\"numericFeatureNamesEncoded:\", numericFeatureNamesEncoded)\n",
    "\n",
    "# Encode categoric column names list\n",
    "temp_column_names = encoded_categorical_column_names.copy()\n",
    "for x in range(len(temp_column_names)):\n",
    "        temp_column_names[x] = '\"{}\"'.format(temp_column_names[x])\n",
    "categoricFeatureNamesEncoded = \"[{}]\".format(\",\".join(temp_column_names))\n",
    "# print(\"categoricFeatureNamesEncoded:\", categoricFeatureNamesEncoded)\n",
    "\n",
    "# Encode split dataset names list\n",
    "for x in range(len(trainTestDataSetNames)):\n",
    "        trainTestDataSetNames[x] = '\"{}\"'.format(trainTestDataSetNames[x])\n",
    "trainTestDataSetNamesEncoded = \"[{}]\".format(\",\".join(trainTestDataSetNames))\n",
    "print(\"splitDatasetNamesEncoded:\", trainTestDataSetNamesEncoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Local Docker Environment up (with System Managed Dependencies, via Conda)\n",
    "#\n",
    "# Learn about Environment and how to use a Docker Environment here:\n",
    "#       https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments\n",
    "#       https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment(class)?view=azure-ml-py\n",
    "#       ! IMPORTANT: https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/environment/\n",
    "#\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "\n",
    "docker_env = Environment(\"docker-env\")\n",
    "# Editing a run configuration property on-fly.\n",
    "docker_env.python.user_managed_dependencies = False\n",
    "# Use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param.\n",
    "#           https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfig.dockerconfiguration?view=azure-ml-py\n",
    "docker_config = DockerConfiguration(use_docker=True)\n",
    "print(\"initial base image from base docker-env Environment: \", docker_env.docker.base_image)\n",
    "\n",
    "# Specify docker steps as a string. \n",
    "dockerfile = r\"\"\"\n",
    "FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220314.v1\n",
    "\n",
    "ARG DEBIAN_FRONTEND=noninteractive\n",
    "ARG ACCEPT_EULA=Y\n",
    "RUN ls -altr\n",
    "RUN apt-get update -y && apt-get upgrade -y &&\\\n",
    "    apt-get install -y build-essential \\\n",
    "                       cmake \\\n",
    "                       curl \\\n",
    "                       gfortran \\\n",
    "                       git \\\n",
    "                       jupyter \\\n",
    "                       libatlas-base-dev \\\n",
    "                       libblas-dev \\\n",
    "                       libbz2-dev \\\n",
    "                       libffi-dev \\\n",
    "                       libgdbm-dev \\\n",
    "                       liblapack-dev \\\n",
    "                       liblzma-dev \\\n",
    "                       libncurses5-dev \\\n",
    "                       libncursesw5-dev \\\n",
    "                       libreadline-dev \\\n",
    "                       libsqlite3-dev \\\n",
    "                       libssl-dev \\\n",
    "                       libxml2-dev \\\n",
    "                       libxmlsec1-dev \\\n",
    "                       llvm \\\n",
    "                       lzma \\\n",
    "                       lzma-dev \\\n",
    "                       make \\\n",
    "                       tcl-dev \\\n",
    "                       tk-dev \\\n",
    "                       wget \\\n",
    "                       xz-utils \\\n",
    "                       zlib1g-dev\n",
    "\n",
    "RUN conda -V\n",
    "RUN echo \"Hello from custom container!\" > ~/hello.txt\n",
    "RUN pip install azureml.interpret azureml-dataset-runtime azureml.train azureml-train-automl jinja2 MarkupSafe raiwidgets python-dotenv pybridge \n",
    "RUN export PIP_LOG=\"/tmp/pip_log.txt\" && touch ${PIP_LOG} && tail -f ${PIP_LOG} & conda env create -f \"conda.yml\" && killall tail && rm ${PIP_LOG}\n",
    "\"\"\"\n",
    "\n",
    "# NOTE: you can pass Dockerfile string to docker build command via stdin like this:\n",
    "#\n",
    "# sudo docker build -t myimage:latest -<<EOF\n",
    "# FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220314.v1\n",
    "# RUN echo \"hello world\"\n",
    "# EOF\n",
    "\n",
    "\n",
    "# Set base image to None, because the image is defined by dockerfile.\n",
    "docker_env.docker.base_image = None\n",
    "# Use the Dockerfile string and build image based on it with this code from above (move it down here)\n",
    "docker_env.docker.base_dockerfile = dockerfile\n",
    "\n",
    "#       For help, try reading this\n",
    "#                    - https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/environment/#custom-docker-image--dockerfile\n",
    "#       May also try using other base Docker images from these container registries\n",
    "#                - https://github.com/microsoft/containerregistry\n",
    "#                - https://github.com/Azure/AzureML-Containers\n",
    "# env.docker.base_image = '<image-name>'\n",
    "# env.docker.base_image_registry.address = '<container-registry-address>'\n",
    "# env.docker.base_image_registry.username = '<acr-username>'\n",
    "# env.docker.base_image_registry.password = os.environ.get(\"CONTAINER_PASSWORD\")\n",
    "#\n",
    "# TODO? use this link to get username and password from Azure KeyVault:\n",
    "#           https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/environment/#use-keyvault-to-pass-secrets\n",
    "# TODO? use this code to set username and password:\n",
    "# # Retrieve username and password from the workspace key vault\n",
    "#       env.docker.base_image_registry.username = ws.get_default_keyvault().get_secret(\"username\")  \n",
    "#       env.docker.base_image_registry.password = ws.get_default_keyvault().get_secret(\"password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify conda dependencies with scikit-learn\n",
    "conda_packages = ['pip',\n",
    "                  'pyspark',\n",
    "                  'scikit-learn'\n",
    "                 ]\n",
    "# TODO get 'azureml-train-automl' installed, the docker build times out for some reason when I added that to pip_packages\n",
    "#           Help: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-environments\n",
    "# TODO try removing packages until maybe the docker image can build, \n",
    "#  (wow, getting stopped at installed azureml.train.automl in environment...)\n",
    "pip_packages =   ['azureml.interpret',\n",
    "                  'azureml-dataset-runtime',\n",
    "                  'azureml.train',\n",
    "                  'azureml.train.automl',\n",
    "                  'jinja2',\n",
    "                  'MarkupSafe',\n",
    "                  'raiwidgets',\n",
    "                  'python-dotenv',\n",
    "                  'pybridge'\n",
    "                 ]\n",
    "\n",
    "condaDependencies = CondaDependencies.create(conda_packages=conda_packages, pip_packages=pip_packages)\n",
    "docker_env.python.conda_dependencies = condaDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Docker Environment and build the Docker image locally\n",
    "registered_docker_env = docker_env.register(ws)\n",
    "print(registered_docker_env)\n",
    "# Need to enable non-root docker user usage of docker for this local build of the image to work, see guide:\n",
    "#           (Actually, this was not enough it seems, I am getting no feedback here, hm.)\n",
    "#           (Then I tried: sudo chmod 777 /var/run/docker.sock, I think 770 is enough because of the docker group owning /var/run/docker.sock at 660 initially)\n",
    "      # (https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user)\n",
    "\n",
    "registered_docker_env.save_to_directory('environment_out', overwrite=True)\n",
    "# If this fails, make sure docker service is running\n",
    "registered_docker_env.build_local(ws, useDocker=True, pushImageToWorkspaceAcr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Have the scriptRunConfig be used to run an AutoML performing script \n",
    "# Prepare to run AutoML Training Experiment in Docker Environment (With Docker running on local device)\n",
    "# TODO? maybe turn off this featurization to leave previous featurization or to enhance featurization yourself\n",
    "\n",
    "from azureml.core import ScriptRunConfig\n",
    "import datetime\n",
    "\n",
    "# Define Compute Cluster to use\n",
    "compute_target = 'local'\n",
    "source_directory = './scripts'\n",
    "script_name = 'diabetesDockerRegressionTrainingAutoML.py'\n",
    "dataset_name = 'Diabetes Sample Full Transform'\n",
    "# set output file name like 'DecisionTreeRegressor_Diabetes_Docker-2022-04-17 21:40:36.114550.pkl'\n",
    "suffix = 'local-' + str(datetime.datetime.now())\n",
    "suffix = suffix.replace(' ', '_') # Clean up datetimestamp\n",
    "suffix = suffix.replace(':', '-') \n",
    "out_model_file_name = 'AutoMLRegression_BestModel_Diabetes_Docker_{}.pkl'.format(suffix)\n",
    "# set output file name like 'DecisionTreeRegressor_Diabetes_Docker-2022-04-17 21:40:36.114550.pkl'\n",
    "\n",
    "script_arguments = [\n",
    "\"--tenant-id\", tenant_id,\n",
    "\"--ws-name\", ws_name,\n",
    "\"--subscription-id\", subscription_id,\n",
    "\"--resource-group\", resource_group,\n",
    "\"--datastore-name\", datastore_name,\n",
    "\"--out-model-file-name\", out_model_file_name,\n",
    "\"--numeric-feature-names\", numericFeatureNamesEncoded,\n",
    "# Pass list encoded as a comma-separated string, containing the raw feature names\n",
    "# like '[\"a\",\"b\"]'\n",
    "\"--categoric-feature-names\", categoricFeatureNamesEncoded,\n",
    "# Pass list encoded as a comma-separated string, containing the name of each dataset (X_train, X_test, y_train, y_test)\n",
    "# like '[\"a\",\"b\"]'\n",
    "\"--x-train-test-y-train-test-combined-train-test\", trainTestDataSetNamesEncoded \n",
    "]\n",
    "print(\"ScriptRunConfig arguments: \", script_arguments)\n",
    "scriptRunConfig = ScriptRunConfig(\n",
    "        source_directory=source_directory,\n",
    "        script=script_name,\n",
    "        arguments=script_arguments,\n",
    "        environment=registered_docker_env,\n",
    "        docker_runtime_config=docker_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit Experiment Run to Docker environment\n",
    "#\n",
    "# (see more on use of Docker environment: \n",
    "#   https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-local/train-on-local.ipynb\n",
    "# )\n",
    "import subprocess\n",
    "# TODO? Add minconda bin to path within docker container? \"/home/johna/miniconda3/bin:\"\n",
    "\n",
    "# import getpass\n",
    "# If you need, you can get a password from user input (Notebook pauses to show a prompt here)\n",
    "# password = getpass.getpass()\n",
    "\n",
    "# Check if Docker is installed and Linux containers are enabled\n",
    "if subprocess.run(\"docker -v\", shell=True).returncode == 0:\n",
    "    subprocess.run(\"service docker status\", shell=True)\n",
    "\n",
    "\n",
    "    # This StackOverflow page will help you run the docker commands with sudo if that is necessary for you: \n",
    "    # https://askubuntu.com/questions/155791/how-do-i-sudo-a-command-in-a-script-without-being-asked-for-a-password\n",
    "    #\n",
    "    # (NOTE: These snippets from the link will allow you to have the docker commands run with sudo)\n",
    "    #\n",
    "    # Create script with the command to run with sudo, for example:\n",
    "    # docker system info\n",
    "    #\n",
    "    # Run sudo chown and chmod commands to grant access of the file to root \n",
    "    # sudo chown root:root ~/docker_system_info.sh\n",
    "    # sudo chmod 700 ~/docker_system_info.sh\n",
    "    #\n",
    "    # Run sudo visudo and insert a line below the line `%sudo   ALL=(ALL:ALL) ALL`\n",
    "    # [username]  ALL=(ALL) NOPASSWD: /home/[username]/docker_system_info.sh\n",
    "    #\n",
    "    # Then call the python script with your python subprocess command\n",
    "    # p = subprocess.run('sudo ~/docker_system_info.sh', shell=True)\n",
    "\n",
    "    # out_docker_system_info = subprocess.run(\"docker system info\", shell=True)\n",
    "    out_docker_system_info = subprocess.check_output('~/docker_system_info.sh', shell=True).decode('ascii')\n",
    "    # out_docker_system_info = subprocess.check_output('sudo su && ~/docker_system_info.sh', shell=True).decode('ascii')\n",
    "    print(out_docker_system_info)\n",
    "        #           [Install Ubuntu](https://docs.docker.com/engine/install/ubuntu/)\n",
    "        #           [Uninstall Docker Engine](https://docs.docker.com/engine/install/ubuntu/#uninstall-docker-engine)\n",
    "        #           WARNING! When I ran this command there is a failure to uninstall and purge all docker engine apt-get packages `sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-compose-plugin`\n",
    "        #                   AND TO RESOLVE (EXTRA CAPS) !!!THIS!!! THING, I DID THE FOLLOWING...\n",
    "        #\n",
    "        # Get /usr/bin/docker: Permission denied? (was not enough for me)\n",
    "        #       See (https://adamtheautomator.com/docker-permission-denied/)\n",
    "        #           (perhaps I need apt dependencies added to Dockerfile string I passed(??))\n",
    "        #       Along with  (https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/environment/)\n",
    "        # TODO! update for my laptop: still not getting any logs past \"nvidia-docker is installed on the target. Using nvidia-docker for docker operations\"\n",
    "        # \n",
    "        # NOTE - NEXT... Try using custom image from your own Docker image repository\n",
    "        #                   (perhaps allow docker to run by johna user normally, without any black magic)\n",
    "        # \n",
    "        # LAST DITCH EFFORTS - If you are stuck, uninstall and install docker, then install different Linux version for WSL, then try reinstall WSL\n",
    "        #\n",
    "        # NOTE - How to get past error: (perhaps uninstall nvidia-docker)\n",
    "        #           nvidia-docker is installed on the target. Using nvidia-docker for docker operations.\n",
    "        # \n",
    "        # You may want to follow this guide to install the Docker engine into Ubuntu:\n",
    "        #           (https://docs.docker.com/engine/install/ubuntu/)\n",
    "        #   Post install steps:\n",
    "        #           (https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user)\n",
    "        #   Alternative way to install:\n",
    "        #            https://github.com/docker/docker-install\n",
    "        #\n",
    "        # Run Experiment in Docker environment\n",
    "        # (NOTE: If you get any errors, in AMLS go to Jobs -> Click Experiment then Run from list -> Look for \"Environment\", \n",
    "        #           There should be a hyperlink to a page for the Environment used for that run!\n",
    "        #           There should be a Docker Build Log you can access\n",
    "        #           You should be able to trigger a build of the Docker image from the Environment's main page\n",
    "        # )\n",
    "    experiment_name = 'Diabetes_Docker_Regression_Training_AutoMLScriptRun'\n",
    "    experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "    # NOTE: I previously got an error message including \"GPU\", because of a --gpu flag used the instructions at this link to get past that:\n",
    "    #           (https://docs.nvidia.com/cuda/wsl-user-guide/index.html)\n",
    "    # NOTE: If script is failing at authentication, follow this link for help:\n",
    "    #           (https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication)\n",
    "\n",
    "    # ScriptRunConfig usage to create a Run\n",
    "    ScriptRunConfig_run = experiment.submit(scriptRunConfig)\n",
    "    RunDetails(ScriptRunConfig_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (While Experiment script runs, Validate the Engineered Feature Explanation will function inside the script)\n",
    "\n",
    "# Split training features into numeric and categoric dataframes\n",
    "# numeric_X_test = pd.DataFrame(X_test[df_numeric_column_names], dtype=np.str, columns=df_numeric_column_names)\n",
    "# categoric_X_test = pd.DataFrame(X_test[df_categorical_column_names], dtype=np.str, columns=df_categorical_column_names)\n",
    "# # Fit and Run the numeric and categoric ColumnTransformers on the split dataframes to perform feature engineering\n",
    "# preprocessor.transformers[0][1].fit(numeric_X_test)\n",
    "# preprocessor.transformers[1][1].steps[0][1].fit(categoric_X_test)\n",
    "# numeric_X_test_preprocessed = preprocessor.transformers[0][1].transform(numeric_X_test)\n",
    "# numeric_X_test_preprocessed = pd.DataFrame(numeric_X_test_preprocessed, dtype=np.float, columns=df_numeric_column_names)\n",
    "# categoric_X_test_preprocessed = preprocessor.transformers[1][1].steps[0][1].transform(categoric_X_test)\n",
    "# # Fit OneHotEncoder\n",
    "# preprocessor.transformers[1][1].steps[1][1].fit(categoric_X_test_preprocessed)\n",
    "# # Get new One Hot Encoded column names\n",
    "# print(categoric_X_test_preprocessed)\n",
    "# print(df_categorical_column_names)\n",
    "df_encoded_categorical_column_names = preprocessor.transformers[1][1].steps[1][1].get_feature_names(df_categorical_column_names)\n",
    "# print(\"df_encoded_categorical_column_names\", df_encoded_categorical_column_names)\n",
    "# # Transform categoric, null-imputed features with fitted OneHotEncoder\n",
    "# categoric_X_test_preprocessed = preprocessor.transformers[1][1].steps[1][1].transform(categoric_X_test_preprocessed)\n",
    "# # Turn preprocessed categoric features into a DataFrame\n",
    "# categoric_X_test_preprocessed = pd.DataFrame(categoric_X_test_preprocessed, dtype=np.float64, columns=df_encoded_categorical_column_names)\n",
    "\n",
    "# # Combine the numeric DF with the categorical DF to submit to the AutoML training experiment\n",
    "# X_test_preprocessed_list = [numeric_X_test_preprocessed, categoric_X_test_preprocessed]\n",
    "# X_test_preprocessed = pd.concat(X_test_preprocessed_list, axis=1)\n",
    "\n",
    "\n",
    "# Save engineered features' names to create TabularExplainer with them\n",
    "engineeredFeatures=[*df_numeric_column_names, *df_encoded_categorical_column_names]\n",
    "print(engineeredFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Register best Model from the AutoML run\n",
    "description = \"Best AutoML Regression Run using Docker Environment and Diabetes Sample Data. This model limits Age values to 3 std from mean. This model sets BMI > 30 to 1, BMI <= 30 to 0.\"\n",
    "tags = {\"project\" : \"AutoML Book Diabetes (Docker)\", \"creator\": \"fox\", \"task\": \"regression\", \"dataset\": \"Diabetes Sample Full Transform\", \"metric\": \"normalized_root_mean_squared_error\"}\n",
    "\n",
    "\n",
    "# Attempt to register model once output model file is available\n",
    "from azureml.core import Model\n",
    "import sklearn\n",
    "import time\n",
    "while True:\n",
    "  try:\n",
    "    ScriptRunConfig_run.wait_for_completion()\n",
    "    # Register Model from the ScriptRunConfig_run\n",
    "    ScriptRunConfig_run.register_model(model_path='./outputs', model_name=out_model_file_name, description=description, tags=tags)\n",
    "    break\n",
    "  except:\n",
    "      print (\"encountered exception registering model output file, waiting and trying again...\") \n",
    "      time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and plot output Best Model metrics\n",
    "\n",
    "# Get all metris logged in the run\n",
    "metrics = ScriptRunConfig_run.get_metrics()\n",
    "print(\"metrics: \", metrics)\n",
    "\n",
    "# Get the metrics that were logged from the run of the training script\n",
    "print(\"metrics['r2']: \" + str(metrics['r2']))\n",
    "print(\"metrics['rsme']: \" + str(metrics['rsme']))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(metrics['r2'], metrics['rsme'], marker='o')\n",
    "plt.ylabel(\"rsme\")\n",
    "plt.xlabel(\"r2\")\n",
    "#\n",
    "# You can also list all the files that are associated with this run record\n",
    "#\n",
    "print(\"Here are the files associated with the Azure AutoML Run: \", ScriptRunConfig_run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Experiment Run that was ran, Get Global Explanations (Downloaded with the Experiment Run object!)\n",
    "# IMPORTANT: this step will fail until you copy the explanation_id values from the printed list of explanations\n",
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run(ScriptRunConfig_run)\n",
    "\n",
    "# Get model explanation data\n",
    "# TODO? There should be a way to download_model_explanation by comment instead of id.\n",
    "print(client.list_model_explanations())\n",
    "# IMPORTANT: this step will fail until you copy the explanation_id values from the printed list of explanations\n",
    "engineered_global_explanation_test = client.download_model_explanation(explanation_id='7e81718e-4481-4dcb-a269-0db977df3436')\n",
    "# engineered_global_explanation_train = client.download_model_explanation(explanation_id='ab8b34cd-6f16-4718-b148-08e2635110e4')\n",
    "# global_explanation = client.download_model_explanation(explanation_id='86861485-00ae-42f0-8bab-3ae41556c6a9')\n",
    "\n",
    "\n",
    "\n",
    "# Or only get the top k (e.g., 4) most important features with their importance values\n",
    "# explanation = client.download_model_explanation(top_k=4)\n",
    "\n",
    "global_importance_values = engineered_global_explanation_test.get_ranked_global_values()\n",
    "global_importance_names = engineered_global_explanation_test.get_ranked_global_names()\n",
    "print('global importance values: {}'.format(global_importance_values))\n",
    "print('global importance names: {}'.format(global_importance_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Model from AzureML and Visualize Explanations with it # TODO? Get this cell working\n",
    "from raiwidgets import ExplanationDashboard\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "import jinja2\n",
    "\n",
    "# print('Model.list(ws)', Model.list(ws))\n",
    "\n",
    "# Download the Model from Azure\n",
    "\n",
    "# Use Model.download and joblib.load()\n",
    "remote_model_obj = Model(ws, out_model_file_name)\n",
    "print('Name:', remote_model_obj.name)\n",
    "print('Version:', remote_model_obj.version)\n",
    "remote_model_path = remote_model_obj.download(exist_ok = True)\n",
    "downloaded_model = joblib.load(remote_model_path)\n",
    "\n",
    "# BEGIN Access \"Local Explanations\", uncomment these lines if you want to do that here...\n",
    "# (Local Explanation meaning \"of individual predictions\") \n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "# \"features\" and \"classes\" fields are optional\n",
    "explainer = TabularExplainer(downloaded_model,\n",
    "                             X_test,\n",
    "                             features=features)\n",
    "\n",
    "# Get explanation for the first few data points in the test set\n",
    "local_explanation = explainer.explain_local(X_test[0:5])\n",
    "# Sorted feature importance values and feature names\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()\n",
    "print('sorted_local_importance_names: ', sorted_local_importance_names)\n",
    "print('len(sorted_local_importance_names): ', len(sorted_local_importance_names))\n",
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()\n",
    "print('sorted_local_importance_values: ', sorted_local_importance_values)\n",
    "print('len(sorted_local_importance_values): ', len(sorted_local_importance_values))\n",
    "# COOL THING TO DO: Sometime could get local explanation of specific data points uploaded, downloaded, and visualized as well...\n",
    "# END Access \"Local Explanations\"\n",
    "\n",
    "# Visualize explanations\n",
    "# Be sure to pass dataset=(test feature columns Dataframe) and true_y=(test predicted column Dataframe)\n",
    "#       1) getting the raiwidgets thing working\n",
    "#       2) see README at https://github.com/interpretml/interpret\n",
    "ExplanationDashboard(engineered_global_explanation_test, downloaded_model, dataset=X_test, true_y=y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46ae9456b2737841401e3d3c7acdf327031e52ebe64544d23b06b17169cc2049"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit ('3.7.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
