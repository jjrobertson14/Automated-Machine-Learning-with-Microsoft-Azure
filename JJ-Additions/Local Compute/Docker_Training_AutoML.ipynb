{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Datastore, Environment\n",
    "from azureml.core import Experiment\n",
    "import azureml.interpret\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Workspace object from Azure\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "# You can find tenant id under azure active directory->properties\n",
    "tenant_id = '198c7d8c-e010-45ce-a018-ec2d9a33f58f'\n",
    "ia = InteractiveLoginAuthentication(tenant_id=tenant_id)\n",
    "ws_name = 'automlbook'\n",
    "subscription_id = '4d278f3d-b4fd-4fa2-86b6-d34b96bc888f'\n",
    "resource_group = 'Foxy_Resources'\n",
    "ws = Workspace.get(name=ws_name,\n",
    "                   subscription_id=subscription_id,\n",
    "                   resource_group=resource_group,\n",
    "                   auth=ia)\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Print statements to list available environments)\n",
    "# envs = Environment.list(workspace=ws)\n",
    "\n",
    "# for env in envs:\n",
    "    # if env.startswith(\"AzureML\"):\n",
    "        # print(\"Name\",env)\n",
    "        # if None != envs[env].python.conda_dependencies:\n",
    "            # print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datastore, try getting datastore via Workspace object\n",
    "datastore = Datastore.get_default(ws)\n",
    "datastore_name = 'workspaceblobstore'\n",
    "datastore = Datastore.get(ws, datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the datastore of the Workspace\n",
    "dataset_name = 'automlbook Titanic Training Data A'\n",
    "# dataset = Dataset.get_by_name(ws, dataset_name)\n",
    "dataset = Dataset.get_by_name(ws, dataset_name, version = 'latest')\n",
    "dataset_columns = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "\n",
    "# Show a sample of the data in the dataset\n",
    "dataset.take(10).to_pandas_dataframe()\n",
    "\n",
    "# Turn Dataset into Pandas Dataframe, it is to be preprocessed\n",
    "df = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (First way I did this, commented out) Preprocess numeric columns\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# df_column_names = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "# df_numeric_column_names = ['Age', 'Fare']\n",
    "\n",
    "# # BEGIN Create dataframe with numeric columns so that it contains all numbers that are preprocessed...\n",
    "\n",
    "# # For int column Age, Impute NaN numeric values, and Remove outliers\n",
    "# print('Before Removing outliers or Imputing null values, df[Age]: ', df['Age'])\n",
    "# ageMedian = np.nanmedian(df['Age'])\n",
    "# print('ageMedian: ', ageMedian)\n",
    "# df['Age'] = np.where(np.isnan(df['Age']), ageMedian, df['Age'])\n",
    "# print('Before Removing outliers and after Imputing null values, df[Age]: ', df['Age'])\n",
    "\n",
    "# # Calculate 3STD and Mean for Age\n",
    "# ageThreeSD = np.std(df['Age']) * 3\n",
    "# ageMean = np.mean(df['Age'])\n",
    "# ageOutlierThreshold = round(ageThreeSD + ageMean)\n",
    "# print('Age Outlier Threshold: ', ageOutlierThreshold)\n",
    "\n",
    "# # Remove Outliers by replacing all values above Threshold (3STD + Mean) with Threshold Value\n",
    "# df['Age'] = df['Age'].mask(df['Age'] > ageOutlierThreshold, ageOutlierThreshold)\n",
    "# print('After Removing outliers and Imputing null values, df[Age]: ', df['Age'])\n",
    "\n",
    "# # Copy df, keeping only Age column, set type of this df copy to float\n",
    "# df_age_column = pd.DataFrame(df['Age'], columns=['Age'])\n",
    "\n",
    "\n",
    "# # Copy df, keeping only float numeric columns, set type of this df copy to float\n",
    "# df_float_column_names = ['Fare']\n",
    "# print('df_float_column_names: ', df_float_column_names)\n",
    "# df_float_columns = pd.DataFrame(df[df_float_column_names], dtype=np.float, columns=df_float_column_names)\n",
    "\n",
    "# # Concatenate the numeric Data Frames to scale them\n",
    "# print('Before concatenation to df_numeric_columns, df[Age]: ', df['Age'])\n",
    "# print('Before concatenation to df_numeric_columns, df_age_column: ', df_age_column)\n",
    "# df_numeric_columns = pd.concat([df_age_column, df_float_columns], keys=df_numeric_column_names, axis=1)\n",
    "# print('concatenated df_numeric_columns: ', df_numeric_columns)\n",
    "\n",
    "\n",
    "# # Use StandardScaler or MinMaxScaler on Numeric/Non-Categorical columns split\n",
    "# scaler = StandardScaler().fit(df_numeric_columns)\n",
    "# print('scaler.mean_: ', scaler.mean_)\n",
    "# print('scaler.scale: ', scaler.scale_)\n",
    "\n",
    "# df_scaled_numeric_columns =  pd.DataFrame(scaler.transform(df_numeric_columns), columns=df_numeric_column_names)\n",
    "# print('df_scaled_numeric_columns: ', df_scaled_numeric_columns)\n",
    "# # Scaled data should have zero mean and unit variance, check with these prints:\n",
    "# print('df_scaled_numeric_columns.mean(axis=0): ', df_scaled_numeric_columns.mean(axis=0))\n",
    "# print('df_scaled_numeric_columns.std(axis=0)', df_scaled_numeric_columns.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (First way I did this, commented out) Preprocess categorical columns\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # BEGIN Create dataframe with categorical columns so that it contains all categorical data that is preprocessed...\n",
    "\n",
    "# # Copy df, keeping only categorical columns, and one-hot encode them\n",
    "# df_categorical_column_names_raw = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin', 'Embarked']\n",
    "# # print('df_categorical_column_names_raw: ', df_categorical_column_names_raw)\n",
    "# df_categorical_columns = pd.DataFrame(df[df_categorical_column_names_raw], dtype=np.str, columns=df_categorical_column_names_raw)\n",
    "# # print('df_categorical_columns: ', df_categorical_columns)\n",
    "# encoder = OneHotEncoder(drop='first', handle_unknown='error', sparse=False, ).fit(df_categorical_columns)\n",
    "# # print('encoder.categories: ', encoder.categories)\n",
    "# df_encoded_categorical_columns = pd.DataFrame(encoder.transform(df_categorical_columns))\n",
    "# df_encoded_categorical_columns.columns = encoder.get_feature_names(df_categorical_column_names_raw)\n",
    "# # print('df_encoded_categorical_columns: ', df_encoded_categorical_columns)\n",
    "# #   By default, the values each feature can take is inferred automatically from the dataset and can be found in the categories_ attribute:\n",
    "\n",
    "\n",
    "# # Combine the numeric DF with the categorical DF\n",
    "# dfs = [df['Survived'], df_scaled_numeric_columns, df_encoded_categorical_columns]\n",
    "# # print('Before concatenation to dfTyped, df[\\'Survived\\']: ', df['Survived'])\n",
    "# # print('Before concatenation to dfTyped, df[Age]: ', df['Age'])\n",
    "# # print('Before concatenation to dfTyped, df_numeric_columns: ', df_numeric_columns)\n",
    "# # print('Before concatenation to dfTyped, df_scaled_numeric_columns: ', df_scaled_numeric_columns)\n",
    "# ??? shouldn't column names be passed in to the keys list here?\n",
    "# dfTyped = pd.concat(dfs, axis=1, keys=['Survived', *df_scaled_numeric_columns, *df_encoded_categorical_columns])\n",
    "# # print('dfTyped: ', dfTyped)\n",
    "# # print('dfTyped[Age]: ', dfTyped['Age'])\n",
    "\n",
    "#         # - With sklearn.preprocessing, preprocess your Dataframes before training model in the Python Script\n",
    "#         #     - [Guide at SciKit Learn site](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "#         #     - Use OneHotEncoder\n",
    "#         #     - Use StandardScaler or  MinMaxScaler while you're at it\n",
    "#         #     - Don't worry about any other preprocessing to just get the training working\n",
    "#         #     - Strategy:\n",
    "#         #         - d Split dataframe into Numeric/Non-Categorial and Non-Numeric/Categorial columns\n",
    "#         #             - ! Use StandardScaler or MinMaxScaler on Numeric/Non-Categorical columns split\n",
    "#         #             - d Use OneHotEncoder on Non-Numeric/Categorical columns split\n",
    "\n",
    "# # Initial Data Frame is now preprocessed in dfPreprocessed\n",
    "# dfPreprocessed = dfTyped\n",
    "# # print('dfPreprocessed: ', dfPreprocessed)\n",
    "\n",
    "# # Split DataFrame for training now that it is pre-processed\n",
    "# target_column_name = 'Survived'\n",
    "# df_x = dfPreprocessed.drop([target_column_name], axis=1)\n",
    "# df_y = dfPreprocessed[target_column_name]\n",
    "# # print(\"See df_x\", df_x)\n",
    "# print(\"See df_y\", df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Dataframe to get one for Numeric columns and one for Categorical columns\n",
    "\n",
    "df_column_names = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "\n",
    "##### BEGIN Create dataframe with numeric columns so that it contains all numbers that are preprocessed...\n",
    "df_numeric_column_names = ['Age', 'Fare']\n",
    "# Copy df, keeping only float numeric columns, set type of this df copy to float\n",
    "df_float_column_names = ['Fare']\n",
    "df_float_columns = pd.DataFrame(df[df_float_column_names], dtype=np.float, columns=df_float_column_names)\n",
    "# # Copy df, keeping only integer Age column to leave as an integer\n",
    "df_integer_column = pd.DataFrame(df['Age'], columns=['Age'])\n",
    "# Concatenate the numeric DataFrames\n",
    "df_numeric_columns = pd.concat([df_integer_column, df_float_columns], axis=1)\n",
    "\n",
    "\n",
    "##### BEGIN Create dataframe with categorical columns so that it contains all categorical data that is preprocessed...\n",
    "df_categorical_column_names = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin', 'Embarked']\n",
    "# Copy df, keeping only categorical columns\n",
    "df_categorical_columns = pd.DataFrame(df[df_categorical_column_names], dtype=np.str, columns=df_categorical_column_names)\n",
    "\n",
    "print('concatenated df_numeric_columns: ', df_numeric_columns)\n",
    "print('df_categorical_columns: ', df_categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor to preprocess numeric and categorical columns (with Transfomer API via ColumnTransformer, including creation of an Explainer)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', one_hot_encoder)])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, df_numeric_column_names),\n",
    "        ('cat', categorical_transformer, df_categorical_column_names)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline (Then we have a full prediction pipeline)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "regressor = DecisionTreeClassifier()\n",
    "\n",
    "classifier_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', regressor)])\n",
    "# The classifier_pipeline is later de-pickled and used in the Experiment Run's python script\n",
    "\n",
    "# Now we have a full prediction pipeline.\n",
    "print(classifier_pipeline.__dict__)\n",
    "# Note that you may access classifier_pipeline ColumnTransformers as follows...\n",
    "# print(classifier_pipeline['preprocessor'].transformers[1][1][1].get_feature_names(df_categorical_column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the SciKit Learn Pipeline to pass it to the script\n",
    "# Link about pickling:\n",
    "# https://codefather.tech/blog/python-pickle/#:~:text=%20Python%20Pickle%3A%20Serialize%20Your%20Objects%20%20,The%20pickle%20module%20also%20allows%20to...%20More%20\n",
    "import pickle\n",
    "\n",
    "pickled_pipeline = pickle.dumps(classifier_pipeline)\n",
    "# print(pickle.loads(pickled_pipeline))\n",
    "\n",
    "with open('./scripts/resources/classifier_pipeline.pickle', 'wb') as file:\n",
    "    pickle.dump(classifier_pipeline, file)\n",
    "\n",
    "# You may copy this to the script and Unpickle the SciKit Pipline (that performs Transformation and Model Training)\n",
    "# with open('classifier_pipeline.pickle', 'rb') as file:\n",
    "#     unpickled_pipeline = pickle.load(file)\n",
    "#     print(unpickled_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Does nothing at the moment) Serialize the SciKit Learn Pipeline to pass it to the script (with simple_python_to_json)\n",
    "# I Quit this to use pickling just to get this working and avoid maintaining simple_python_to_json for now...\n",
    "#       lesson learned, \n",
    "#       ideally the code would serialize/deserialize every single Sklearn object type...\n",
    "# TODO? maybe add to simple_python_to_json get this cell working?\n",
    "# You may use code within to help you serialize\n",
    "#      (Contain the code behind the 'simple_python_to_json' reference )\n",
    "# https://robotfantastic.org/serializing-python-data-to-json-some-edge-cases.html\n",
    "# https://cmry.github.io/notes/serialize\n",
    "# import json\n",
    "# import simple_python_to_json\n",
    "# def serialize_dict(in_dictionary, name):\n",
    "#     print(classifier_pipeline.__dict__)\n",
    "#     for k, v in in_dictionary.items():\n",
    "#         print(\"k,v is: \", k, \", \", v)\n",
    "#         print(simple_python_to_json.serializer.json_to_data(simple_python_to_json.serializer.data_to_json(v)))\n",
    "#         in_dictionary[k] = simple_python_to_json.data_to_json(v)\n",
    "#     json.dump(in_dictionary, open(name + '.json', 'w'))\n",
    "\n",
    "# TODO? maybe finish changes to serializer.py to serialize this\n",
    "# serialize_dict(classifier_pipeline.__dict__, 'classifier_pipeline')\n",
    "\n",
    "# TODO? maybe copy below to script, make todo to get it working in script\n",
    "# Load the json and deserialize:\n",
    "\n",
    "# def deserialize(class_init, attr):\n",
    "#     for k, v in attr.items():\n",
    "#         setattr(class_init, k, sr.json_to_data(v))\n",
    "#     return class_init\n",
    "\n",
    "# Load and deserialize the json:\n",
    "# deserialized_classifier_pipeline = deserialize(Pipeline(), json.load(open('classifier_pipeline.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the numeric DF with the categorical DF\n",
    "# print(\"df['Survived'] is \", df['Survived'])\n",
    "# print(\"df_numeric_columns is \", df_numeric_columns)\n",
    "# print(\"df_numeric_columns.columns is \", df_numeric_columns.columns)\n",
    "# print(\"df_categorical_columns is \", df_categorical_columns)\n",
    "# print(\"df_categorical_columns.columns is \", df_categorical_columns.columns)\n",
    "\n",
    "# Concatenate dfs to get DataFrame of all columns to submit to the classifier_pipeline\n",
    "dfs = [df['Survived'], df_numeric_columns, df_categorical_columns]\n",
    "# print(\"dfs is\" + str(dfs))\n",
    "# print('Before concatenation to dfTyped, df[\\'Survived\\']: ', df['Survived'])\n",
    "# print('Before concatenation to dfTyped, df_numeric_columns: ', df_numeric_columns)\n",
    "dfTyped = pd.concat(dfs, axis=1)\n",
    "print('dfTyped: ', dfTyped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pre-transformation Data Frame into feature/target columns\n",
    "target_column_name = 'Survived'\n",
    "df_x = dfTyped.drop([target_column_name], axis=1)\n",
    "df_y = dfTyped[target_column_name].to_frame()\n",
    "# print(\"See df_x\", df_x)\n",
    "print(\"See df_y\", df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Validate the Pipeline will function once passed to the script)\n",
    "\n",
    "# Get the preprocessed Data Frame columns in a list\n",
    "one_hot_encoder.fit(df_categorical_columns)\n",
    "# Get new One Hot Encoded column names\n",
    "# df_encoded_categorical_column_names = one_hot_encoder.get_feature_names(df_categorical_column_names)\n",
    "# print(str(df_encoded_categorical_column_names))\n",
    "\n",
    "# (fail to) Get the preprocessed Data Frame\n",
    "# # dfPreprocessed = preprocessor.fit_transform(df_x_pre_transformation, df_y_pre_transformation)\n",
    "\n",
    "# Attempt to fit the preprocessor to the data, to validate it will function inside the script it is passed to\n",
    "preprocessor.fit(df_x, df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Pandas Dataframe of base df_x and df_y\n",
    "Dataset.Tabular.register_pandas_dataframe(df_x, datastore, \"Titanic Feature Column Data for train_test_split usage (LocalDocker notebook)\")\n",
    "Dataset.Tabular.register_pandas_dataframe(df_y, datastore, \"Titanic Target Column Data for train_test_split usage (LocalDocker notebook)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test data, register the resulting Datasets with Azure\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "# What you need to pass to train_test_split...\n",
    "# ... I need X and Y dataframe, X just with target missing, Y just with target column present\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "# print(\"See y_test\", y_test)\n",
    "print(\"See y_test.columns.tolist()\", str(y_test.columns.tolist()))\n",
    "# print(\"See y_test.values.tolist() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.tolist())\n",
    "# print(\"See y_test.values.ravel() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.ravel())\n",
    "# print(\"See y_test.values.tolist().flatten() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.flatten())\n",
    "\n",
    "# Register the splits\n",
    "X_train_registered_name = \"Titanic Feature Column Data for training (LocalDocker notebook)\"\n",
    "X_test_registered_name = \"Titanic Feature Column Data for testing (LocalDocker notebook)\"\n",
    "y_train_registered_name = \"Titanic Target Column Data for training (LocalDocker notebook)\"\n",
    "y_test_registered_name = \"Titanic Target Column Data for testing (LocalDocker notebook)\"\n",
    "Dataset.Tabular.register_pandas_dataframe(X_train, datastore, X_train_registered_name)\n",
    "Dataset.Tabular.register_pandas_dataframe(X_test, datastore, X_test_registered_name)\n",
    "Dataset.Tabular.register_pandas_dataframe(y_train, datastore, y_train_registered_name)\n",
    "Dataset.Tabular.register_pandas_dataframe(y_test, datastore, y_test_registered_name)\n",
    "xTrainTestYTrainTest = [X_train_registered_name, X_test_registered_name, y_train_registered_name, y_test_registered_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names and create TabularExplainer with them\n",
    "features=[*df_numeric_column_names, *df_categorical_column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Experiment script arguments list into a string like '[\"a\",\"b\"]'\n",
    "\n",
    "# Encode numeric column names list\n",
    "temp_column_names = df_numeric_column_names.copy()\n",
    "for x in range(len(temp_column_names)):\n",
    "        temp_column_names[x] = '\"{}\"'.format(temp_column_names[x])\n",
    "numericFeatureNamesEncoded = \"[{}]\".format(\",\".join(temp_column_names))\n",
    "# print(\"numericFeatureNamesEncoded:\", numericFeatureNamesEncoded)\n",
    "\n",
    "# Encode categoric column names list\n",
    "temp_column_names = df_categorical_column_names.copy()\n",
    "for x in range(len(temp_column_names)):\n",
    "        temp_column_names[x] = '\"{}\"'.format(temp_column_names[x])\n",
    "categoricFeatureNamesEncoded = \"[{}]\".format(\",\".join(temp_column_names))\n",
    "# print(\"categoricFeatureNamesEncoded:\", categoricFeatureNamesEncoded)\n",
    "\n",
    "# Encode split dataset names list\n",
    "for x in range(len(xTrainTestYTrainTest)):\n",
    "        xTrainTestYTrainTest[x] = '\"{}\"'.format(xTrainTestYTrainTest[x])\n",
    "xTrainTestYTrainTestEncoded = \"[{}]\".format(\",\".join(xTrainTestYTrainTest))\n",
    "print(\"splitDatasetNamesEncoded:\", xTrainTestYTrainTestEncoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Local Docker Environment up (with System Managed Dependencies, via Conda)\n",
    "#\n",
    "# Learn about Environment and how to use a Docker Environment here:\n",
    "#       https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments\n",
    "#       https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment(class)?view=azure-ml-py\n",
    "#       ! IMPORTANT: https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/environment/\n",
    "#\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "\n",
    "docker_env = Environment(\"docker-env\")\n",
    "# Editing a run configuration property on-fly.\n",
    "docker_env.python.user_managed_dependencies = False\n",
    "# Use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param.\n",
    "#           https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfig.dockerconfiguration?view=azure-ml-py\n",
    "docker_config = DockerConfiguration(use_docker=True)\n",
    "print(\"initial base image from base docker-env Environment: \", docker_env.docker.base_image)\n",
    "\n",
    "# Specify docker steps as a string. \n",
    "dockerfile = r\"\"\"\n",
    "FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220314.v1\n",
    "\n",
    "ARG DEBIAN_FRONTEND=noninteractive\n",
    "RUN apt-get update -y && apt-get upgrade -y &&\\\n",
    "    apt-get install -y build-essential \\\n",
    "                       cmake \\\n",
    "                       curl \\\n",
    "                       gfortran \\\n",
    "                       git \\\n",
    "                       jupyter \\\n",
    "                       libatlas-base-dev \\\n",
    "                       libblas-dev \\\n",
    "                       libbz2-dev \\\n",
    "                       libffi-dev \\\n",
    "                       libgdbm-dev \\\n",
    "                       liblapack-dev \\\n",
    "                       liblzma-dev \\\n",
    "                       libncurses5-dev \\\n",
    "                       libncursesw5-dev \\\n",
    "                       libreadline-dev \\\n",
    "                       libsqlite3-dev \\\n",
    "                       libssl-dev \\\n",
    "                       libxml2-dev \\\n",
    "                       libxmlsec1-dev \\\n",
    "                       llvm \\\n",
    "                       lzma \\\n",
    "                       lzma-dev \\\n",
    "                       make \\\n",
    "                       tcl-dev \\\n",
    "                       tk-dev \\\n",
    "                       wget \\\n",
    "                       xz-utils \\\n",
    "                       zlib1g-dev\n",
    "\n",
    "RUN echo \"Hello from custom container!\" > ~/hello.txt\n",
    "\"\"\"\n",
    "\n",
    "# NOTE: you can pass Dockerfile string to docker build command via stdin like this:\n",
    "#\n",
    "# sudo docker build -t myimage:latest -<<EOF\n",
    "# FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220314.v1\n",
    "# RUN echo \"hello world\"\n",
    "# EOF\n",
    "\n",
    "\n",
    "# Set base image to None, because the image is defined by dockerfile.\n",
    "docker_env.docker.base_image = None\n",
    "# Use the Dockerfile string and build image based on it with this code from above (move it down here)\n",
    "docker_env.docker.base_dockerfile = dockerfile\n",
    "\n",
    "#       For help, try reading this\n",
    "#                    - https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/environment/#custom-docker-image--dockerfile\n",
    "#       May also try using other base Docker images from these container registries\n",
    "#                - https://github.com/microsoft/containerregistry\n",
    "#                - https://github.com/Azure/AzureML-Containers\n",
    "# env.docker.base_image = '<image-name>'\n",
    "# env.docker.base_image_registry.address = '<container-registry-address>'\n",
    "# env.docker.base_image_registry.username = '<acr-username>'\n",
    "# env.docker.base_image_registry.password = os.environ.get(\"CONTAINER_PASSWORD\")\n",
    "#\n",
    "# TODO? use this code to set username and password:\n",
    "# # Retrieve username and password from the workspace key vault\n",
    "#       env.docker.base_image_registry.username = ws.get_default_keyvault().get_secret(\"username\")  \n",
    "#       env.docker.base_image_registry.password = ws.get_default_keyvault().get_secret(\"password\")\n",
    "\n",
    "\n",
    "# Specify conda dependencies with scikit-learn\n",
    "conda_packages = ['pip',\n",
    "                  'pyspark',\n",
    "                  'scikit-learn'\n",
    "                 ]\n",
    "pip_packages =   ['azureml.interpret',\n",
    "                  'azureml-dataset-runtime',\n",
    "                  'jinja2',\n",
    "                  'MarkupSafe',\n",
    "                  'raiwidgets'\n",
    "                 ]\n",
    "                 \n",
    "condaDependencies = CondaDependencies.create(conda_packages=conda_packages, pip_packages=pip_packages)\n",
    "docker_env.python.conda_dependencies = condaDependencies\n",
    "# NOTE: can set this:\n",
    "#           docker_env.python.interpreter_path = \"/opt/miniconda/bin/python\"\n",
    "\n",
    "# Register the Docker Environment and build the Docker image locally\n",
    "registered_docker_env = docker_env.register(ws)\n",
    "print(registered_docker_env)\n",
    "registered_docker_env.build_local(ws, useDocker=False, pushImageToWorkspaceAcr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to run Training Experiment in Docker Environment (With Docker running on local device)\n",
    "\n",
    "from azureml.core import ScriptRunConfig\n",
    "import datetime\n",
    "\n",
    "# Experiment\n",
    "experiment_name = 'LocalDocker_Training_AutoML'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "# Define Compute Cluster to use\n",
    "compute_target = 'local'\n",
    "source_directory = './scripts'\n",
    "script_name = 'localDockerTrainingAutoML.py'\n",
    "dataset_name = 'automlbook Titanic Training Data A'\n",
    "# set output file name like 'DecisionTreeClassifier_Titanic_LocalDocker-2022-04-17 21:40:36.114550.pkl'\n",
    "suffix = 'local-' + str(datetime.datetime.now())\n",
    "suffix = suffix.replace(' ', '_') # Clean up datetimestamp\n",
    "suffix = suffix.replace(':', '-') \n",
    "out_model_file_name = 'DecisionTreeClassifier_Titanic_LocalDocker_{}.pkl'.format(suffix)\n",
    "# set output file name like 'DecisionTreeClassifier_Titanic_LocalDocker-2022-04-17 21:40:36.114550.pkl'\n",
    "\n",
    "script_arguments = [\n",
    "\"--tenant-id\", tenant_id,\n",
    "\"--ws-name\", ws_name,\n",
    "\"--subscription-id\", subscription_id,\n",
    "\"--resource-group\", resource_group,\n",
    "\"--datastore-name\", datastore_name,\n",
    "\"--out-model-file-name\", out_model_file_name,\n",
    "\"--numeric-feature-names\", numericFeatureNamesEncoded,\n",
    "# Pass list encoded as a comma-separated string, containing the raw feature names\n",
    "# like '[\"a\",\"b\"]'\n",
    "\"--categoric-feature-names\", categoricFeatureNamesEncoded,\n",
    "# Pass list encoded as a comma-separated string, containing the name of each dataset (X_train, X_test, y_train, y_test)\n",
    "# like '[\"a\",\"b\"]'\n",
    "\"--x-train-test-y-train-test\", xTrainTestYTrainTestEncoded \n",
    "]\n",
    "print(\"ScriptRunConfig arguments: \", script_arguments)\n",
    "scriptRunConfig = ScriptRunConfig(\n",
    "        source_directory=source_directory,\n",
    "        script=script_name,\n",
    "        arguments=script_arguments,\n",
    "        environment=docker_env,\n",
    "        docker_runtime_config=docker_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit Experiment Run to Docker environment\n",
    "#\n",
    "# (see more on use of Docker environment: \n",
    "#   https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-local/train-on-local.ipynb\n",
    "# )\n",
    "import subprocess\n",
    "# TODO? Add minconda bin to path within docker container? \"/home/johna/miniconda3/bin:\"\n",
    "\n",
    "subprocess.run(\"whoami\", shell=True)\n",
    "\n",
    "# import getpass\n",
    "# If you need, you can get a password from user input (Notebook pauses to show a prompt here)\n",
    "# password = getpass.getpass()\n",
    "\n",
    "scriptRunConfig.run_config.environment = docker_env\n",
    "# Check if Docker is installed and Linux containers are enabled\n",
    "if subprocess.run(\"docker -v\", shell=True).returncode == 0:\n",
    "    subprocess.run(\"service docker status\", shell=True)\n",
    "\n",
    "\n",
    "    # This StackOverflow page will help you run the docker commands with sudo if that is necessary for you: \n",
    "    # https://askubuntu.com/questions/155791/how-do-i-sudo-a-command-in-a-script-without-being-asked-for-a-password\n",
    "    #\n",
    "    # (NOTE: These snippets from the link will allow you to have the docker commandsd run with sudo)\n",
    "    #\n",
    "    # Create script with the command to run with sudo, for example:\n",
    "    # docker system info\n",
    "    #\n",
    "    # Run sudo chown and chmod commands to grant access of the file to root \n",
    "    # sudo chown root:root ~/docker_system_info.sh\n",
    "    # sudo chmod 700 ~/docker_system_info.sh\n",
    "    #\n",
    "    # Run sudo visudo and insert a line below the line `%sudo   ALL=(ALL:ALL) ALL`\n",
    "    # [username]  ALL=(ALL) NOPASSWD: /home/[username]/docker_system_info.sh\n",
    "    #\n",
    "    # Then call the python script with your python subprocess command\n",
    "    # p = subprocess.run('sudo ~/docker_system_info.sh', shell=True)\n",
    "\n",
    "\n",
    "    out = subprocess.check_output('sudo ~/docker_system_info.sh', shell=True).decode('ascii')\n",
    "    if not \"OSType: linux\" in out:\n",
    "        print(\"Switch Docker engine to use Linux containers.\")\n",
    "    else:\n",
    "        # TODO get this experiment run to work, it is hanging, perhaps I need apt dependencies added to Dockerfile string I passed(??)\n",
    "        # TODO update: still not getting any logs past \"nvidia-docker is installed on the target. Using nvidia-docker for docker operations\"\n",
    "        # TODO update: perhaps it is the user that is being used to run docker command?\n",
    "        #                   (perhaps allow docker to run by johna user normally, without any black magic)\n",
    "        # TODO!!!!!! update: perhaps add authentication for getting the image from whatever registry it is being gotten from\n",
    "        #                   See: https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/environment/\n",
    "        #\n",
    "        AutoML_run = experiment.submit(scriptRunConfig)\n",
    "        RunDetails(AutoML_run).show()\n",
    "else:\n",
    "    print(\"Docker engine is not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO DELETE THIS BACKUP CODE\n",
    "# Submit Experiment Run to Docker environment\n",
    "#\n",
    "# (see more on use of Docker environment: \n",
    "#   https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-local/train-on-local.ipynb\n",
    "# )\n",
    "# import subprocess\n",
    "# # TODO? Add minconda bin to path within docker container? \"/home/johna/miniconda3/bin:\"\n",
    "\n",
    "# subprocess.run(\"whoami\", shell=True)\n",
    "\n",
    "# import getpass\n",
    "# import os\n",
    "# # TODO run failing commands with sudo like this.) Get password from user input (Notebook pauses to show a prompt here)\n",
    "# password = getpass.getpass()\n",
    "\n",
    "# # command = \"sudo -S docker system info\" # can be any command but don't forget -S as it enables input from stdin\n",
    "# # os.popen(command, 'w').write(password+'\\n') # newline char is important otherwise prompt will wait for you to manually perform newline\n",
    "\n",
    "# scriptRunConfig.run_config.environment = docker_env\n",
    "# # Check if Docker is installed and Linux containers are enabled\n",
    "# if subprocess.run(\"docker -v\", shell=True).returncode == 0:\n",
    "#     subprocess.run(\"service docker status\", shell=True)\n",
    "#     # TODO FIX THIS perhaps have docker run as a user, or can I pass a username/password?\n",
    "#     command = \"sudo -S docker system info\" # can be any command but don't forget -S as it enables input from stdin\n",
    "#     print(os.popen(command, 'w').write(password+'\\n')) # newline char is important otherwise prompt will wait for you to manually perform newline\n",
    "#     out = subprocess.check_output(\"sudo docker system info\", shell=True).decode('ascii')\n",
    "#     print(\"out: \", out)\n",
    "#     if not \"OSType: linux\" in out:\n",
    "#         print(\"Switch Docker engine to use Linux containers.\")\n",
    "#     else:\n",
    "#         AutoML_run = experiment.submit(scriptRunConfig)\n",
    "#         RunDetails(AutoML_run).show()\n",
    "# else:\n",
    "#     print(\"Docker engine is not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (While Experiment script runs, Validate the Engineered Feature Explanation will function inside the script)\n",
    "\n",
    "# Split training features into numeric and categoric dataframes\n",
    "numeric_X_test = pd.DataFrame(X_test[df_numeric_column_names], dtype=np.str, columns=df_numeric_column_names)\n",
    "categoric_X_test = pd.DataFrame(X_test[df_categorical_column_names], dtype=np.str, columns=df_categorical_column_names)\n",
    "# Fit and Run the numeric and categoric ColumnTransformers on the split dataframes to perform feature engineering\n",
    "preprocessor.transformers[0][1].fit(numeric_X_test)\n",
    "preprocessor.transformers[1][1].steps[0][1].fit(categoric_X_test)\n",
    "numeric_X_test_preprocessed = preprocessor.transformers[0][1].transform(numeric_X_test)\n",
    "numeric_X_test_preprocessed = pd.DataFrame(numeric_X_test_preprocessed, dtype=np.float, columns=df_numeric_column_names)\n",
    "categoric_X_test_preprocessed = preprocessor.transformers[1][1].steps[0][1].transform(categoric_X_test)\n",
    "# Fit OneHotEncoder\n",
    "preprocessor.transformers[1][1].steps[1][1].fit(categoric_X_test_preprocessed)\n",
    "# Get new One Hot Encoded column names\n",
    "print(categoric_X_test_preprocessed)\n",
    "print(df_categorical_column_names)\n",
    "df_encoded_categorical_column_names = preprocessor.transformers[1][1].steps[1][1].get_feature_names(df_categorical_column_names)\n",
    "print(\"df_encoded_categorical_column_names\", df_encoded_categorical_column_names)\n",
    "# Transform categoric, null-imputed features with fitted OneHotEncoder\n",
    "categoric_X_test_preprocessed = preprocessor.transformers[1][1].steps[1][1].transform(categoric_X_test_preprocessed)\n",
    "# Turn preprocessed categoric features into a DataFrame\n",
    "categoric_X_test_preprocessed = pd.DataFrame(categoric_X_test_preprocessed, dtype=np.float64, columns=df_encoded_categorical_column_names)\n",
    "\n",
    "# Combine the numeric DF with the categorical DF to submit to the classifier_pipeline\n",
    "X_test_preprocessed_list = [numeric_X_test_preprocessed, categoric_X_test_preprocessed]\n",
    "X_test_preprocessed = pd.concat(X_test_preprocessed_list, axis=1)\n",
    "\n",
    "\n",
    "# Save engineered features' names to create TabularExplainer with them\n",
    "engineeredFeatures=[*df_numeric_column_names, *df_encoded_categorical_column_names]\n",
    "\n",
    "# Fit the model\n",
    "classifier_pipeline.steps[-1][1].fit(X_test_preprocessed, y_test)\n",
    "# Explain in terms of engineered features\n",
    "# NOTE: classifier_pipeline.steps[-1][1] contains the Model\n",
    "# NOTE: \"features\" and \"classes\" fields are optional for TabularExplainers\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "engineered_explainer = TabularExplainer(classifier_pipeline.steps[-1][1],\n",
    "                                     initialization_examples=X_test_preprocessed,\n",
    "                                     features=engineeredFeatures)\n",
    "# Explain results with this Explainer and upload the Explanation...\n",
    "\n",
    "# Get Global Explanations of raw features, global as in 'of total data'...\n",
    "# You can use the training data or the test data here, but test data would allow you to use Explanation Exploration\n",
    "# print(\"X_test, line value before engineered_explainer.explain_global: \\n\" + str(X_test))\n",
    "global_explanation = engineered_explainer.explain_global(X_test_preprocessed, y_test)\n",
    "# If you used the PFIExplainer in the previous step, use the next line of code instead\n",
    "# global_explanation = engineered_explainer.explain_global(X_test, true_labels=y_train)\n",
    "# Sorted feature importance values and feature names\n",
    "sorted_global_importance_values = global_explanation.get_ranked_global_values()\n",
    "sorted_global_importance_names = global_explanation.get_ranked_global_names()\n",
    "globalFeatureExplanations = dict(zip(sorted_global_importance_names, sorted_global_importance_values))\n",
    "print('globalFeatureExplanations: ', globalFeatureExplanations)\n",
    "# Alternatively, you can print out a dictionary that holds the top K feature names and values\n",
    "print('global_explanation.get_feature_importance_dict(): ', global_explanation.get_feature_importance_dict())\n",
    "\n",
    "# Upload the explanation in terms of engineered features\n",
    "from azureml.interpret import ExplanationClient\n",
    "client = ExplanationClient.from_run(AutoML_run)\n",
    "client.upload_model_explanation(global_explanation, true_ys=y_test.values.ravel(), comment='global explanation: test dataset features, engineered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Model from the AutoML_run\n",
    "description = \"Best LocalDocker AutoML Regression Run using Titanic Sample Data.\"\n",
    "tags = {\n",
    "  \"project\" : \"LocalDocker Training AutoML\", \n",
    "  \"creator\": \"fox\", \n",
    "  \"task\": \"classification\", \n",
    "  \"dataset\": \"automlbook Titanic Training Data A\", \n",
    "  \"metric\": \"normalized_root_mean_squared_error\"\n",
    "}\n",
    "\n",
    "# Attempt to register model once output model file is available\n",
    "from azureml.core import Model\n",
    "import sklearn\n",
    "import time\n",
    "# TODO get this working\n",
    "AutoML_run.wait_for_completion(show_output=True)\n",
    "while True:\n",
    "  try:\n",
    "    AutoML_run.register_model(model_path='./outputs', model_name=out_model_file_name, description=description, tags=tags,\n",
    "                            model_framework=Model.Framework.SCIKITLEARN, # Framework used to create the model.\n",
    "                            model_framework_version=sklearn.__version__)  # Version of scikit-learn used to create the model.)\n",
    "    break\n",
    "  except:\n",
    "      print (\"encountered exception registering model output file, waiting and trying again...\") \n",
    "      time.sleep(60)\n",
    "# Set output file name like 'DecisionTreeClassifier_Titanic_LocalDocker-2022-04-17 21:40:36.114550.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Now done in script instead of this Notebook) Upload global model explanation data...\n",
    "# from azureml.interpret import ExplanationClient\n",
    "\n",
    "# The explanation can then be downloaded on any compute\n",
    "# Multiple explanations can be uploaded\n",
    "# print(\"y_test value the line before client.upload_model_explanation(): \\n\" + str(y_test))\n",
    "# print(\"y_test.values.ravel() value passed as true_ys to client.upload_model_explanation(): \\n\" + str(y_test.values.ravel()))\n",
    "# client = ExplanationClient.from_run(AutoML_run)\n",
    "# client.upload_model_explanation(global_explanation, true_ys=y_test.values.ravel(), comment='global explanation: all features')\n",
    "\n",
    "# Or you can only upload the explanation object with the top k feature info with this...\n",
    "# client.upload_model_explanation(global_explanation, top_k=2, comment='global explanation: Only top 2 features')\n",
    "# END Upload global model explanation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Experiment Run that was ran, Get Global Explanations (Downloaded with the Experiment Run object!)\n",
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run(AutoML_run)\n",
    "\n",
    "# Get model explanation data\n",
    "# TODO? There should be a way to download_model_explanation by comment instead of id.\n",
    "print(client.list_model_explanations())\n",
    "# NOTE: this step will fail until you copy the explanation_id values from the printed list of explanations\n",
    "engineered_global_explanation_test = client.download_model_explanation(explanation_id='2ef948ba-9b01-453e-80da-f1c498022e5b')\n",
    "engineered_global_explanation_train = client.download_model_explanation(explanation_id='846c159e-5ee7-4c79-8994-8757055de83a')\n",
    "# TODO? Do something with the Engineered Feature explanations here\n",
    "global_explanation = client.download_model_explanation(explanation_id='6eac21c3-891c-4d43-beae-62eb7b5b9965')\n",
    "\n",
    "\n",
    "\n",
    "# Or only get the top k (e.g., 4) most important features with their importance values\n",
    "# explanation = client.download_model_explanation(top_k=4)\n",
    "\n",
    "global_importance_values = global_explanation.get_ranked_global_values()\n",
    "global_importance_names = global_explanation.get_ranked_global_names()\n",
    "print('global importance values: {}'.format(global_importance_values))\n",
    "print('global importance names: {}'.format(global_importance_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Model from AzureML and Visualize Explanations with it\n",
    "from raiwidgets import ExplanationDashboard\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "import jinja2\n",
    "\n",
    "# print('Model.list(ws)', Model.list(ws))\n",
    "\n",
    "# Download the Model from Azure\n",
    "\n",
    "# Try 1: just Model constructor and joblib.load()\n",
    "# downloaded_model = Model(ws, out_model_file_name)\n",
    "# joblib.load(downloaded_model)\n",
    "\n",
    "# Try 2: Use Model.get_model_path and joblib.load()\n",
    "# remote_model_path = Model.get_model_path(out_model_file_name, _workspace=ws)\n",
    "# downloaded_model = joblib.load(remote_model_path)\n",
    "\n",
    "# Try 3: Use Model.download and joblib.load()\n",
    "remote_model_obj = Model(ws, out_model_file_name)\n",
    "print('Name:', remote_model_obj.name)\n",
    "print('Version:', remote_model_obj.version)\n",
    "remote_model_path = remote_model_obj.download(exist_ok = True)\n",
    "downloaded_model = joblib.load(remote_model_path)\n",
    "\n",
    "# BEGIN Access \"Local Explanations\", uncomment these lines if you want to do that here...\n",
    "# (Local Explanation meaning \"of individual predictions\") \n",
    "# from interpret.ext.blackbox import TabularExplainer\n",
    "# \"features\" and \"classes\" fields are optional\n",
    "# explainer = TabularExplainer(downloaded_model,\n",
    "                            #  X_train,\n",
    "                            #  features=features)\n",
    "#\n",
    "# Get explanation for the first few data points in the train set\n",
    "# local_explanation = explainer.explain_local(X_train[0:5])\n",
    "# Sorted feature importance values and feature names\n",
    "# sorted_local_importance_names = local_explanation.get_ranked_local_names()\n",
    "# print('sorted_local_importance_names: ', sorted_local_importance_names)\n",
    "# print('len(sorted_local_importance_names): ', len(sorted_local_importance_names))\n",
    "# sorted_local_importance_values = local_explanation.get_ranked_local_values()\n",
    "# print('sorted_local_importance_values: ', sorted_local_importance_values)\n",
    "# print('len(sorted_local_importance_values): ', len(sorted_local_importance_values))\n",
    "# COOL THING TO DO: Sometime could get local explanation of specific data points uploaded, downloaded, and visualized as well...\n",
    "# END Access \"Local Explanations\"\n",
    "\n",
    "# Visualize explanations\n",
    "# Be sure to pass dataset=(train feature columns Dataframe) and true_y=(train predicted column Dataframe)\n",
    "#\n",
    "# TODO Get base statistics to appear in visualization's Model Performance tab: \n",
    "# TODO      (the accuracy, precision, f1 scores, false positive rates, false negative rates...)\n",
    "#\n",
    "# TODO? Show both raw and engineered features from raiwidgets ExplanationDashboard like this.\n",
    "#\n",
    "#       1) getting the raiwidgets thing working\n",
    "#       2) see README at https://github.com/interpretml/interpret\n",
    "ExplanationDashboard(global_explanation, downloaded_model, dataset=X_train, true_y=y_train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46ae9456b2737841401e3d3c7acdf327031e52ebe64544d23b06b17169cc2049"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit ('3.7.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
