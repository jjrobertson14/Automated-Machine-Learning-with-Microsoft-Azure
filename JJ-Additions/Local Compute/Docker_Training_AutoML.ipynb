{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.40.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Datastore, Environment\n",
    "from azureml.core import Experiment\n",
    "import azureml.interpret\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automlbook\n",
      "Foxy_Resources\n",
      "centralus\n",
      "4d278f3d-b4fd-4fa2-86b6-d34b96bc888f\n"
     ]
    }
   ],
   "source": [
    "# Get the Workspace object from Azure\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "# You can find tenant id under azure active directory->properties\n",
    "tenant_id = '198c7d8c-e010-45ce-a018-ec2d9a33f58f'\n",
    "ia = InteractiveLoginAuthentication(tenant_id=tenant_id)\n",
    "ws_name = 'automlbook'\n",
    "subscription_id = '4d278f3d-b4fd-4fa2-86b6-d34b96bc888f'\n",
    "resource_group = 'Foxy_Resources'\n",
    "ws = Workspace.get(name=ws_name,\n",
    "                   subscription_id=subscription_id,\n",
    "                   resource_group=resource_group,\n",
    "                   auth=ia)\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Print statements to list available environments)\n",
    "# envs = Environment.list(workspace=ws)\n",
    "\n",
    "# for env in envs:\n",
    "    # if env.startswith(\"AzureML\"):\n",
    "        # print(\"Name\",env)\n",
    "        # if None != envs[env].python.conda_dependencies:\n",
    "            # print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datastore, try getting datastore via Workspace object\n",
    "datastore = Datastore.get_default(ws)\n",
    "datastore_name = 'workspaceblobstore'\n",
    "datastore = Datastore.get(ws, datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the datastore of the Workspace\n",
    "dataset_name = 'automlbook Titanic Training Data A'\n",
    "# dataset = Dataset.get_by_name(ws, dataset_name)\n",
    "dataset = Dataset.get_by_name(ws, dataset_name, version = 'latest')\n",
    "dataset_columns = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "\n",
    "# Show a sample of the data in the dataset\n",
    "dataset.take(10).to_pandas_dataframe()\n",
    "\n",
    "# Turn Dataset into Pandas Dataframe, it is to be preprocessed\n",
    "df = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (First way I did this, commented out) Preprocess numeric columns\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# df_column_names = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "# df_numeric_column_names = ['Age', 'Fare']\n",
    "\n",
    "# # BEGIN Create dataframe with numeric columns so that it contains all numbers that are preprocessed...\n",
    "\n",
    "# # For int column Age, Impute NaN numeric values, and Remove outliers\n",
    "# print('Before Removing outliers or Imputing null values, df[Age]: ', df['Age'])\n",
    "# ageMedian = np.nanmedian(df['Age'])\n",
    "# print('ageMedian: ', ageMedian)\n",
    "# df['Age'] = np.where(np.isnan(df['Age']), ageMedian, df['Age'])\n",
    "# print('Before Removing outliers and after Imputing null values, df[Age]: ', df['Age'])\n",
    "\n",
    "# # Calculate 3STD and Mean for Age\n",
    "# ageThreeSD = np.std(df['Age']) * 3\n",
    "# ageMean = np.mean(df['Age'])\n",
    "# ageOutlierThreshold = round(ageThreeSD + ageMean)\n",
    "# print('Age Outlier Threshold: ', ageOutlierThreshold)\n",
    "\n",
    "# # Remove Outliers by replacing all values above Threshold (3STD + Mean) with Threshold Value\n",
    "# df['Age'] = df['Age'].mask(df['Age'] > ageOutlierThreshold, ageOutlierThreshold)\n",
    "# print('After Removing outliers and Imputing null values, df[Age]: ', df['Age'])\n",
    "\n",
    "# # Copy df, keeping only Age column, set type of this df copy to float\n",
    "# df_age_column = pd.DataFrame(df['Age'], columns=['Age'])\n",
    "\n",
    "\n",
    "# # Copy df, keeping only float numeric columns, set type of this df copy to float\n",
    "# df_float_column_names = ['Fare']\n",
    "# print('df_float_column_names: ', df_float_column_names)\n",
    "# df_float_columns = pd.DataFrame(df[df_float_column_names], dtype=np.float, columns=df_float_column_names)\n",
    "\n",
    "# # Concatenate the numeric Data Frames to scale them\n",
    "# print('Before concatenation to df_numeric_columns, df[Age]: ', df['Age'])\n",
    "# print('Before concatenation to df_numeric_columns, df_age_column: ', df_age_column)\n",
    "# df_numeric_columns = pd.concat([df_age_column, df_float_columns], keys=df_numeric_column_names, axis=1)\n",
    "# print('concatenated df_numeric_columns: ', df_numeric_columns)\n",
    "\n",
    "\n",
    "# # Use StandardScaler or MinMaxScaler on Numeric/Non-Categorical columns split\n",
    "# scaler = StandardScaler().fit(df_numeric_columns)\n",
    "# print('scaler.mean_: ', scaler.mean_)\n",
    "# print('scaler.scale: ', scaler.scale_)\n",
    "\n",
    "# df_scaled_numeric_columns =  pd.DataFrame(scaler.transform(df_numeric_columns), columns=df_numeric_column_names)\n",
    "# print('df_scaled_numeric_columns: ', df_scaled_numeric_columns)\n",
    "# # Scaled data should have zero mean and unit variance, check with these prints:\n",
    "# print('df_scaled_numeric_columns.mean(axis=0): ', df_scaled_numeric_columns.mean(axis=0))\n",
    "# print('df_scaled_numeric_columns.std(axis=0)', df_scaled_numeric_columns.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (First way I did this, commented out) Preprocess categorical columns\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # BEGIN Create dataframe with categorical columns so that it contains all categorical data that is preprocessed...\n",
    "\n",
    "# # Copy df, keeping only categorical columns, and one-hot encode them\n",
    "# df_categorical_column_names_raw = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin', 'Embarked']\n",
    "# # print('df_categorical_column_names_raw: ', df_categorical_column_names_raw)\n",
    "# df_categorical_columns = pd.DataFrame(df[df_categorical_column_names_raw], dtype=np.str, columns=df_categorical_column_names_raw)\n",
    "# # print('df_categorical_columns: ', df_categorical_columns)\n",
    "# encoder = OneHotEncoder(drop='first', handle_unknown='error', sparse=False, ).fit(df_categorical_columns)\n",
    "# # print('encoder.categories: ', encoder.categories)\n",
    "# df_encoded_categorical_columns = pd.DataFrame(encoder.transform(df_categorical_columns))\n",
    "# df_encoded_categorical_columns.columns = encoder.get_feature_names(df_categorical_column_names_raw)\n",
    "# # print('df_encoded_categorical_columns: ', df_encoded_categorical_columns)\n",
    "# #   By default, the values each feature can take is inferred automatically from the dataset and can be found in the categories_ attribute:\n",
    "\n",
    "\n",
    "# # Combine the numeric DF with the categorical DF\n",
    "# dfs = [df['Survived'], df_scaled_numeric_columns, df_encoded_categorical_columns]\n",
    "# # print('Before concatenation to dfTyped, df[\\'Survived\\']: ', df['Survived'])\n",
    "# # print('Before concatenation to dfTyped, df[Age]: ', df['Age'])\n",
    "# # print('Before concatenation to dfTyped, df_numeric_columns: ', df_numeric_columns)\n",
    "# # print('Before concatenation to dfTyped, df_scaled_numeric_columns: ', df_scaled_numeric_columns)\n",
    "# ??? shouldn't column names be passed in to the keys list here?\n",
    "# dfTyped = pd.concat(dfs, axis=1, keys=['Survived', *df_scaled_numeric_columns, *df_encoded_categorical_columns])\n",
    "# # print('dfTyped: ', dfTyped)\n",
    "# # print('dfTyped[Age]: ', dfTyped['Age'])\n",
    "\n",
    "#         # - With sklearn.preprocessing, preprocess your Dataframes before training model in the Python Script\n",
    "#         #     - [Guide at SciKit Learn site](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "#         #     - Use OneHotEncoder\n",
    "#         #     - Use StandardScaler or  MinMaxScaler while you're at it\n",
    "#         #     - Don't worry about any other preprocessing to just get the training working\n",
    "#         #     - Strategy:\n",
    "#         #         - d Split dataframe into Numeric/Non-Categorial and Non-Numeric/Categorial columns\n",
    "#         #             - ! Use StandardScaler or MinMaxScaler on Numeric/Non-Categorical columns split\n",
    "#         #             - d Use OneHotEncoder on Non-Numeric/Categorical columns split\n",
    "\n",
    "# # Initial Data Frame is now preprocessed in dfPreprocessed\n",
    "# dfPreprocessed = dfTyped\n",
    "# # print('dfPreprocessed: ', dfPreprocessed)\n",
    "\n",
    "# # Split DataFrame for training now that it is pre-processed\n",
    "# target_column_name = 'Survived'\n",
    "# df_x = dfPreprocessed.drop([target_column_name], axis=1)\n",
    "# df_y = dfPreprocessed[target_column_name]\n",
    "# # print(\"See df_x\", df_x)\n",
    "# print(\"See df_y\", df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated df_numeric_columns:        Age     Fare\n",
      "0    22.0   7.2500\n",
      "1    38.0  71.2833\n",
      "2    26.0   7.9250\n",
      "3    35.0  53.1000\n",
      "4    35.0   8.0500\n",
      "..    ...      ...\n",
      "886  27.0  13.0000\n",
      "887  19.0  30.0000\n",
      "888   NaN  23.4500\n",
      "889  26.0  30.0000\n",
      "890  32.0   7.7500\n",
      "\n",
      "[891 rows x 2 columns]\n",
      "df_categorical_columns:      Pclass     Sex SibSp Parch Cabin Embarked\n",
      "0        3    male     1     0  None        S\n",
      "1        1  female     1     0   C85        C\n",
      "2        3  female     0     0  None        S\n",
      "3        1  female     1     0  C123        S\n",
      "4        3    male     0     0  None        S\n",
      "..     ...     ...   ...   ...   ...      ...\n",
      "886      2    male     0     0  None        S\n",
      "887      1  female     0     0   B42        S\n",
      "888      3  female     1     2  None        S\n",
      "889      1    male     0     0  C148        C\n",
      "890      3    male     0     0  None        Q\n",
      "\n",
      "[891 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Partition Dataframe to get one for Numeric columns and one for Categorical columns\n",
    "\n",
    "df_column_names = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "\n",
    "##### BEGIN Create dataframe with numeric columns so that it contains all numbers that are preprocessed...\n",
    "df_numeric_column_names = ['Age', 'Fare']\n",
    "# Copy df, keeping only float numeric columns, set type of this df copy to float\n",
    "df_float_column_names = ['Fare']\n",
    "df_float_columns = pd.DataFrame(df[df_float_column_names], dtype=np.float, columns=df_float_column_names)\n",
    "# # Copy df, keeping only integer Age column to leave as an integer\n",
    "df_integer_column = pd.DataFrame(df['Age'], columns=['Age'])\n",
    "# Concatenate the numeric DataFrames\n",
    "df_numeric_columns = pd.concat([df_integer_column, df_float_columns], axis=1)\n",
    "\n",
    "\n",
    "##### BEGIN Create dataframe with categorical columns so that it contains all categorical data that is preprocessed...\n",
    "df_categorical_column_names = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin', 'Embarked']\n",
    "# Copy df, keeping only categorical columns\n",
    "df_categorical_columns = pd.DataFrame(df[df_categorical_column_names], dtype=np.str, columns=df_categorical_column_names)\n",
    "\n",
    "print('concatenated df_numeric_columns: ', df_numeric_columns)\n",
    "print('df_categorical_columns: ', df_categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor to preprocess numeric and categorical columns (with Transfomer API via ColumnTransformer, including creation of an Explainer)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', one_hot_encoder)])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, df_numeric_column_names),\n",
    "        ('cat', categorical_transformer, df_categorical_column_names)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': [('preprocessor', ColumnTransformer(transformers=[('num',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='median')),\n",
      "                                                 ('scaler', StandardScaler())]),\n",
      "                                 ['Age', 'Fare']),\n",
      "                                ('cat',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(fill_value='missing',\n",
      "                                                                strategy='constant')),\n",
      "                                                 ('onehot',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                sparse=False))]),\n",
      "                                 ['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin',\n",
      "                                  'Embarked'])])), ('classifier', DecisionTreeClassifier())], 'memory': None, 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "# Append classifier to preprocessing pipeline (Then we have a full prediction pipeline)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "regressor = DecisionTreeClassifier()\n",
    "\n",
    "classifier_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', regressor)])\n",
    "# The classifier_pipeline is later de-pickled and used in the Experiment Run's python script\n",
    "\n",
    "# Now we have a full prediction pipeline.\n",
    "print(classifier_pipeline.__dict__)\n",
    "# Note that you may access classifier_pipeline ColumnTransformers as follows...\n",
    "# print(classifier_pipeline['preprocessor'].transformers[1][1][1].get_feature_names(df_categorical_column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the SciKit Learn Pipeline to pass it to the script\n",
    "# Link about pickling:\n",
    "# https://codefather.tech/blog/python-pickle/#:~:text=%20Python%20Pickle%3A%20Serialize%20Your%20Objects%20%20,The%20pickle%20module%20also%20allows%20to...%20More%20\n",
    "import pickle\n",
    "\n",
    "pickled_pipeline = pickle.dumps(classifier_pipeline)\n",
    "# print(pickle.loads(pickled_pipeline))\n",
    "\n",
    "with open('./scripts/resources/classifier_pipeline.pickle', 'wb') as file:\n",
    "    pickle.dump(classifier_pipeline, file)\n",
    "\n",
    "# You may copy this to the script and Unpickle the SciKit Pipline (that performs Transformation and Model Training)\n",
    "# with open('classifier_pipeline.pickle', 'rb') as file:\n",
    "#     unpickled_pipeline = pickle.load(file)\n",
    "#     print(unpickled_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Does nothing at the moment) Serialize the SciKit Learn Pipeline to pass it to the script (with simple_python_to_json)\n",
    "# I Quit this to use pickling just to get this working and avoid maintaining simple_python_to_json for now...\n",
    "#       lesson learned, \n",
    "#       ideally the code would serialize/deserialize every single Sklearn object type...\n",
    "# TODO? maybe add to simple_python_to_json get this cell working?\n",
    "# You may use code within to help you serialize\n",
    "#      (Contain the code behind the 'simple_python_to_json' reference )\n",
    "# https://robotfantastic.org/serializing-python-data-to-json-some-edge-cases.html\n",
    "# https://cmry.github.io/notes/serialize\n",
    "# import json\n",
    "# import simple_python_to_json\n",
    "# def serialize_dict(in_dictionary, name):\n",
    "#     print(classifier_pipeline.__dict__)\n",
    "#     for k, v in in_dictionary.items():\n",
    "#         print(\"k,v is: \", k, \", \", v)\n",
    "#         print(simple_python_to_json.serializer.json_to_data(simple_python_to_json.serializer.data_to_json(v)))\n",
    "#         in_dictionary[k] = simple_python_to_json.data_to_json(v)\n",
    "#     json.dump(in_dictionary, open(name + '.json', 'w'))\n",
    "\n",
    "# TODO? maybe finish changes to serializer.py to serialize this\n",
    "# serialize_dict(classifier_pipeline.__dict__, 'classifier_pipeline')\n",
    "\n",
    "# TODO? maybe copy below to script, make todo to get it working in script\n",
    "# Load the json and deserialize:\n",
    "\n",
    "# def deserialize(class_init, attr):\n",
    "#     for k, v in attr.items():\n",
    "#         setattr(class_init, k, sr.json_to_data(v))\n",
    "#     return class_init\n",
    "\n",
    "# Load and deserialize the json:\n",
    "# deserialized_classifier_pipeline = deserialize(Pipeline(), json.load(open('classifier_pipeline.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfTyped:       Survived   Age     Fare Pclass     Sex SibSp Parch Cabin Embarked\n",
      "0       False  22.0   7.2500      3    male     1     0  None        S\n",
      "1        True  38.0  71.2833      1  female     1     0   C85        C\n",
      "2        True  26.0   7.9250      3  female     0     0  None        S\n",
      "3        True  35.0  53.1000      1  female     1     0  C123        S\n",
      "4       False  35.0   8.0500      3    male     0     0  None        S\n",
      "..        ...   ...      ...    ...     ...   ...   ...   ...      ...\n",
      "886     False  27.0  13.0000      2    male     0     0  None        S\n",
      "887      True  19.0  30.0000      1  female     0     0   B42        S\n",
      "888     False   NaN  23.4500      3  female     1     2  None        S\n",
      "889      True  26.0  30.0000      1    male     0     0  C148        C\n",
      "890     False  32.0   7.7500      3    male     0     0  None        Q\n",
      "\n",
      "[891 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Combine the numeric DF with the categorical DF\n",
    "# print(\"df['Survived'] is \", df['Survived'])\n",
    "# print(\"df_numeric_columns is \", df_numeric_columns)\n",
    "# print(\"df_numeric_columns.columns is \", df_numeric_columns.columns)\n",
    "# print(\"df_categorical_columns is \", df_categorical_columns)\n",
    "# print(\"df_categorical_columns.columns is \", df_categorical_columns.columns)\n",
    "\n",
    "# Concatenate dfs to get DataFrame of all columns to submit to the classifier_pipeline\n",
    "dfs = [df['Survived'], df_numeric_columns, df_categorical_columns]\n",
    "# print(\"dfs is\" + str(dfs))\n",
    "# print('Before concatenation to dfTyped, df[\\'Survived\\']: ', df['Survived'])\n",
    "# print('Before concatenation to dfTyped, df_numeric_columns: ', df_numeric_columns)\n",
    "dfTyped = pd.concat(dfs, axis=1)\n",
    "print('dfTyped: ', dfTyped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See df_y      Survived\n",
      "0       False\n",
      "1        True\n",
      "2        True\n",
      "3        True\n",
      "4       False\n",
      "..        ...\n",
      "886     False\n",
      "887      True\n",
      "888     False\n",
      "889      True\n",
      "890     False\n",
      "\n",
      "[891 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Split pre-transformation Data Frame into feature/target columns\n",
    "target_column_name = 'Survived'\n",
    "df_x = dfTyped.drop([target_column_name], axis=1)\n",
    "df_y = dfTyped[target_column_name].to_frame()\n",
    "# print(\"See df_x\", df_x)\n",
    "print(\"See df_y\", df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('num',\n",
       "                                 Pipeline(steps=[('imputer',\n",
       "                                                  SimpleImputer(strategy='median')),\n",
       "                                                 ('scaler', StandardScaler())]),\n",
       "                                 ['Age', 'Fare']),\n",
       "                                ('cat',\n",
       "                                 Pipeline(steps=[('imputer',\n",
       "                                                  SimpleImputer(fill_value='missing',\n",
       "                                                                strategy='constant')),\n",
       "                                                 ('onehot',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                sparse=False))]),\n",
       "                                 ['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin',\n",
       "                                  'Embarked'])])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Validate the Pipeline will function once passed to the script)\n",
    "\n",
    "# Get the preprocessed Data Frame columns in a list\n",
    "one_hot_encoder.fit(df_categorical_columns)\n",
    "# Get new One Hot Encoded column names\n",
    "# df_encoded_categorical_column_names = one_hot_encoder.get_feature_names(df_categorical_column_names)\n",
    "# print(str(df_encoded_categorical_column_names))\n",
    "\n",
    "# (fail to) Get the preprocessed Data Frame\n",
    "# # dfPreprocessed = preprocessor.fit_transform(df_x_pre_transformation, df_y_pre_transformation)\n",
    "\n",
    "# Attempt to fit the preprocessor to the data, to validate it will function inside the script it is passed to\n",
    "preprocessor.fit(df_x, df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Successfully obtained datastore reference and path.\n",
      "Uploading file to managed-dataset/34f35593-6547-42a1-b757-8013a272f2b8/\n",
      "Successfully uploaded file to datastore.\n",
      "Creating and registering a new dataset.\n",
      "Successfully created and registered a new dataset.\n",
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Successfully obtained datastore reference and path.\n",
      "Uploading file to managed-dataset/45c94852-ef4f-46e5-9e64-e01294ddedfd/\n",
      "Successfully uploaded file to datastore.\n",
      "Creating and registering a new dataset.\n",
      "Successfully created and registered a new dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'managed-dataset/45c94852-ef4f-46e5-9e64-e01294ddedfd/')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\",\n",
       "    \"ReadParquetFile\",\n",
       "    \"DropColumns\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"b1fb608b-1b7b-4b9a-99b7-5e310bd9fe3d\",\n",
       "    \"name\": \"Titanic Target Column Data for train_test_split usage (LocalDocker notebook)\",\n",
       "    \"version\": 7,\n",
       "    \"workspace\": \"Workspace.create(name='automlbook', subscription_id='4d278f3d-b4fd-4fa2-86b6-d34b96bc888f', resource_group='Foxy_Resources')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register Pandas Dataframe of base df_x and df_y\n",
    "Dataset.Tabular.register_pandas_dataframe(df_x, datastore, \"Titanic Feature Column Data for train_test_split usage (LocalDocker notebook)\")\n",
    "Dataset.Tabular.register_pandas_dataframe(df_y, datastore, \"Titanic Target Column Data for train_test_split usage (LocalDocker notebook)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See y_test.columns.tolist() ['Survived']\n",
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Successfully obtained datastore reference and path.\n",
      "Uploading file to managed-dataset/42b51bd3-c5c3-492b-ae6f-4baa6a6311d5/\n",
      "Successfully uploaded file to datastore.\n",
      "Creating and registering a new dataset.\n",
      "Successfully created and registered a new dataset.\n",
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Successfully obtained datastore reference and path.\n",
      "Uploading file to managed-dataset/1effeaf1-c21f-4e05-97c0-db7931487cd4/\n",
      "Successfully uploaded file to datastore.\n",
      "Creating and registering a new dataset.\n",
      "Successfully created and registered a new dataset.\n",
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Successfully obtained datastore reference and path.\n",
      "Uploading file to managed-dataset/85fb803b-fd9b-465c-bd62-fa77f3b5080e/\n",
      "Successfully uploaded file to datastore.\n",
      "Creating and registering a new dataset.\n",
      "Successfully created and registered a new dataset.\n",
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Successfully obtained datastore reference and path.\n",
      "Uploading file to managed-dataset/93020485-841f-47f5-9c79-a93bf554080e/\n",
      "Successfully uploaded file to datastore.\n",
      "Creating and registering a new dataset.\n",
      "Successfully created and registered a new dataset.\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test data, register the resulting Datasets with Azure\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "# What you need to pass to train_test_split...\n",
    "# ... I need X and Y dataframe, X just with target missing, Y just with target column present\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "# print(\"See y_test\", y_test)\n",
    "print(\"See y_test.columns.tolist()\", str(y_test.columns.tolist()))\n",
    "# print(\"See y_test.values.tolist() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.tolist())\n",
    "# print(\"See y_test.values.ravel() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.ravel())\n",
    "# print(\"See y_test.values.tolist().flatten() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.flatten())\n",
    "\n",
    "# Register the splits\n",
    "X_train_registered_name = \"Titanic Feature Column Data for training (LocalDocker notebook)\"\n",
    "X_test_registered_name = \"Titanic Feature Column Data for testing (LocalDocker notebook)\"\n",
    "y_train_registered_name = \"Titanic Target Column Data for training (LocalDocker notebook)\"\n",
    "y_test_registered_name = \"Titanic Target Column Data for testing (LocalDocker notebook)\"\n",
    "Dataset.Tabular.register_pandas_dataframe(X_train, datastore, X_train_registered_name)\n",
    "Dataset.Tabular.register_pandas_dataframe(X_test, datastore, X_test_registered_name)\n",
    "Dataset.Tabular.register_pandas_dataframe(y_train, datastore, y_train_registered_name)\n",
    "Dataset.Tabular.register_pandas_dataframe(y_test, datastore, y_test_registered_name)\n",
    "xTrainTestYTrainTest = [X_train_registered_name, X_test_registered_name, y_train_registered_name, y_test_registered_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names and create TabularExplainer with them\n",
    "features=[*df_numeric_column_names, *df_categorical_column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitDatasetNamesEncoded: [\"Titanic Feature Column Data for training (LocalDocker notebook)\",\"Titanic Feature Column Data for testing (LocalDocker notebook)\",\"Titanic Target Column Data for training (LocalDocker notebook)\",\"Titanic Target Column Data for testing (LocalDocker notebook)\"]\n"
     ]
    }
   ],
   "source": [
    "# Encode Experiment script arguments list into a string like '[\"a\",\"b\"]'\n",
    "\n",
    "# Encode numeric column names list\n",
    "temp_column_names = df_numeric_column_names.copy()\n",
    "for x in range(len(temp_column_names)):\n",
    "        temp_column_names[x] = '\"{}\"'.format(temp_column_names[x])\n",
    "numericFeatureNamesEncoded = \"[{}]\".format(\",\".join(temp_column_names))\n",
    "# print(\"numericFeatureNamesEncoded:\", numericFeatureNamesEncoded)\n",
    "\n",
    "# Encode categoric column names list\n",
    "temp_column_names = df_categorical_column_names.copy()\n",
    "for x in range(len(temp_column_names)):\n",
    "        temp_column_names[x] = '\"{}\"'.format(temp_column_names[x])\n",
    "categoricFeatureNamesEncoded = \"[{}]\".format(\",\".join(temp_column_names))\n",
    "# print(\"categoricFeatureNamesEncoded:\", categoricFeatureNamesEncoded)\n",
    "\n",
    "# Encode split dataset names list\n",
    "for x in range(len(xTrainTestYTrainTest)):\n",
    "        xTrainTestYTrainTest[x] = '\"{}\"'.format(xTrainTestYTrainTest[x])\n",
    "xTrainTestYTrainTestEncoded = \"[{}]\".format(\",\".join(xTrainTestYTrainTest))\n",
    "print(\"splitDatasetNamesEncoded:\", xTrainTestYTrainTestEncoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220314.v1\n"
     ]
    }
   ],
   "source": [
    "# Set Local Docker Environment up (with System Managed Dependencies, via Conda)\n",
    "#\n",
    "# Learn about Environment here:\n",
    "#       https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments\n",
    "#       https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment(class)?view=azure-ml-py\n",
    "#\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "\n",
    "docker_env = Environment(\"docker-env\")\n",
    "# Editing a run configuration property on-fly.\n",
    "docker_env.python.user_managed_dependencies = False\n",
    "# Use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param.\n",
    "#           https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfig.dockerconfiguration?view=azure-ml-py\n",
    "docker_config = DockerConfiguration(use_docker=True)\n",
    "print(docker_env.docker.base_image)\n",
    "\n",
    "# Specify docker steps as a string. \n",
    "dockerfile = r\"\"\"\n",
    "FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220314.v1\n",
    "\n",
    "ARG DEBIAN_FRONTEND=noninteractive\n",
    "RUN apt-get update -y && apt-get upgrade -y &&\\\n",
    "    apt-get install -y build-essential \\\n",
    "                       cmake \\\n",
    "                       curl \\\n",
    "                       gfortran \\\n",
    "                       git \\\n",
    "                       jupyter \\\n",
    "                       libatlas-base-dev \\\n",
    "                       libblas-dev \\\n",
    "                       libbz2-dev \\\n",
    "                       libffi-dev \\\n",
    "                       libgdbm-dev \\\n",
    "                       liblapack-dev \\\n",
    "                       liblzma-dev \\\n",
    "                       libncurses5-dev \\\n",
    "                       libncursesw5-dev \\\n",
    "                       libreadline-dev \\\n",
    "                       libsqlite3-dev \\\n",
    "                       libssl-dev \\\n",
    "                       libxml2-dev \\\n",
    "                       libxmlsec1-dev \\\n",
    "                       llvm \\\n",
    "                       lzma \\\n",
    "                       lzma-dev \\\n",
    "                       make \\\n",
    "                       pip \\\n",
    "                       tcl-dev \\\n",
    "                       tk-dev \\\n",
    "                       wget \\\n",
    "                       xz-utils \\\n",
    "                       zlib1g-dev\n",
    "\n",
    "RUN echo \"Hello from custom container!\"\n",
    "\"\"\"\n",
    "\n",
    "# Set base image to None, because the image is defined by dockerfile.\n",
    "docker_env.docker.base_image = None\n",
    "docker_env.docker.base_dockerfile = dockerfile\n",
    "\n",
    "\n",
    "# Specify conda dependencies with scikit-learn\n",
    "conda_packages = ['pip',\n",
    "                  'pyspark',\n",
    "                  'scikit-learn'\n",
    "                 ]\n",
    "pip_packages =   ['azureml.interpret',\n",
    "                  'azureml-dataset-runtime',\n",
    "                  'jinja2',\n",
    "                  'MarkupSafe',\n",
    "                  'raiwidgets'\n",
    "                 ]\n",
    "                 \n",
    "condaDependencies = CondaDependencies.create(conda_packages=conda_packages, pip_packages=pip_packages)\n",
    "docker_env.python.conda_dependencies = condaDependencies\n",
    "# NOTE: can set this:\n",
    "#           docker_env.python.interpreter_path = \"/opt/miniconda/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScriptRunConfig arguments:  ['--tenant-id', '198c7d8c-e010-45ce-a018-ec2d9a33f58f', '--ws-name', 'automlbook', '--subscription-id', '4d278f3d-b4fd-4fa2-86b6-d34b96bc888f', '--resource-group', 'Foxy_Resources', '--datastore-name', 'workspaceblobstore', '--out-model-file-name', 'DecisionTreeClassifier_Titanic_LocalDocker_local-2022-05-22_10-29-57.431566.pkl', '--numeric-feature-names', '[\"Age\",\"Fare\"]', '--categoric-feature-names', '[\"Pclass\",\"Sex\",\"SibSp\",\"Parch\",\"Cabin\",\"Embarked\"]', '--x-train-test-y-train-test', '[\"Titanic Feature Column Data for training (LocalDocker notebook)\",\"Titanic Feature Column Data for testing (LocalDocker notebook)\",\"Titanic Target Column Data for training (LocalDocker notebook)\",\"Titanic Target Column Data for testing (LocalDocker notebook)\"]']\n"
     ]
    }
   ],
   "source": [
    "# Prepare to run Training Experiment in Docker Environment (With Docker running on local device)\n",
    "\n",
    "from azureml.core import ScriptRunConfig\n",
    "import datetime\n",
    "\n",
    "# Experiment\n",
    "experiment_name = 'LocalDocker_Training_AutoML'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "# Define Compute Cluster to use\n",
    "compute_target = 'local'\n",
    "source_directory = './scripts'\n",
    "script_name = 'localDockerTrainingAutoML.py'\n",
    "dataset_name = 'automlbook Titanic Training Data A'\n",
    "# set output file name like 'DecisionTreeClassifier_Titanic_LocalDocker-2022-04-17 21:40:36.114550.pkl'\n",
    "suffix = 'local-' + str(datetime.datetime.now())\n",
    "suffix = suffix.replace(' ', '_') # Clean up datetimestamp\n",
    "suffix = suffix.replace(':', '-') \n",
    "out_model_file_name = 'DecisionTreeClassifier_Titanic_LocalDocker_{}.pkl'.format(suffix)\n",
    "# set output file name like 'DecisionTreeClassifier_Titanic_LocalDocker-2022-04-17 21:40:36.114550.pkl'\n",
    "\n",
    "script_arguments = [\n",
    "\"--tenant-id\", tenant_id,\n",
    "\"--ws-name\", ws_name,\n",
    "\"--subscription-id\", subscription_id,\n",
    "\"--resource-group\", resource_group,\n",
    "\"--datastore-name\", datastore_name,\n",
    "\"--out-model-file-name\", out_model_file_name,\n",
    "\"--numeric-feature-names\", numericFeatureNamesEncoded,\n",
    "# Pass list encoded as a comma-separated string, containing the raw feature names\n",
    "# like '[\"a\",\"b\"]'\n",
    "\"--categoric-feature-names\", categoricFeatureNamesEncoded,\n",
    "# Pass list encoded as a comma-separated string, containing the name of each dataset (X_train, X_test, y_train, y_test)\n",
    "# like '[\"a\",\"b\"]'\n",
    "\"--x-train-test-y-train-test\", xTrainTestYTrainTestEncoded \n",
    "]\n",
    "print(\"ScriptRunConfig arguments: \", script_arguments)\n",
    "scriptRunConfig = ScriptRunConfig(\n",
    "        source_directory=source_directory,\n",
    "        script=script_name,\n",
    "        arguments=script_arguments,\n",
    "        environment=docker_env,\n",
    "        docker_runtime_config=docker_config,\n",
    "        compute_target=compute_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "johna\n",
      "Docker version 20.10.14, build a224086\n",
      " * Docker is running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Plugin \"/usr/local/lib/docker/cli-plugins/docker-buildx\" is not valid: failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-buildx: no such file or directory\n",
      "WARNING: Plugin \"/usr/local/lib/docker/cli-plugins/docker-compose\" is not valid: failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-compose: no such file or directory\n",
      "WARNING: Plugin \"/usr/local/lib/docker/cli-plugins/docker-scan\" is not valid: failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-scan: no such file or directory\n",
      "WARNING: No blkio throttle.read_bps_device support\n",
      "WARNING: No blkio throttle.write_bps_device support\n",
      "WARNING: No blkio throttle.read_iops_device support\n",
      "WARNING: No blkio throttle.write_iops_device support\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e836acef3d4a8ba0ba0deaa825dcc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Starting\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/LocalDocker_Training_AutoML_1653229799_05867879?wsid=/subscriptions/4d278f3d-b4fd-4fa2-86b6-d34b96bc888f/resourcegroups/Foxy_Resources/workspaces/automlbook&tid=198c7d8c-e010-45ce-a018-ec2d9a33f58f\", \"run_id\": \"LocalDocker_Training_AutoML_1653229799_05867879\", \"run_properties\": {\"run_id\": \"LocalDocker_Training_AutoML_1653229799_05867879\", \"created_utc\": \"2022-05-22T14:30:02.000196Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"local\", \"ContentSnapshotId\": \"7ebfd33f-da0c-47af-8657-eb683e932160\", \"azureml.git.repository_uri\": \"https://github.com/jjrobertson14/Automated-Machine-Learning-with-Microsoft-Azure.git\", \"mlflow.source.git.repoURL\": \"https://github.com/jjrobertson14/Automated-Machine-Learning-with-Microsoft-Azure.git\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"a95e5c7eaad3e6c0e618a03eae17cb80d6870373\", \"mlflow.source.git.commit\": \"a95e5c7eaad3e6c0e618a03eae17cb80d6870373\", \"azureml.git.dirty\": \"False\"}, \"tags\": {\"model_explanation\": \"True\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Starting\", \"log_files\": {\"azureml-logs/60_control_log.txt\": \"https://automlbook5923937161.blob.core.windows.net/azureml/ExperimentRun/dcid.LocalDocker_Training_AutoML_1653229799_05867879/azureml-logs/60_control_log.txt?sv=2019-07-07&sr=b&sig=ZBFI1BMWEK%2FoPlShrxOVL2Eg%2FDlCuaKTfDBhGjx7i7Y%3D&skoid=dfacfa98-b2f2-4e5a-be85-5b882716a4d6&sktid=198c7d8c-e010-45ce-a018-ec2d9a33f58f&skt=2022-05-22T12%3A28%3A00Z&ske=2022-05-23T20%3A38%3A00Z&sks=b&skv=2019-07-07&st=2022-05-22T14%3A21%3A11Z&se=2022-05-22T22%3A31%3A11Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/60_control_log.txt\"]], \"run_duration\": \"0:04:06\", \"run_number\": \"1653229802\", \"run_queued_details\": {\"status\": \"Starting\", \"details\": null}}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"model_explanation\", \"run_id\": \"LocalDocker_Training_AutoML_1653229799_05867879\", \"categories\": [0], \"series\": [{\"data\": [{\"class_labels\": null, \"overall_summary\": [0.23638433857696622, 0.10331734118150442, 0.10138131081536406, 0.07461945709647486, 0.05275172539534105, 0.039333430260920205, 0.01598570829989234, 0.015976001552046787, 0.012744362366743178, 0.012341766496088168, 0.006855148512595239, 0.00612482091397778, 0.005945192432017909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"overall_imp\": [\"Sex_female\", \"Pclass_3\", \"Fare\", \"Cabin_None\", \"Age\", \"Embarked_C\", \"SibSp_1\", \"Parch_1\", \"Embarked_S\", \"SibSp_2\", \"Cabin_B49\", \"SibSp_0\", \"Cabin_B96 B98\", \"Parch_0\", \"Cabin_B42\", \"Cabin_B41\", \"Cabin_B37\", \"Cabin_B18\", \"Cabin_B102\", \"Parch_5\", \"Parch_4\", \"Parch_2\", \"Cabin_E67\", \"Cabin_E34\", \"Cabin_B50\", \"SibSp_4\", \"SibSp_3\", \"Cabin_F G73\", \"Sex_male\", \"Cabin_F2\", \"Cabin_F33\", \"Pclass_2\", \"Pclass_1\", \"Cabin_G6\", \"SibSp_5\", \"Cabin_B57 B59 B63 B66\", \"Cabin_E33\", \"Cabin_C78\", \"Cabin_D9\", \"Cabin_D49\", \"Cabin_D45\", \"Cabin_D36\", \"Cabin_D20\", \"Cabin_D17\", \"Cabin_D10 D12\", \"Cabin_C87\", \"Cabin_C85\", \"Cabin_C83\", \"Cabin_C7\", \"Cabin_B58 B60\", \"Cabin_C65\", \"Cabin_C62 C64\", \"Cabin_C54\", \"Cabin_C52\", \"Cabin_C47\", \"Cabin_C23 C25 C27\", \"Cabin_C126\", \"Embarked_Q\", \"Cabin_C106\", \"Cabin_B78\", \"Cabin_C125\"], \"per_class_summary\": [[0.2363843385769662, 0.10331734118150442, 0.10138131081536406, 0.07461945709647484, 0.052751725395341044, 0.03933343026092022, 0.015985708299892352, 0.015976001552046787, 0.012744362366743178, 0.012341766496088166, 0.006855148512595238, 0.00612482091397778, 0.005945192432017908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23638433857696625, 0.10331734118150442, 0.10138131081536407, 0.07461945709647486, 0.05275172539534106, 0.03933343026092019, 0.015985708299892334, 0.015976001552046787, 0.012744362366743178, 0.012341766496088171, 0.006855148512595239, 0.006124820913977779, 0.0059451924320179095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]], \"per_class_imp\": [[\"Sex_female\", \"Pclass_3\", \"Fare\", \"Cabin_None\", \"Age\", \"Embarked_C\", \"SibSp_1\", \"Parch_1\", \"Embarked_S\", \"SibSp_2\", \"Cabin_B49\", \"SibSp_0\", \"Cabin_B96 B98\", \"Parch_0\", \"Cabin_B42\", \"Cabin_B41\", \"Cabin_B37\", \"Cabin_B18\", \"Cabin_B102\", \"Parch_5\", \"Parch_4\", \"Parch_2\", \"Cabin_E67\", \"Cabin_E34\", \"Cabin_B50\", \"SibSp_4\", \"SibSp_3\", \"Cabin_F G73\", \"Sex_male\", \"Cabin_F2\", \"Cabin_F33\", \"Pclass_2\", \"Pclass_1\", \"Cabin_G6\", \"SibSp_5\", \"Cabin_B57 B59 B63 B66\", \"Cabin_E33\", \"Cabin_C78\", \"Cabin_D9\", \"Cabin_D49\", \"Cabin_D45\", \"Cabin_D36\", \"Cabin_D20\", \"Cabin_D17\", \"Cabin_D10 D12\", \"Cabin_C87\", \"Cabin_C85\", \"Cabin_C83\", \"Cabin_C7\", \"Cabin_B58 B60\", \"Cabin_C65\", \"Cabin_C62 C64\", \"Cabin_C54\", \"Cabin_C52\", \"Cabin_C47\", \"Cabin_C23 C25 C27\", \"Cabin_C126\", \"Embarked_Q\", \"Cabin_C106\", \"Cabin_B78\", \"Cabin_C125\"], [\"Sex_female\", \"Pclass_3\", \"Fare\", \"Cabin_None\", \"Age\", \"Embarked_C\", \"SibSp_1\", \"Parch_1\", \"Embarked_S\", \"SibSp_2\", \"Cabin_B49\", \"SibSp_0\", \"Cabin_B96 B98\", \"Parch_0\", \"Cabin_B42\", \"Cabin_B41\", \"Cabin_B37\", \"Cabin_B18\", \"Cabin_B102\", \"Parch_5\", \"Parch_4\", \"Parch_2\", \"Cabin_E67\", \"Cabin_E34\", \"Cabin_B50\", \"SibSp_4\", \"SibSp_3\", \"Cabin_F G73\", \"Sex_male\", \"Cabin_F2\", \"Cabin_F33\", \"Pclass_2\", \"Pclass_1\", \"Cabin_G6\", \"SibSp_5\", \"Cabin_B57 B59 B63 B66\", \"Cabin_E33\", \"Cabin_C78\", \"Cabin_D9\", \"Cabin_D49\", \"Cabin_D45\", \"Cabin_D36\", \"Cabin_D20\", \"Cabin_D17\", \"Cabin_D10 D12\", \"Cabin_C87\", \"Cabin_C85\", \"Cabin_C83\", \"Cabin_C7\", \"Cabin_B58 B60\", \"Cabin_C65\", \"Cabin_C62 C64\", \"Cabin_C54\", \"Cabin_C52\", \"Cabin_C47\", \"Cabin_C23 C25 C27\", \"Cabin_C126\", \"Embarked_Q\", \"Cabin_C106\", \"Cabin_B78\", \"Cabin_C125\"]]}]}]}], \"run_logs\": \"[2022-05-22T14:30:01.798428] Using urllib.request Python 3.0 or later\\nStreaming log file azureml-logs/60_control_log.txt\\nStarting the daemon thread to refresh tokens in background for process with pid = 8992\\nnvidia-docker is installed on the target. Using nvidia-docker for docker operations.\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.40.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Submit Experiment Run to Docker environment\n",
    "#\n",
    "# (see more on use of Docker environment: \n",
    "#   https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-local/train-on-local.ipynb\n",
    "# )\n",
    "import subprocess\n",
    "# TODO? Add minconda bin to path within docker container? \"/home/johna/miniconda3/bin:\"\n",
    "\n",
    "subprocess.run(\"whoami\", shell=True)\n",
    "\n",
    "# import getpass\n",
    "# If you need, you can get a password from user input (Notebook pauses to show a prompt here)\n",
    "# password = getpass.getpass()\n",
    "\n",
    "scriptRunConfig.run_config.environment = docker_env\n",
    "# Check if Docker is installed and Linux containers are enabled\n",
    "if subprocess.run(\"docker -v\", shell=True).returncode == 0:\n",
    "    subprocess.run(\"service docker status\", shell=True)\n",
    "\n",
    "\n",
    "    # This StackOverflow page will help you run the docker commands with sudo if that is necessary for you: \n",
    "    # https://askubuntu.com/questions/155791/how-do-i-sudo-a-command-in-a-script-without-being-asked-for-a-password\n",
    "    #\n",
    "    # (NOTE: These snippets from the link will allow you to have the docker commandsd run with sudo)\n",
    "    #\n",
    "    # Create script with the command to run with sudo, for example:\n",
    "    # docker system info\n",
    "    #\n",
    "    # Run sudo chown and chmod commands to grant access of the file to root \n",
    "    # sudo chown root:root ~/docker_system_info.sh\n",
    "    # sudo chmod 700 ~/docker_system_info.sh\n",
    "    #\n",
    "    # Run sudo visudo and insert a line below the line `%sudo   ALL=(ALL:ALL) ALL`\n",
    "    # [username]  ALL=(ALL) NOPASSWD: /home/[username]/docker_system_info.sh\n",
    "    #\n",
    "    # Then call the python script with your python subprocess command\n",
    "    # p = subprocess.run('sudo ~/docker_system_info.sh', shell=True)\n",
    "\n",
    "\n",
    "    out = subprocess.check_output('sudo ~/docker_system_info.sh', shell=True).decode('ascii')\n",
    "    if not \"OSType: linux\" in out:\n",
    "        print(\"Switch Docker engine to use Linux containers.\")\n",
    "    else:\n",
    "        # TODO get this experiment run to work, it is hanging, perhaps I need apt dependencies added to Dockerfile string I passed(??)\n",
    "        # TODO update: still not getting any logs past \"nvidia-docker is installed on the target. Using nvidia-docker for docker operations\"\n",
    "        AutoML_run = experiment.submit(scriptRunConfig)\n",
    "        RunDetails(AutoML_run).show()\n",
    "else:\n",
    "    print(\"Docker engine is not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO DELETE THIS BACKUP CODE\n",
    "# Submit Experiment Run to Docker environment\n",
    "#\n",
    "# (see more on use of Docker environment: \n",
    "#   https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-local/train-on-local.ipynb\n",
    "# )\n",
    "# import subprocess\n",
    "# # TODO? Add minconda bin to path within docker container? \"/home/johna/miniconda3/bin:\"\n",
    "\n",
    "# subprocess.run(\"whoami\", shell=True)\n",
    "\n",
    "# import getpass\n",
    "# import os\n",
    "# # TODO run failing commands with sudo like this.) Get password from user input (Notebook pauses to show a prompt here)\n",
    "# password = getpass.getpass()\n",
    "\n",
    "# # command = \"sudo -S docker system info\" # can be any command but don't forget -S as it enables input from stdin\n",
    "# # os.popen(command, 'w').write(password+'\\n') # newline char is important otherwise prompt will wait for you to manually perform newline\n",
    "\n",
    "# scriptRunConfig.run_config.environment = docker_env\n",
    "# # Check if Docker is installed and Linux containers are enabled\n",
    "# if subprocess.run(\"docker -v\", shell=True).returncode == 0:\n",
    "#     subprocess.run(\"service docker status\", shell=True)\n",
    "#     # TODO FIX THIS perhaps have docker run as a user, or can I pass a username/password?\n",
    "#     command = \"sudo -S docker system info\" # can be any command but don't forget -S as it enables input from stdin\n",
    "#     print(os.popen(command, 'w').write(password+'\\n')) # newline char is important otherwise prompt will wait for you to manually perform newline\n",
    "#     out = subprocess.check_output(\"sudo docker system info\", shell=True).decode('ascii')\n",
    "#     print(\"out: \", out)\n",
    "#     if not \"OSType: linux\" in out:\n",
    "#         print(\"Switch Docker engine to use Linux containers.\")\n",
    "#     else:\n",
    "#         AutoML_run = experiment.submit(scriptRunConfig)\n",
    "#         RunDetails(AutoML_run).show()\n",
    "# else:\n",
    "#     print(\"Docker engine is not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3' 'male' '0' '0' 'None' 'C']\n",
      " ['3' 'male' '0' '0' 'None' 'S']\n",
      " ['3' 'male' '4' '1' 'None' 'Q']\n",
      " ...\n",
      " ['1' 'female' '1' '0' 'D36' 'C']\n",
      " ['3' 'male' '0' '0' 'None' 'S']\n",
      " ['3' 'male' '0' '0' 'None' 'S']]\n",
      "['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin', 'Embarked']\n",
      "df_encoded_categorical_column_names ['Pclass_1' 'Pclass_2' 'Pclass_3' 'Sex_female' 'Sex_male' 'SibSp_0'\n",
      " 'SibSp_1' 'SibSp_2' 'SibSp_3' 'SibSp_4' 'SibSp_5' 'Parch_0' 'Parch_1'\n",
      " 'Parch_2' 'Parch_4' 'Parch_5' 'Cabin_B102' 'Cabin_B18' 'Cabin_B37'\n",
      " 'Cabin_B41' 'Cabin_B42' 'Cabin_B49' 'Cabin_B50' 'Cabin_B57 B59 B63 B66'\n",
      " 'Cabin_B58 B60' 'Cabin_B78' 'Cabin_B96 B98' 'Cabin_C106' 'Cabin_C125'\n",
      " 'Cabin_C126' 'Cabin_C23 C25 C27' 'Cabin_C47' 'Cabin_C52' 'Cabin_C54'\n",
      " 'Cabin_C62 C64' 'Cabin_C65' 'Cabin_C7' 'Cabin_C78' 'Cabin_C83'\n",
      " 'Cabin_C85' 'Cabin_C87' 'Cabin_D10 D12' 'Cabin_D17' 'Cabin_D20'\n",
      " 'Cabin_D36' 'Cabin_D45' 'Cabin_D49' 'Cabin_D9' 'Cabin_E33' 'Cabin_E34'\n",
      " 'Cabin_E67' 'Cabin_F G73' 'Cabin_F2' 'Cabin_F33' 'Cabin_G6' 'Cabin_None'\n",
      " 'Embarked_C' 'Embarked_Q' 'Embarked_S']\n",
      "globalFeatureExplanations:  {'Sex_female': 0.23638433857696622, 'Pclass_3': 0.10331734118150442, 'Fare': 0.10138131081536406, 'Cabin_None': 0.07461945709647486, 'Age': 0.05275172539534105, 'Embarked_C': 0.039333430260920205, 'SibSp_1': 0.01598570829989234, 'Parch_1': 0.015976001552046787, 'Embarked_S': 0.012744362366743178, 'SibSp_2': 0.012341766496088168, 'Cabin_B49': 0.006855148512595239, 'SibSp_0': 0.00612482091397778, 'Cabin_B96 B98': 0.005945192432017909, 'Parch_0': 0.0, 'Cabin_B42': 0.0, 'Cabin_B41': 0.0, 'Cabin_B37': 0.0, 'Cabin_B18': 0.0, 'Cabin_B102': 0.0, 'Parch_5': 0.0, 'Parch_4': 0.0, 'Parch_2': 0.0, 'Cabin_E67': 0.0, 'Cabin_E34': 0.0, 'Cabin_B50': 0.0, 'SibSp_4': 0.0, 'SibSp_3': 0.0, 'Cabin_F G73': 0.0, 'Sex_male': 0.0, 'Cabin_F2': 0.0, 'Cabin_F33': 0.0, 'Pclass_2': 0.0, 'Pclass_1': 0.0, 'Cabin_G6': 0.0, 'SibSp_5': 0.0, 'Cabin_B57 B59 B63 B66': 0.0, 'Cabin_E33': 0.0, 'Cabin_C78': 0.0, 'Cabin_D9': 0.0, 'Cabin_D49': 0.0, 'Cabin_D45': 0.0, 'Cabin_D36': 0.0, 'Cabin_D20': 0.0, 'Cabin_D17': 0.0, 'Cabin_D10 D12': 0.0, 'Cabin_C87': 0.0, 'Cabin_C85': 0.0, 'Cabin_C83': 0.0, 'Cabin_C7': 0.0, 'Cabin_B58 B60': 0.0, 'Cabin_C65': 0.0, 'Cabin_C62 C64': 0.0, 'Cabin_C54': 0.0, 'Cabin_C52': 0.0, 'Cabin_C47': 0.0, 'Cabin_C23 C25 C27': 0.0, 'Cabin_C126': 0.0, 'Embarked_Q': 0.0, 'Cabin_C106': 0.0, 'Cabin_B78': 0.0, 'Cabin_C125': 0.0}\n",
      "global_explanation.get_feature_importance_dict():  {'Sex_female': 0.23638433857696622, 'Pclass_3': 0.10331734118150442, 'Fare': 0.10138131081536406, 'Cabin_None': 0.07461945709647486, 'Age': 0.05275172539534105, 'Embarked_C': 0.039333430260920205, 'SibSp_1': 0.01598570829989234, 'Parch_1': 0.015976001552046787, 'Embarked_S': 0.012744362366743178, 'SibSp_2': 0.012341766496088168, 'Cabin_B49': 0.006855148512595239, 'SibSp_0': 0.00612482091397778, 'Cabin_B96 B98': 0.005945192432017909, 'Parch_0': 0.0, 'Cabin_B42': 0.0, 'Cabin_B41': 0.0, 'Cabin_B37': 0.0, 'Cabin_B18': 0.0, 'Cabin_B102': 0.0, 'Parch_5': 0.0, 'Parch_4': 0.0, 'Parch_2': 0.0, 'Cabin_E67': 0.0, 'Cabin_E34': 0.0, 'Cabin_B50': 0.0, 'SibSp_4': 0.0, 'SibSp_3': 0.0, 'Cabin_F G73': 0.0, 'Sex_male': 0.0, 'Cabin_F2': 0.0, 'Cabin_F33': 0.0, 'Pclass_2': 0.0, 'Pclass_1': 0.0, 'Cabin_G6': 0.0, 'SibSp_5': 0.0, 'Cabin_B57 B59 B63 B66': 0.0, 'Cabin_E33': 0.0, 'Cabin_C78': 0.0, 'Cabin_D9': 0.0, 'Cabin_D49': 0.0, 'Cabin_D45': 0.0, 'Cabin_D36': 0.0, 'Cabin_D20': 0.0, 'Cabin_D17': 0.0, 'Cabin_D10 D12': 0.0, 'Cabin_C87': 0.0, 'Cabin_C85': 0.0, 'Cabin_C83': 0.0, 'Cabin_C7': 0.0, 'Cabin_B58 B60': 0.0, 'Cabin_C65': 0.0, 'Cabin_C62 C64': 0.0, 'Cabin_C54': 0.0, 'Cabin_C52': 0.0, 'Cabin_C47': 0.0, 'Cabin_C23 C25 C27': 0.0, 'Cabin_C126': 0.0, 'Embarked_Q': 0.0, 'Cabin_C106': 0.0, 'Cabin_B78': 0.0, 'Cabin_C125': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# (While Experiment script runs, Validate the Engineered Feature Explanation will function inside the script)\n",
    "\n",
    "# Split training features into numeric and categoric dataframes\n",
    "numeric_X_test = pd.DataFrame(X_test[df_numeric_column_names], dtype=np.str, columns=df_numeric_column_names)\n",
    "categoric_X_test = pd.DataFrame(X_test[df_categorical_column_names], dtype=np.str, columns=df_categorical_column_names)\n",
    "# Fit and Run the numeric and categoric ColumnTransformers on the split dataframes to perform feature engineering\n",
    "preprocessor.transformers[0][1].fit(numeric_X_test)\n",
    "preprocessor.transformers[1][1].steps[0][1].fit(categoric_X_test)\n",
    "numeric_X_test_preprocessed = preprocessor.transformers[0][1].transform(numeric_X_test)\n",
    "numeric_X_test_preprocessed = pd.DataFrame(numeric_X_test_preprocessed, dtype=np.float, columns=df_numeric_column_names)\n",
    "categoric_X_test_preprocessed = preprocessor.transformers[1][1].steps[0][1].transform(categoric_X_test)\n",
    "# Fit OneHotEncoder\n",
    "preprocessor.transformers[1][1].steps[1][1].fit(categoric_X_test_preprocessed)\n",
    "# Get new One Hot Encoded column names\n",
    "print(categoric_X_test_preprocessed)\n",
    "print(df_categorical_column_names)\n",
    "df_encoded_categorical_column_names = preprocessor.transformers[1][1].steps[1][1].get_feature_names(df_categorical_column_names)\n",
    "print(\"df_encoded_categorical_column_names\", df_encoded_categorical_column_names)\n",
    "# Transform categoric, null-imputed features with fitted OneHotEncoder\n",
    "categoric_X_test_preprocessed = preprocessor.transformers[1][1].steps[1][1].transform(categoric_X_test_preprocessed)\n",
    "# Turn preprocessed categoric features into a DataFrame\n",
    "categoric_X_test_preprocessed = pd.DataFrame(categoric_X_test_preprocessed, dtype=np.float64, columns=df_encoded_categorical_column_names)\n",
    "\n",
    "# Combine the numeric DF with the categorical DF to submit to the classifier_pipeline\n",
    "X_test_preprocessed_list = [numeric_X_test_preprocessed, categoric_X_test_preprocessed]\n",
    "X_test_preprocessed = pd.concat(X_test_preprocessed_list, axis=1)\n",
    "\n",
    "\n",
    "# Save engineered features' names to create TabularExplainer with them\n",
    "engineeredFeatures=[*df_numeric_column_names, *df_encoded_categorical_column_names]\n",
    "\n",
    "# Fit the model\n",
    "classifier_pipeline.steps[-1][1].fit(X_test_preprocessed, y_test)\n",
    "# Explain in terms of engineered features\n",
    "# NOTE: classifier_pipeline.steps[-1][1] contains the Model\n",
    "# NOTE: \"features\" and \"classes\" fields are optional for TabularExplainers\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "engineered_explainer = TabularExplainer(classifier_pipeline.steps[-1][1],\n",
    "                                     initialization_examples=X_test_preprocessed,\n",
    "                                     features=engineeredFeatures)\n",
    "# Explain results with this Explainer and upload the Explanation...\n",
    "\n",
    "# Get Global Explanations of raw features, global as in 'of total data'...\n",
    "# You can use the training data or the test data here, but test data would allow you to use Explanation Exploration\n",
    "# print(\"X_test, line value before engineered_explainer.explain_global: \\n\" + str(X_test))\n",
    "global_explanation = engineered_explainer.explain_global(X_test_preprocessed, y_test)\n",
    "# If you used the PFIExplainer in the previous step, use the next line of code instead\n",
    "# global_explanation = engineered_explainer.explain_global(X_test, true_labels=y_train)\n",
    "# Sorted feature importance values and feature names\n",
    "sorted_global_importance_values = global_explanation.get_ranked_global_values()\n",
    "sorted_global_importance_names = global_explanation.get_ranked_global_names()\n",
    "globalFeatureExplanations = dict(zip(sorted_global_importance_names, sorted_global_importance_values))\n",
    "print('globalFeatureExplanations: ', globalFeatureExplanations)\n",
    "# Alternatively, you can print out a dictionary that holds the top K feature names and values\n",
    "print('global_explanation.get_feature_importance_dict(): ', global_explanation.get_feature_importance_dict())\n",
    "\n",
    "# Upload the explanation in terms of engineered features\n",
    "from azureml.interpret import ExplanationClient\n",
    "client = ExplanationClient.from_run(AutoML_run)\n",
    "client.upload_model_explanation(global_explanation, true_ys=y_test.values.ravel(), comment='global explanation: test dataset features, engineered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: LocalDocker_Training_AutoML_1653229799_05867879\n",
      "Web View: https://ml.azure.com/runs/LocalDocker_Training_AutoML_1653229799_05867879?wsid=/subscriptions/4d278f3d-b4fd-4fa2-86b6-d34b96bc888f/resourcegroups/Foxy_Resources/workspaces/automlbook&tid=198c7d8c-e010-45ce-a018-ec2d9a33f58f\n",
      "\n",
      "Streaming azureml-logs/60_control_log.txt\n",
      "=========================================\n",
      "\n",
      "[2022-05-22T14:30:01.798428] Using urllib.request Python 3.0 or later\n",
      "Streaming log file azureml-logs/60_control_log.txt\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 8992\n",
      "nvidia-docker is installed on the target. Using nvidia-docker for docker operations.\n"
     ]
    },
    {
     "ename": "ExperimentExecutionException",
     "evalue": "ExperimentExecutionException:\n\tMessage: The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"The output streaming for the run interrupted.\\nBut the run is still executing on the compute target. \\nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.7.13/lib/python3.7/site-packages/azureml/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    843\u001b[0m                     \u001b[0mwait_post_processing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_post_processing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m                     raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.13/lib/python3.7/site-packages/azureml/core/run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[0;34m(self, file_handle, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_before_polling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpoll_start_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO use FileWatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mExperimentExecutionException\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8645/430856759.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# TODO get this working\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mAutoML_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.13/lib/python3.7/site-packages/azureml/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    850\u001b[0m                                 \u001b[0;34m\"https://aka.ms/aml-docs-cancel-run\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mExperimentExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0mrunning_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRUNNING_STATES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExperimentExecutionException\u001b[0m: ExperimentExecutionException:\n\tMessage: The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"The output streaming for the run interrupted.\\nBut the run is still executing on the compute target. \\nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "# Register Model from the AutoML_run\n",
    "description = \"Best LocalDocker AutoML Regression Run using Titanic Sample Data.\"\n",
    "tags = {\n",
    "  \"project\" : \"LocalDocker Training AutoML\", \n",
    "  \"creator\": \"fox\", \n",
    "  \"task\": \"classification\", \n",
    "  \"dataset\": \"automlbook Titanic Training Data A\", \n",
    "  \"metric\": \"normalized_root_mean_squared_error\"\n",
    "}\n",
    "\n",
    "# Attempt to register model once output model file is available\n",
    "from azureml.core import Model\n",
    "import sklearn\n",
    "import time\n",
    "# TODO get this working\n",
    "AutoML_run.wait_for_completion(show_output=True)\n",
    "while True:\n",
    "  try:\n",
    "    AutoML_run.register_model(model_path='./outputs', model_name=out_model_file_name, description=description, tags=tags,\n",
    "                            model_framework=Model.Framework.SCIKITLEARN, # Framework used to create the model.\n",
    "                            model_framework_version=sklearn.__version__)  # Version of scikit-learn used to create the model.)\n",
    "    break\n",
    "  except:\n",
    "      print (\"encountered exception registering model output file, waiting and trying again...\") \n",
    "      time.sleep(60)\n",
    "# Set output file name like 'DecisionTreeClassifier_Titanic_LocalDocker-2022-04-17 21:40:36.114550.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Now done in script instead of this Notebook) Upload global model explanation data...\n",
    "# from azureml.interpret import ExplanationClient\n",
    "\n",
    "# The explanation can then be downloaded on any compute\n",
    "# Multiple explanations can be uploaded\n",
    "# print(\"y_test value the line before client.upload_model_explanation(): \\n\" + str(y_test))\n",
    "# print(\"y_test.values.ravel() value passed as true_ys to client.upload_model_explanation(): \\n\" + str(y_test.values.ravel()))\n",
    "# client = ExplanationClient.from_run(AutoML_run)\n",
    "# client.upload_model_explanation(global_explanation, true_ys=y_test.values.ravel(), comment='global explanation: all features')\n",
    "\n",
    "# Or you can only upload the explanation object with the top k feature info with this...\n",
    "# client.upload_model_explanation(global_explanation, top_k=2, comment='global explanation: Only top 2 features')\n",
    "# END Upload global model explanation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Experiment Run that was ran, Get Global Explanations (Downloaded with the Experiment Run object!)\n",
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run(AutoML_run)\n",
    "\n",
    "# Get model explanation data\n",
    "# TODO? There should be a way to download_model_explanation by comment instead of id.\n",
    "print(client.list_model_explanations())\n",
    "# NOTE: this step will fail until you copy the explanation_id values from the printed list of explanations\n",
    "engineered_global_explanation_test = client.download_model_explanation(explanation_id='2ef948ba-9b01-453e-80da-f1c498022e5b')\n",
    "engineered_global_explanation_train = client.download_model_explanation(explanation_id='846c159e-5ee7-4c79-8994-8757055de83a')\n",
    "# TODO? Do something with the Engineered Feature explanations here\n",
    "global_explanation = client.download_model_explanation(explanation_id='6eac21c3-891c-4d43-beae-62eb7b5b9965')\n",
    "\n",
    "\n",
    "\n",
    "# Or only get the top k (e.g., 4) most important features with their importance values\n",
    "# explanation = client.download_model_explanation(top_k=4)\n",
    "\n",
    "global_importance_values = global_explanation.get_ranked_global_values()\n",
    "global_importance_names = global_explanation.get_ranked_global_names()\n",
    "print('global importance values: {}'.format(global_importance_values))\n",
    "print('global importance names: {}'.format(global_importance_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Model from AzureML and Visualize Explanations with it\n",
    "from raiwidgets import ExplanationDashboard\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "import jinja2\n",
    "\n",
    "# print('Model.list(ws)', Model.list(ws))\n",
    "\n",
    "# Download the Model from Azure\n",
    "\n",
    "# Try 1: just Model constructor and joblib.load()\n",
    "# downloaded_model = Model(ws, out_model_file_name)\n",
    "# joblib.load(downloaded_model)\n",
    "\n",
    "# Try 2: Use Model.get_model_path and joblib.load()\n",
    "# remote_model_path = Model.get_model_path(out_model_file_name, _workspace=ws)\n",
    "# downloaded_model = joblib.load(remote_model_path)\n",
    "\n",
    "# Try 3: Use Model.download and joblib.load()\n",
    "remote_model_obj = Model(ws, out_model_file_name)\n",
    "print('Name:', remote_model_obj.name)\n",
    "print('Version:', remote_model_obj.version)\n",
    "remote_model_path = remote_model_obj.download(exist_ok = True)\n",
    "downloaded_model = joblib.load(remote_model_path)\n",
    "\n",
    "# BEGIN Access \"Local Explanations\", uncomment these lines if you want to do that here...\n",
    "# (Local Explanation meaning \"of individual predictions\") \n",
    "# from interpret.ext.blackbox import TabularExplainer\n",
    "# \"features\" and \"classes\" fields are optional\n",
    "# explainer = TabularExplainer(downloaded_model,\n",
    "                            #  X_train,\n",
    "                            #  features=features)\n",
    "#\n",
    "# Get explanation for the first few data points in the train set\n",
    "# local_explanation = explainer.explain_local(X_train[0:5])\n",
    "# Sorted feature importance values and feature names\n",
    "# sorted_local_importance_names = local_explanation.get_ranked_local_names()\n",
    "# print('sorted_local_importance_names: ', sorted_local_importance_names)\n",
    "# print('len(sorted_local_importance_names): ', len(sorted_local_importance_names))\n",
    "# sorted_local_importance_values = local_explanation.get_ranked_local_values()\n",
    "# print('sorted_local_importance_values: ', sorted_local_importance_values)\n",
    "# print('len(sorted_local_importance_values): ', len(sorted_local_importance_values))\n",
    "# COOL THING TO DO: Sometime could get local explanation of specific data points uploaded, downloaded, and visualized as well...\n",
    "# END Access \"Local Explanations\"\n",
    "\n",
    "# Visualize explanations\n",
    "# Be sure to pass dataset=(train feature columns Dataframe) and true_y=(train predicted column Dataframe)\n",
    "#\n",
    "# TODO Get base statistics to appear in visualization's Model Performance tab: \n",
    "# TODO      (the accuracy, precision, f1 scores, false positive rates, false negative rates...)\n",
    "#\n",
    "# TODO? Show both raw and engineered features from raiwidgets ExplanationDashboard like this.\n",
    "#\n",
    "#       1) getting the raiwidgets thing working\n",
    "#       2) see README at https://github.com/interpretml/interpret\n",
    "ExplanationDashboard(global_explanation, downloaded_model, dataset=X_train, true_y=y_train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46ae9456b2737841401e3d3c7acdf327031e52ebe64544d23b06b17169cc2049"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit ('3.7.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
