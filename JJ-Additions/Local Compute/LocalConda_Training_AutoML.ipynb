{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Datastore, Environment\n",
    "from azureml.core import Experiment\n",
    "import azureml.interpret\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set (System Managed) Local Environment up\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "system_managed_env = Environment(\"system-managed-env\")\n",
    "# Editing a run configuration property on-fly.\n",
    "system_managed_env.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify conda dependencies with scikit-learn\n",
    "conda_packages = ['pip',\n",
    "                  'pyspark',\n",
    "                  'scikit-learn'\n",
    "                 ]\n",
    "pip_packages =   ['azureml.interpret',\n",
    "                  'azureml-dataset-runtime',\n",
    "                  'jinja2',\n",
    "                  'MarkupSafe',\n",
    "                  'raiwidgets'\n",
    "                 ]\n",
    "                 \n",
    "condaDependencies = CondaDependencies.create(conda_packages=conda_packages, pip_packages=pip_packages)\n",
    "system_managed_env.python.conda_dependencies = condaDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Workspace object from Azure\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "# You can find tenant id under azure active directory->properties\n",
    "tenant_id = '198c7d8c-e010-45ce-a018-ec2d9a33f58f'\n",
    "ia = InteractiveLoginAuthentication(tenant_id=tenant_id)\n",
    "ws_name = 'automlbook'\n",
    "subscription_id = '4d278f3d-b4fd-4fa2-86b6-d34b96bc888f'\n",
    "resource_group = 'Foxy_Resources'\n",
    "ws = Workspace.get(name=ws_name,\n",
    "                   subscription_id=subscription_id,\n",
    "                   resource_group=resource_group,\n",
    "                   auth=ia)\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datastore, try getting datastore via Workspace object\n",
    "datastore = Datastore.get_default(ws)\n",
    "datastore_name = 'workspaceblobstore'\n",
    "datastore = Datastore.get(ws, datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the datastore of the Workspace\n",
    "dataset_name = 'automlbook Titanic Training Data A'\n",
    "# dataset = Dataset.get_by_name(ws, dataset_name)\n",
    "dataset = Dataset.get_by_name(ws, dataset_name, version = 'latest')\n",
    "dataset_columns = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "\n",
    "# Show a sample of the data in the dataset\n",
    "dataset.take(10).to_pandas_dataframe()\n",
    "\n",
    "# Turn Dataset into Pandas Dataframe, it is to be preprocessed\n",
    "df = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (First way I did this, commented out) Preprocess numeric columns\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# df_column_names = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "# df_numeric_column_names = ['Age', 'Fare']\n",
    "\n",
    "# # BEGIN Create dataframe with numeric columns so that it contains all numbers that are preprocessed...\n",
    "\n",
    "# # For int column Age, Impute NaN numeric values, and Remove outliers\n",
    "# print('Before Removing outliers or Imputing null values, df[Age]: ', df['Age'])\n",
    "# ageMedian = np.nanmedian(df['Age'])\n",
    "# print('ageMedian: ', ageMedian)\n",
    "# df['Age'] = np.where(np.isnan(df['Age']), ageMedian, df['Age'])\n",
    "# print('Before Removing outliers and after Imputing null values, df[Age]: ', df['Age'])\n",
    "\n",
    "# # Calculate 3STD and Mean for Age\n",
    "# ageThreeSD = np.std(df['Age']) * 3\n",
    "# ageMean = np.mean(df['Age'])\n",
    "# ageOutlierThreshold = round(ageThreeSD + ageMean)\n",
    "# print('Age Outlier Threshold: ', ageOutlierThreshold)\n",
    "\n",
    "# # Remove Outliers by replacing all values above Threshold (3STD + Mean) with Threshold Value\n",
    "# df['Age'] = df['Age'].mask(df['Age'] > ageOutlierThreshold, ageOutlierThreshold)\n",
    "# print('After Removing outliers and Imputing null values, df[Age]: ', df['Age'])\n",
    "\n",
    "# # Copy df, keeping only Age column, set type of this df copy to float\n",
    "# df_age_column = pd.DataFrame(df['Age'], columns=['Age'])\n",
    "\n",
    "\n",
    "# # Copy df, keeping only float numeric columns, set type of this df copy to float\n",
    "# df_float_column_names = ['Fare']\n",
    "# print('df_float_column_names: ', df_float_column_names)\n",
    "# df_float_columns = pd.DataFrame(df[df_float_column_names], dtype=np.float, columns=df_float_column_names)\n",
    "\n",
    "# # Concatenate the numeric Data Frames to scale them\n",
    "# print('Before concatenation to df_numeric_columns, df[Age]: ', df['Age'])\n",
    "# print('Before concatenation to df_numeric_columns, df_age_column: ', df_age_column)\n",
    "# df_numeric_columns = pd.concat([df_age_column, df_float_columns], keys=df_numeric_column_names, axis=1)\n",
    "# print('concatenated df_numeric_columns: ', df_numeric_columns)\n",
    "\n",
    "\n",
    "# # Use StandardScaler or MinMaxScaler on Numeric/Non-Categorical columns split\n",
    "# scaler = StandardScaler().fit(df_numeric_columns)\n",
    "# print('scaler.mean_: ', scaler.mean_)\n",
    "# print('scaler.scale: ', scaler.scale_)\n",
    "\n",
    "# df_scaled_numeric_columns =  pd.DataFrame(scaler.transform(df_numeric_columns), columns=df_numeric_column_names)\n",
    "# print('df_scaled_numeric_columns: ', df_scaled_numeric_columns)\n",
    "# # Scaled data should have zero mean and unit variance, check with these prints:\n",
    "# print('df_scaled_numeric_columns.mean(axis=0): ', df_scaled_numeric_columns.mean(axis=0))\n",
    "# print('df_scaled_numeric_columns.std(axis=0)', df_scaled_numeric_columns.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (First way I did this, commented out) Preprocess categorical columns\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # BEGIN Create dataframe with categorical columns so that it contains all categorical data that is preprocessed...\n",
    "\n",
    "# # Copy df, keeping only categorical columns, and one-hot encode them\n",
    "# df_categorical_column_names_raw = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin', 'Embarked']\n",
    "# # print('df_categorical_column_names_raw: ', df_categorical_column_names_raw)\n",
    "# df_categorical_columns = pd.DataFrame(df[df_categorical_column_names_raw], dtype=np.str, columns=df_categorical_column_names_raw)\n",
    "# # print('df_categorical_columns: ', df_categorical_columns)\n",
    "# encoder = OneHotEncoder(drop='first', handle_unknown='error', sparse=False, ).fit(df_categorical_columns)\n",
    "# # print('encoder.categories: ', encoder.categories)\n",
    "# df_encoded_categorical_columns = pd.DataFrame(encoder.transform(df_categorical_columns))\n",
    "# df_encoded_categorical_columns.columns = encoder.get_feature_names(df_categorical_column_names_raw)\n",
    "# # print('df_encoded_categorical_columns: ', df_encoded_categorical_columns)\n",
    "# #   By default, the values each feature can take is inferred automatically from the dataset and can be found in the categories_ attribute:\n",
    "\n",
    "\n",
    "# # Combine the numeric DF with the categorical DF\n",
    "# dfs = [df['Survived'], df_scaled_numeric_columns, df_encoded_categorical_columns]\n",
    "# # print('Before concatenation to dfTyped, df[\\'Survived\\']: ', df['Survived'])\n",
    "# # print('Before concatenation to dfTyped, df[Age]: ', df['Age'])\n",
    "# # print('Before concatenation to dfTyped, df_numeric_columns: ', df_numeric_columns)\n",
    "# # print('Before concatenation to dfTyped, df_scaled_numeric_columns: ', df_scaled_numeric_columns)\n",
    "# TODO maybe, shouldn't column names be passed in to the keys list here?\n",
    "# dfTyped = pd.concat(dfs, axis=1, keys=['Survived', *df_scaled_numeric_columns, *df_encoded_categorical_columns])\n",
    "# # print('dfTyped: ', dfTyped)\n",
    "# # print('dfTyped[Age]: ', dfTyped['Age'])\n",
    "\n",
    "#         # - With sklearn.preprocessing, preprocess your Dataframes before training model in the Python Script\n",
    "#         #     - [Guide at SciKit Learn site](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "#         #     - Use OneHotEncoder\n",
    "#         #     - Use StandardScaler or  MinMaxScaler while you're at it\n",
    "#         #     - Don't worry about any other preprocessing to just get the training working\n",
    "#         #     - Strategy:\n",
    "#         #         - d Split dataframe into Numeric/Non-Categorial and Non-Numeric/Categorial columns\n",
    "#         #             - ! Use StandardScaler or MinMaxScaler on Numeric/Non-Categorical columns split\n",
    "#         #             - d Use OneHotEncoder on Non-Numeric/Categorical columns split\n",
    "\n",
    "# # Initial Data Frame is now preprocessed in dfPreprocessed\n",
    "# dfPreprocessed = dfTyped\n",
    "# # print('dfPreprocessed: ', dfPreprocessed)\n",
    "\n",
    "# # Split DataFrame for training now that it is pre-processed\n",
    "# target_column_name = 'Survived'\n",
    "# df_x = dfPreprocessed.drop([target_column_name], axis=1)\n",
    "# df_y = dfPreprocessed[target_column_name]\n",
    "# # print(\"See df_x\", df_x)\n",
    "# print(\"See df_y\", df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Dataframe to get one for Numeric columns and one for Categorical columns\n",
    "\n",
    "df_column_names = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "\n",
    "##### BEGIN Create dataframe with numeric columns so that it contains all numbers that are preprocessed...\n",
    "df_numeric_column_names = ['Age', 'Fare']\n",
    "# Copy df, keeping only float numeric columns, set type of this df copy to float\n",
    "df_float_column_names = ['Fare']\n",
    "df_float_columns = pd.DataFrame(df[df_float_column_names], dtype=np.float, columns=df_float_column_names)\n",
    "# # Copy df, keeping only integer Age column to leave as an integer\n",
    "df_integer_column = pd.DataFrame(df['Age'], columns=['Age'])\n",
    "# Concatenate the numeric DataFrames\n",
    "df_numeric_columns = pd.concat([df_integer_column, df_float_columns], axis=1)\n",
    "\n",
    "\n",
    "##### BEGIN Create dataframe with categorical columns so that it contains all categorical data that is preprocessed...\n",
    "df_categorical_column_names = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin', 'Embarked']\n",
    "# Copy df, keeping only categorical columns\n",
    "df_categorical_columns = pd.DataFrame(df[df_categorical_column_names], dtype=np.str, columns=df_categorical_column_names)\n",
    "\n",
    "print('concatenated df_numeric_columns: ', df_numeric_columns)\n",
    "print('df_categorical_columns: ', df_categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO reorder things to get Data Frame as it is after transformation with the ColumnTransformer preprocessor\n",
    "\n",
    "# Create preprocessor to preprocess numeric and categorical columns (with Transfomer API via ColumnTransformer, including creation of an Explainer)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', one_hot_encoder)])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, df_numeric_column_names),\n",
    "        ('cat', categorical_transformer, df_categorical_column_names)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the numeric DF with the categorical DF\n",
    "# print(\"df['Survived'] is \", df['Survived'])\n",
    "# print(\"df_numeric_columns is \", df_numeric_columns)\n",
    "# print(\"df_numeric_columns.columns is \", df_numeric_columns.columns)\n",
    "# print(\"df_categorical_columns is \", df_categorical_columns)\n",
    "# print(\"df_categorical_columns.columns is \", df_categorical_columns.columns)\n",
    "\n",
    "# Concatenate dfs to get DataFrame of all columns to submit to the classifier_pipeline\n",
    "dfs = [df['Survived'], df_numeric_columns, df_categorical_columns]\n",
    "# print(\"dfs is\" + str(dfs))\n",
    "# print('Before concatenation to dfTyped, df[\\'Survived\\']: ', df['Survived'])\n",
    "# print('Before concatenation to dfTyped, df_numeric_columns: ', df_numeric_columns)\n",
    "dfTyped = pd.concat(dfs, axis=1)\n",
    "print('dfTyped: ', dfTyped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pre-transformation Data Frame into feature/target columns\n",
    "target_column_name = 'Survived'\n",
    "df_x_pre_transformation = dfTyped.drop([target_column_name], axis=1)\n",
    "df_y_pre_transformation = dfTyped[target_column_name].ravel()\n",
    "print(\"See df_x_pre_transformation\", df_x_pre_transformation)\n",
    "print(\"See df_y_pre_transformation\", df_y_pre_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfom Data Frame and get new One Hot Encoded column names\n",
    "\n",
    "# Get the preprocessed Data Frame columns in a list\n",
    "# print(str(preprocessor))\n",
    "# print(str(preprocessor.transformers_[1][1]\\\n",
    "   # .named_steps['onehot']))\n",
    "one_hot_encoder.fit(df_categorical_columns)\n",
    "df_encoded_categorical_column_names = one_hot_encoder.get_feature_names(df_categorical_column_names)\n",
    "# print(str(df_encoded_categorical_column_names))\n",
    "\n",
    "# TODO (dfProcessed doesn't look right...) Get the preprocessed Data Frame\n",
    "dfPreprocessed = preprocessor.fit_transform(df_x_pre_transformation, df_y_pre_transformation)\n",
    "print(dfPreprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline (Then we have a full prediction pipeline)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "regressor = LogisticRegression(solver='lbfgs')\n",
    "classifier_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', regressor)])\n",
    "# Now we have a full prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split DataFrame for training now that it is pre-processed\n",
    "# print('dfPreprocessed: ', dfPreprocessed)\n",
    "\n",
    "# Split Data Frame into feature/target columns\n",
    "target_column_name = 'Survived'\n",
    "df_x = dfPreprocessed.drop([target_column_name], axis=1)\n",
    "df_y = dfPreprocessed[target_column_name]\n",
    "# print(\"See df_x\", df_x)\n",
    "print(\"See df_y\", df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Pandas Dataframe of base df_x and df_y\n",
    "Dataset.Tabular.register_pandas_dataframe(df_x, datastore, \"Titanic Feature Column Data for train_test_split usage (LocalConda notebook)\")\n",
    "Dataset.Tabular.register_pandas_dataframe(df_y, datastore, \"Titanic Target Column Data for train_test_split usage (LocalConda notebook)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test data, register the resulting Datasets with Azure\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "# What you need to pass to train_test_split...\n",
    "# ... I need X and Y dataframe, X just with target missing, Y just with target column present\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "# print(\"See y_test\", y_test)\n",
    "print(\"See y_test.columns.tolist()\", str(y_test.columns.tolist()))\n",
    "# print(\"See y_test.values.tolist() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.tolist())\n",
    "# print(\"See y_test.values.ravel() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.ravel())\n",
    "# print(\"See y_test.values.tolist().flatten() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.flatten())\n",
    "\n",
    "# Register the splits\n",
    "Dataset.Tabular.register_pandas_dataframe(X_train, datastore, \"Titanic Feature Column Data for training (LocalConda notebook)\")\n",
    "Dataset.Tabular.register_pandas_dataframe(X_test, datastore, \"Titanic Feature Column Data for testing (LocalConda notebook)\")\n",
    "Dataset.Tabular.register_pandas_dataframe(y_train, datastore, \"Titanic Target Column Data for training (LocalConda notebook)\")\n",
    "Dataset.Tabular.register_pandas_dataframe(y_test, datastore, \"Titanic Target Column Data for testing (LocalConda notebook)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names\n",
    "features=[*df_numeric_column_names, *df_encoded_categorical_column_names]\n",
    "\n",
    "# classifier_pipeline.steps[-1][1] returns the trained classification model\n",
    "# pass transformation as an input to create the explanation object\n",
    "# \"features\" and \"classes\" fields are optional\n",
    "explainer = TabularExplainer(classifier_pipeline.steps[-1][1],\n",
    "                                     initialization_examples=X_train,\n",
    "                                     features=features,\n",
    "                                     transformations=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain results with Explainer and upload the explanation\n",
    "\n",
    "# BEGIN Get Global Explanations, global as in 'of total data'...\n",
    "\n",
    "# You can use the training data or the test data here, but test data would allow you to use Explanation Exploration\n",
    "# print(\"X_test, line value before explainer.explain_global: \\n\" + str(X_test))\n",
    "global_explanation = explainer.explain_global(X_test, y_test)\n",
    "# If you used the PFIExplainer in the previous step, use the next line of code instead\n",
    "# global_explanation = explainer.explain_global(x_train, true_labels=y_train)\n",
    "# Sorted feature importance values and feature names\n",
    "sorted_global_importance_values = global_explanation.get_ranked_global_values()\n",
    "sorted_global_importance_names = global_explanation.get_ranked_global_names()\n",
    "globalFeatureExplanations = dict(zip(sorted_global_importance_names, sorted_global_importance_values))\n",
    "print('globalFeatureExplanations: ', globalFeatureExplanations)\n",
    "# Alternatively, you can print out a dictionary that holds the top K feature names and values\n",
    "print('global_explanation.get_feature_importance_dict(): ', global_explanation.get_feature_importance_dict())\n",
    "\n",
    "# BEGIN Get local explanations of individual predictions\n",
    "# Get explanation for the first few data points in the test set\n",
    "# local_explanation = explainer.explain_local(X_test[0:5])\n",
    "# Sorted feature importance values and feature names\n",
    "# sorted_local_importance_names = local_explanation.get_ranked_local_names()\n",
    "# print('sorted_local_importance_names: ', sorted_local_importance_names)\n",
    "# print('len(sorted_local_importance_names): ', len(sorted_local_importance_names))\n",
    "# sorted_local_importance_values = local_explanation.get_ranked_local_values()\n",
    "# print('sorted_local_importance_values: ', sorted_local_importance_values)\n",
    "# print('len(sorted_local_importance_values): ', len(sorted_local_importance_values)) \n",
    "# \n",
    "# THIS DOES NOT WORK LIKE IT DOES WITH THE GLOBAL_EXPLANATION, HOWEVER!\n",
    "# client.upload_model_explanation(sorted_local_importance_values, comment='local explanation for data points 0-5: all features')\n",
    "#\n",
    "# END Get local explanations of individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode features (names) list into a string like '[\"a\",\"b\"]'\n",
    "for x in range(len(features)):\n",
    "        features[x] = '\"{}\"'.format(features[x])\n",
    "featuresEncoded = \"[{}]\".format(\",\".join(features))\n",
    "print(featuresEncoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training Experiment locally\n",
    "# TODO use classifier_pipeline, perhaps move that code to create and use the Pipeline into the python script\n",
    "# TODO (Consider what minimally needs to be in notebook vs script)\n",
    "from azureml.core import ScriptRunConfig\n",
    "import datetime\n",
    "\n",
    "# Experiment\n",
    "experiment_name = 'LocalConda_Training_AutoML'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "# Define Compute Cluster to use\n",
    "compute_target = 'local'\n",
    "source_directory = './scripts'\n",
    "script_name = 'localCondaTrainingAutoML.py'\n",
    "dataset_name = 'automlbook Titanic Training Data A'\n",
    "# set output file name like 'DecisionTreeClassifier_Titanic_LocalConda-2022-04-17 21:40:36.114550.pkl'\n",
    "suffix = 'local-' + str(datetime.datetime.now())\n",
    "suffix = suffix.replace(' ', '_') # Clean up datetimestamp\n",
    "suffix = suffix.replace(':', '-') \n",
    "out_model_file_name = 'DecisionTreeClassifier_Titanic_LocalConda_{}.pkl'.format(suffix)\n",
    "# set output file name like 'DecisionTreeClassifier_Titanic_LocalConda-2022-04-17 21:40:36.114550.pkl'\n",
    "\n",
    "script_arguments = [\n",
    "\"--tenant-id\", tenant_id,\n",
    "\"--ws-name\", ws_name,\n",
    "\"--subscription-id\", subscription_id,\n",
    "\"--resource-group\", resource_group,\n",
    "\"--datastore-name\", datastore_name,\n",
    "\"--out-model-file-name\", out_model_file_name,\n",
    "\"--features\", featuresEncoded\n",
    "]\n",
    "scriptRunConfig = ScriptRunConfig(\n",
    "        source_directory=source_directory,\n",
    "        script=script_name,\n",
    "        arguments=script_arguments,\n",
    "        environment=system_managed_env,\n",
    "        compute_target=compute_target)\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] = \"/home/johna/miniconda3/bin:\" + os.environ[\"PATH\"]\n",
    "AutoML_run = experiment.submit(scriptRunConfig)\n",
    "RunDetails(AutoML_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload global model explanation data...\n",
    "# The explanation can then be downloaded on any compute\n",
    "# Multiple explanations can be uploaded\n",
    "print(\"y_test value the line before client.upload_model_explanation(): \\n\" + str(y_test))\n",
    "print(\"y_test.values.ravel() value passed as true_ys to client.upload_model_explanation(): \\n\" + str(y_test.values.ravel()))\n",
    "client = ExplanationClient.from_run(AutoML_run)\n",
    "client.upload_model_explanation(global_explanation, true_ys=y_test.values.ravel(), comment='global explanation: all features')\n",
    "\n",
    "# Or you can only upload the explanation object with the top k feature info with this...\n",
    "# client.upload_model_explanation(global_explanation, top_k=2, comment='global explanation: Only top 2 features')\n",
    "# END Upload global model explanation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Model from the AutoML_run\n",
    "description = \"Best LocalConda AutoML Regression Run using Titanic Sample Data.\"\n",
    "tags = {\n",
    "  \"project\" : \"Local Training AutoML\", \n",
    "  \"creator\": \"fox\", \n",
    "  \"task\": \"classification\", \n",
    "  \"dataset\": \"automlbook Titanic Training Data A\", \n",
    "  \"metric\": \"normalized_root_mean_squared_error\"\n",
    "}\n",
    "\n",
    "# Attempt to register model once output model file is available\n",
    "from azureml.core import Model\n",
    "import sklearn\n",
    "import time\n",
    "while True:\n",
    "  try:\n",
    "    AutoML_run.register_model(model_path='./outputs', model_name=out_model_file_name, description=description, tags=tags,\n",
    "                            model_framework=Model.Framework.SCIKITLEARN, # Framework used to create the model.\n",
    "                            model_framework_version=sklearn.__version__)  # Version of scikit-learn used to create the model.)\n",
    "    break\n",
    "  except:\n",
    "      print (\"encountered exception registering model output file, waiting and trying again...\") \n",
    "      time.sleep(60)\n",
    "# Set output file name like 'DecisionTreeClassifier_Titanic_LocalConda-2022-04-17 21:40:36.114550.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Experiment Run that was ran, Get Global Explanations (Downloaded with the Experiment Run object!)\n",
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run(AutoML_run)\n",
    "\n",
    "# get model explanation data\n",
    "global_explanation = client.download_model_explanation()\n",
    "# or only get the top k (e.g., 4) most important features with their importance values\n",
    "# explanation = client.download_model_explanation(top_k=4)\n",
    "\n",
    "global_importance_values = global_explanation.get_ranked_global_values()\n",
    "global_importance_names = global_explanation.get_ranked_global_names()\n",
    "print('global importance values: {}'.format(global_importance_values))\n",
    "print('global importance names: {}'.format(global_importance_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Model from AzureML and Visualize Explanations with it\n",
    "from raiwidgets import ExplanationDashboard\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "import jinja2\n",
    "# from jinja2 import escape\n",
    "# from MarkupSafe import escape\n",
    "# print(version(jinja2))\n",
    "\n",
    "# print('Model.list(ws)', Model.list(ws))\n",
    "\n",
    "# Download the Model from Azure\n",
    "\n",
    "# Try 1: just Model constructor and joblib.load()\n",
    "# downloaded_model = Model(ws, out_model_file_name)\n",
    "# joblib.load(downloaded_model)\n",
    "\n",
    "# Try 2: Use Model.get_model_path and joblib.load()\n",
    "# remote_model_path = Model.get_model_path(out_model_file_name, _workspace=ws)\n",
    "# downloaded_model = joblib.load(remote_model_path)\n",
    "\n",
    "# Try 3: Use Model.download and joblib.load()\n",
    "remote_model_obj = Model(ws, out_model_file_name)\n",
    "print('Name:', remote_model_obj.name)\n",
    "print('Version:', remote_model_obj.version)\n",
    "remote_model_path = remote_model_obj.download(exist_ok = True)\n",
    "downloaded_model = joblib.load(remote_model_path)\n",
    "\n",
    "# BEGIN Access \"Local Explanations\"\n",
    "# (Local Explanation meaning \"of individual predictions\") \n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "# \"features\" and \"classes\" fields are optional\n",
    "explainer = TabularExplainer(downloaded_model,\n",
    "                             X_train,\n",
    "                             features=features)\n",
    "\n",
    "# Get explanation for the first few data points in the test set\n",
    "local_explanation = explainer.explain_local(X_test[0:5])\n",
    "# Sorted feature importance values and feature names\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()\n",
    "# print('sorted_local_importance_names: ', sorted_local_importance_names)\n",
    "print('len(sorted_local_importance_names): ', len(sorted_local_importance_names))\n",
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()\n",
    "# print('sorted_local_importance_values: ', sorted_local_importance_values)\n",
    "print('len(sorted_local_importance_values): ', len(sorted_local_importance_values))\n",
    "# END Access \"Local Explanations\"\n",
    "\n",
    "# TODO get local explanation uploaded, downloaded, and visualized as well...\n",
    "\n",
    "# Visualize explanations\n",
    "# Be sure to pass dataset=(test feature columns Dataframe) and true_y=(test predicted column Dataframe)\n",
    "ExplanationDashboard(global_explanation, downloaded_model, dataset=X_test, true_y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46ae9456b2737841401e3d3c7acdf327031e52ebe64544d23b06b17169cc2049"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit ('3.7.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
