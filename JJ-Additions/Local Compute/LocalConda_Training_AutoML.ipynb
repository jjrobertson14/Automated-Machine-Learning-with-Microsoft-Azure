{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Datastore, Environment\n",
    "from azureml.core import Experiment\n",
    "import azureml.interpret\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set (System Managed) Local Environment up\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "system_managed_env = Environment(\"system-managed-env\")\n",
    "# Editing a run configuration property on-fly.\n",
    "system_managed_env.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify conda dependencies with scikit-learn\n",
    "conda_packages = ['pip',\n",
    "                  'pyspark',\n",
    "                  'scikit-learn'\n",
    "                 ]\n",
    "pip_packages =   ['azureml.interpret',\n",
    "                  'azureml-dataset-runtime',\n",
    "                  'jinja2',\n",
    "                  'MarkupSafe',\n",
    "                  'raiwidgets'\n",
    "                 ]\n",
    "                 \n",
    "condaDependencies = CondaDependencies.create(conda_packages=conda_packages, pip_packages=pip_packages)\n",
    "system_managed_env.python.conda_dependencies = condaDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Workspace object from Azure\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "# You can find tenant id under azure active directory->properties\n",
    "tenant_id = '198c7d8c-e010-45ce-a018-ec2d9a33f58f'\n",
    "ia = InteractiveLoginAuthentication(tenant_id=tenant_id)\n",
    "ws_name = 'automlbook'\n",
    "subscription_id = '4d278f3d-b4fd-4fa2-86b6-d34b96bc888f'\n",
    "resource_group = 'Foxy_Resources'\n",
    "ws = Workspace.get(name=ws_name,\n",
    "                   subscription_id=subscription_id,\n",
    "                   resource_group=resource_group,\n",
    "                   auth=ia)\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datastore, try getting datastore via Workspace object\n",
    "datastore = Datastore.get_default(ws)\n",
    "datastore_name = 'workspaceblobstore'\n",
    "datastore = Datastore.get(ws, datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the datastore of the Workspace\n",
    "dataset_name = 'automlbook Titanic Training Data A'\n",
    "# dataset = Dataset.get_by_name(ws, dataset_name)\n",
    "dataset = Dataset.get_by_name(ws, dataset_name, version = 'latest')\n",
    "dataset_columns = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "\n",
    "# Show a sample of the data in the dataset\n",
    "dataset.take(10).to_pandas_dataframe()\n",
    "\n",
    "# Turn Dataset into Pandas Dataframe, it is to be preprocessed\n",
    "df = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (First way I did this, commented out) Preprocess numeric columns\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# df_column_names = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "# df_numeric_column_names = ['Age', 'Fare']\n",
    "\n",
    "# # BEGIN Create dataframe with numeric columns so that it contains all numbers that are preprocessed...\n",
    "\n",
    "# # For int column Age, Impute NaN numeric values, and Remove outliers\n",
    "# print('Before Removing outliers or Imputing null values, df[Age]: ', df['Age'])\n",
    "# ageMedian = np.nanmedian(df['Age'])\n",
    "# print('ageMedian: ', ageMedian)\n",
    "# df['Age'] = np.where(np.isnan(df['Age']), ageMedian, df['Age'])\n",
    "# print('Before Removing outliers and after Imputing null values, df[Age]: ', df['Age'])\n",
    "\n",
    "# # Calculate 3STD and Mean for Age\n",
    "# ageThreeSD = np.std(df['Age']) * 3\n",
    "# ageMean = np.mean(df['Age'])\n",
    "# ageOutlierThreshold = round(ageThreeSD + ageMean)\n",
    "# print('Age Outlier Threshold: ', ageOutlierThreshold)\n",
    "\n",
    "# # Remove Outliers by replacing all values above Threshold (3STD + Mean) with Threshold Value\n",
    "# df['Age'] = df['Age'].mask(df['Age'] > ageOutlierThreshold, ageOutlierThreshold)\n",
    "# print('After Removing outliers and Imputing null values, df[Age]: ', df['Age'])\n",
    "\n",
    "# # Copy df, keeping only Age column, set type of this df copy to float\n",
    "# df_age_column = pd.DataFrame(df['Age'], columns=['Age'])\n",
    "\n",
    "\n",
    "# # Copy df, keeping only float numeric columns, set type of this df copy to float\n",
    "# df_float_column_names = ['Fare']\n",
    "# print('df_float_column_names: ', df_float_column_names)\n",
    "# df_float_columns = pd.DataFrame(df[df_float_column_names], dtype=np.float, columns=df_float_column_names)\n",
    "\n",
    "# # Concatenate the numeric Data Frames to scale them\n",
    "# print('Before concatenation to df_numeric_columns, df[Age]: ', df['Age'])\n",
    "# print('Before concatenation to df_numeric_columns, df_age_column: ', df_age_column)\n",
    "# df_numeric_columns = pd.concat([df_age_column, df_float_columns], keys=df_numeric_column_names, axis=1)\n",
    "# print('concatenated df_numeric_columns: ', df_numeric_columns)\n",
    "\n",
    "\n",
    "# # Use StandardScaler or MinMaxScaler on Numeric/Non-Categorical columns split\n",
    "# scaler = StandardScaler().fit(df_numeric_columns)\n",
    "# print('scaler.mean_: ', scaler.mean_)\n",
    "# print('scaler.scale: ', scaler.scale_)\n",
    "\n",
    "# df_scaled_numeric_columns =  pd.DataFrame(scaler.transform(df_numeric_columns), columns=df_numeric_column_names)\n",
    "# print('df_scaled_numeric_columns: ', df_scaled_numeric_columns)\n",
    "# # Scaled data should have zero mean and unit variance, check with these prints:\n",
    "# print('df_scaled_numeric_columns.mean(axis=0): ', df_scaled_numeric_columns.mean(axis=0))\n",
    "# print('df_scaled_numeric_columns.std(axis=0)', df_scaled_numeric_columns.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (First way I did this, commented out) Preprocess categorical columns\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # BEGIN Create dataframe with categorical columns so that it contains all categorical data that is preprocessed...\n",
    "\n",
    "# # Copy df, keeping only categorical columns, and one-hot encode them\n",
    "# df_categorical_column_names_raw = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin', 'Embarked']\n",
    "# # print('df_categorical_column_names_raw: ', df_categorical_column_names_raw)\n",
    "# df_categorical_columns = pd.DataFrame(df[df_categorical_column_names_raw], dtype=np.str, columns=df_categorical_column_names_raw)\n",
    "# # print('df_categorical_columns: ', df_categorical_columns)\n",
    "# encoder = OneHotEncoder(drop='first', handle_unknown='error', sparse=False, ).fit(df_categorical_columns)\n",
    "# # print('encoder.categories: ', encoder.categories)\n",
    "# df_encoded_categorical_columns = pd.DataFrame(encoder.transform(df_categorical_columns))\n",
    "# df_encoded_categorical_columns.columns = encoder.get_feature_names(df_categorical_column_names_raw)\n",
    "# # print('df_encoded_categorical_columns: ', df_encoded_categorical_columns)\n",
    "# #   By default, the values each feature can take is inferred automatically from the dataset and can be found in the categories_ attribute:\n",
    "\n",
    "\n",
    "# # Combine the numeric DF with the categorical DF\n",
    "# dfs = [df['Survived'], df_scaled_numeric_columns, df_encoded_categorical_columns]\n",
    "# # print('Before concatenation to dfTyped, df[\\'Survived\\']: ', df['Survived'])\n",
    "# # print('Before concatenation to dfTyped, df[Age]: ', df['Age'])\n",
    "# # print('Before concatenation to dfTyped, df_numeric_columns: ', df_numeric_columns)\n",
    "# # print('Before concatenation to dfTyped, df_scaled_numeric_columns: ', df_scaled_numeric_columns)\n",
    "# ??? shouldn't column names be passed in to the keys list here?\n",
    "# dfTyped = pd.concat(dfs, axis=1, keys=['Survived', *df_scaled_numeric_columns, *df_encoded_categorical_columns])\n",
    "# # print('dfTyped: ', dfTyped)\n",
    "# # print('dfTyped[Age]: ', dfTyped['Age'])\n",
    "\n",
    "#         # - With sklearn.preprocessing, preprocess your Dataframes before training model in the Python Script\n",
    "#         #     - [Guide at SciKit Learn site](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "#         #     - Use OneHotEncoder\n",
    "#         #     - Use StandardScaler or  MinMaxScaler while you're at it\n",
    "#         #     - Don't worry about any other preprocessing to just get the training working\n",
    "#         #     - Strategy:\n",
    "#         #         - d Split dataframe into Numeric/Non-Categorial and Non-Numeric/Categorial columns\n",
    "#         #             - ! Use StandardScaler or MinMaxScaler on Numeric/Non-Categorical columns split\n",
    "#         #             - d Use OneHotEncoder on Non-Numeric/Categorical columns split\n",
    "\n",
    "# # Initial Data Frame is now preprocessed in dfPreprocessed\n",
    "# dfPreprocessed = dfTyped\n",
    "# # print('dfPreprocessed: ', dfPreprocessed)\n",
    "\n",
    "# # Split DataFrame for training now that it is pre-processed\n",
    "# target_column_name = 'Survived'\n",
    "# df_x = dfPreprocessed.drop([target_column_name], axis=1)\n",
    "# df_y = dfPreprocessed[target_column_name]\n",
    "# # print(\"See df_x\", df_x)\n",
    "# print(\"See df_y\", df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Dataframe to get one for Numeric columns and one for Categorical columns\n",
    "\n",
    "df_column_names = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
    "\n",
    "##### BEGIN Create dataframe with numeric columns so that it contains all numbers that are preprocessed...\n",
    "df_numeric_column_names = ['Age', 'Fare']\n",
    "# Copy df, keeping only float numeric columns, set type of this df copy to float\n",
    "df_float_column_names = ['Fare']\n",
    "df_float_columns = pd.DataFrame(df[df_float_column_names], dtype=np.float, columns=df_float_column_names)\n",
    "# # Copy df, keeping only integer Age column to leave as an integer\n",
    "df_integer_column = pd.DataFrame(df['Age'], columns=['Age'])\n",
    "# Concatenate the numeric DataFrames\n",
    "df_numeric_columns = pd.concat([df_integer_column, df_float_columns], axis=1)\n",
    "\n",
    "\n",
    "##### BEGIN Create dataframe with categorical columns so that it contains all categorical data that is preprocessed...\n",
    "df_categorical_column_names = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin', 'Embarked']\n",
    "# Copy df, keeping only categorical columns\n",
    "df_categorical_columns = pd.DataFrame(df[df_categorical_column_names], dtype=np.str, columns=df_categorical_column_names)\n",
    "\n",
    "print('concatenated df_numeric_columns: ', df_numeric_columns)\n",
    "print('df_categorical_columns: ', df_categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor to preprocess numeric and categorical columns (with Transfomer API via ColumnTransformer, including creation of an Explainer)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', one_hot_encoder)])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, df_numeric_column_names),\n",
    "        ('cat', categorical_transformer, df_categorical_column_names)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline (Then we have a full prediction pipeline)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "regressor = DecisionTreeClassifier()\n",
    "\n",
    "classifier_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', regressor)])\n",
    "# Now we have a full prediction pipeline.\n",
    "print(classifier_pipeline.__dict__)\n",
    "# Note that you may access classifier_pipeline ColumnTransformers as follows...\n",
    "# print(classifier_pipeline['preprocessor'].transformers[1][1][1].get_feature_names(df_categorical_column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the SciKit Learn Pipeline to pass it to the script\n",
    "# Link about pickling:\n",
    "# https://codefather.tech/blog/python-pickle/#:~:text=%20Python%20Pickle%3A%20Serialize%20Your%20Objects%20%20,The%20pickle%20module%20also%20allows%20to...%20More%20\n",
    "import pickle\n",
    "\n",
    "pickled_pipeline = pickle.dumps(classifier_pipeline)\n",
    "# print(pickle.loads(pickled_pipeline))\n",
    "\n",
    "with open('./scripts/resources/classifier_pipeline.pickle', 'wb') as file:\n",
    "    pickle.dump(classifier_pipeline, file)\n",
    "\n",
    "# You may copy this to the script and Unpickle the SciKit Pipline (that performs Transformation and Model Training)\n",
    "# with open('classifier_pipeline.pickle', 'rb') as file:\n",
    "#     unpickled_pipeline = pickle.load(file)\n",
    "#     print(unpickled_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Does nothing at the moment) Serialize the SciKit Learn Pipeline to pass it to the script (with simple_python_to_json)\n",
    "# I Quit this to use pickling just to get this working and avoid maintaining simple_python_to_json for now...\n",
    "#       lesson learned, \n",
    "#       ideally the code would serialize/deserialize every single Sklearn object type...\n",
    "# TODO? maybe add to simple_python_to_json get this cell working?\n",
    "# You may use code within to help you serialize\n",
    "#      (Contain the code behind the 'simple_python_to_json' reference )\n",
    "# https://robotfantastic.org/serializing-python-data-to-json-some-edge-cases.html\n",
    "# https://cmry.github.io/notes/serialize\n",
    "# import json\n",
    "# import simple_python_to_json\n",
    "# def serialize_dict(in_dictionary, name):\n",
    "#     print(classifier_pipeline.__dict__)\n",
    "#     for k, v in in_dictionary.items():\n",
    "#         print(\"k,v is: \", k, \", \", v)\n",
    "#         print(simple_python_to_json.serializer.json_to_data(simple_python_to_json.serializer.data_to_json(v)))\n",
    "#         in_dictionary[k] = simple_python_to_json.data_to_json(v)\n",
    "#     json.dump(in_dictionary, open(name + '.json', 'w'))\n",
    "\n",
    "# TODO? maybe finish changes to serializer.py to serialize this\n",
    "# serialize_dict(classifier_pipeline.__dict__, 'classifier_pipeline')\n",
    "\n",
    "# TODO? maybe copy below to script, make todo to get it working in script\n",
    "# Load the json and deserialize:\n",
    "\n",
    "# def deserialize(class_init, attr):\n",
    "#     for k, v in attr.items():\n",
    "#         setattr(class_init, k, sr.json_to_data(v))\n",
    "#     return class_init\n",
    "\n",
    "# Load and deserialize the json:\n",
    "# deserialized_classifier_pipeline = deserialize(Pipeline(), json.load(open('classifier_pipeline.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the numeric DF with the categorical DF\n",
    "# print(\"df['Survived'] is \", df['Survived'])\n",
    "# print(\"df_numeric_columns is \", df_numeric_columns)\n",
    "# print(\"df_numeric_columns.columns is \", df_numeric_columns.columns)\n",
    "# print(\"df_categorical_columns is \", df_categorical_columns)\n",
    "# print(\"df_categorical_columns.columns is \", df_categorical_columns.columns)\n",
    "\n",
    "# Concatenate dfs to get DataFrame of all columns to submit to the classifier_pipeline\n",
    "dfs = [df['Survived'], df_numeric_columns, df_categorical_columns]\n",
    "# print(\"dfs is\" + str(dfs))\n",
    "# print('Before concatenation to dfTyped, df[\\'Survived\\']: ', df['Survived'])\n",
    "# print('Before concatenation to dfTyped, df_numeric_columns: ', df_numeric_columns)\n",
    "dfTyped = pd.concat(dfs, axis=1)\n",
    "print('dfTyped: ', dfTyped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pre-transformation Data Frame into feature/target columns\n",
    "target_column_name = 'Survived'\n",
    "df_x = dfTyped.drop([target_column_name], axis=1)\n",
    "df_y = dfTyped[target_column_name].to_frame()\n",
    "# print(\"See df_x\", df_x)\n",
    "print(\"See df_y\", df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Validate the Pipeline will function once passed to the script)\n",
    "\n",
    "# Get the preprocessed Data Frame columns in a list\n",
    "one_hot_encoder.fit(df_categorical_columns)\n",
    "# Get new One Hot Encoded column names\n",
    "# df_encoded_categorical_column_names = one_hot_encoder.get_feature_names(df_categorical_column_names)\n",
    "# print(str(df_encoded_categorical_column_names))\n",
    "\n",
    "# (fail to) Get the preprocessed Data Frame\n",
    "# # dfPreprocessed = preprocessor.fit_transform(df_x_pre_transformation, df_y_pre_transformation)\n",
    "\n",
    "# Attempt to fit the preprocessor to the data, to validate it will function inside the script it is passed to\n",
    "preprocessor.fit(df_x, df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Pandas Dataframe of base df_x and df_y\n",
    "Dataset.Tabular.register_pandas_dataframe(df_x, datastore, \"Titanic Feature Column Data for train_test_split usage (LocalConda notebook)\")\n",
    "Dataset.Tabular.register_pandas_dataframe(df_y, datastore, \"Titanic Target Column Data for train_test_split usage (LocalConda notebook)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test data, register the resulting Datasets with Azure\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "# What you need to pass to train_test_split...\n",
    "# ... I need X and Y dataframe, X just with target missing, Y just with target column present\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "# print(\"See y_test\", y_test)\n",
    "print(\"See y_test.columns.tolist()\", str(y_test.columns.tolist()))\n",
    "# print(\"See y_test.values.tolist() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.tolist())\n",
    "# print(\"See y_test.values.ravel() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.ravel())\n",
    "# print(\"See y_test.values.tolist().flatten() to pass as true_ys to ExplanationClient.upload_model_explanation()\", y_test.values.flatten())\n",
    "\n",
    "# Register the splits\n",
    "Dataset.Tabular.register_pandas_dataframe(X_train, datastore, \"Titanic Feature Column Data for training (LocalConda notebook)\")\n",
    "Dataset.Tabular.register_pandas_dataframe(X_test, datastore, \"Titanic Feature Column Data for testing (LocalConda notebook)\")\n",
    "Dataset.Tabular.register_pandas_dataframe(y_train, datastore, \"Titanic Target Column Data for training (LocalConda notebook)\")\n",
    "Dataset.Tabular.register_pandas_dataframe(y_test, datastore, \"Titanic Target Column Data for testing (LocalConda notebook)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names and create TabularExplainer with them\n",
    "features=[*df_numeric_column_names, *df_categorical_column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode feature names list into a string like '[\"a\",\"b\"]'\n",
    "for x in range(len(df_numeric_column_names)):\n",
    "        df_numeric_column_names[x] = '\"{}\"'.format(df_numeric_column_names[x])\n",
    "numericFeatureNamesEncoded = \"[{}]\".format(\",\".join(df_numeric_column_names))\n",
    "print(\"numericFeatureNamesEncoded:\", numericFeatureNamesEncoded)\n",
    "\n",
    "for x in range(len(df_categorical_column_names)):\n",
    "        df_categorical_column_names[x] = '\"{}\"'.format(df_categorical_column_names[x])\n",
    "categoricFeatureNamesEncoded = \"[{}]\".format(\",\".join(df_categorical_column_names))\n",
    "print(\"categoricFeatureNamesEncoded:\", categoricFeatureNamesEncoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training Experiment locally\n",
    "# To use classifier_pipeline, there is code use it in the python script\n",
    "from azureml.core import ScriptRunConfig\n",
    "import datetime\n",
    "\n",
    "# Experiment\n",
    "experiment_name = 'LocalConda_Training_AutoML'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "# Define Compute Cluster to use\n",
    "compute_target = 'local'\n",
    "source_directory = './scripts'\n",
    "script_name = 'localCondaTrainingAutoML.py'\n",
    "dataset_name = 'automlbook Titanic Training Data A'\n",
    "# set output file name like 'DecisionTreeClassifier_Titanic_LocalConda-2022-04-17 21:40:36.114550.pkl'\n",
    "suffix = 'local-' + str(datetime.datetime.now())\n",
    "suffix = suffix.replace(' ', '_') # Clean up datetimestamp\n",
    "suffix = suffix.replace(':', '-') \n",
    "out_model_file_name = 'DecisionTreeClassifier_Titanic_LocalConda_{}.pkl'.format(suffix)\n",
    "# set output file name like 'DecisionTreeClassifier_Titanic_LocalConda-2022-04-17 21:40:36.114550.pkl'\n",
    "\n",
    "script_arguments = [\n",
    "\"--tenant-id\", tenant_id,\n",
    "\"--ws-name\", ws_name,\n",
    "\"--subscription-id\", subscription_id,\n",
    "\"--resource-group\", resource_group,\n",
    "\"--datastore-name\", datastore_name,\n",
    "\"--out-model-file-name\", out_model_file_name,\n",
    "\"--numeric-feature-names\", numericFeatureNamesEncoded,\n",
    "\"--categoric-feature-names\", categoricFeatureNamesEncoded\n",
    "]\n",
    "print(\"ScriptRunConfig arguments: \", script_arguments)\n",
    "scriptRunConfig = ScriptRunConfig(\n",
    "        source_directory=source_directory,\n",
    "        script=script_name,\n",
    "        arguments=script_arguments,\n",
    "        environment=system_managed_env,\n",
    "        compute_target=compute_target)\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] = \"/home/johna/miniconda3/bin:\" + os.environ[\"PATH\"]\n",
    "AutoML_run = experiment.submit(scriptRunConfig)\n",
    "RunDetails(AutoML_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Model from the AutoML_run\n",
    "description = \"Best LocalConda AutoML Regression Run using Titanic Sample Data.\"\n",
    "tags = {\n",
    "  \"project\" : \"Local Training AutoML\", \n",
    "  \"creator\": \"fox\", \n",
    "  \"task\": \"classification\", \n",
    "  \"dataset\": \"automlbook Titanic Training Data A\", \n",
    "  \"metric\": \"normalized_root_mean_squared_error\"\n",
    "}\n",
    "\n",
    "# Attempt to register model once output model file is available\n",
    "from azureml.core import Model\n",
    "import sklearn\n",
    "import time\n",
    "while True:\n",
    "  try:\n",
    "    AutoML_run.register_model(model_path='./outputs', model_name=out_model_file_name, description=description, tags=tags,\n",
    "                            model_framework=Model.Framework.SCIKITLEARN, # Framework used to create the model.\n",
    "                            model_framework_version=sklearn.__version__)  # Version of scikit-learn used to create the model.)\n",
    "    break\n",
    "  except:\n",
    "      print (\"encountered exception registering model output file, waiting and trying again...\") \n",
    "      time.sleep(60)\n",
    "# Set output file name like 'DecisionTreeClassifier_Titanic_LocalConda-2022-04-17 21:40:36.114550.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Now done in script instead of this Notebook) Upload global model explanation data...\n",
    "# from azureml.interpret import ExplanationClient\n",
    "\n",
    "# The explanation can then be downloaded on any compute\n",
    "# Multiple explanations can be uploaded\n",
    "# print(\"y_test value the line before client.upload_model_explanation(): \\n\" + str(y_test))\n",
    "# print(\"y_test.values.ravel() value passed as true_ys to client.upload_model_explanation(): \\n\" + str(y_test.values.ravel()))\n",
    "# client = ExplanationClient.from_run(AutoML_run)\n",
    "# client.upload_model_explanation(global_explanation, true_ys=y_test.values.ravel(), comment='global explanation: all features')\n",
    "\n",
    "# Or you can only upload the explanation object with the top k feature info with this...\n",
    "# client.upload_model_explanation(global_explanation, top_k=2, comment='global explanation: Only top 2 features')\n",
    "# END Upload global model explanation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Experiment Run that was ran, Get Global Explanations (Downloaded with the Experiment Run object!)\n",
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run(AutoML_run)\n",
    "\n",
    "# get model explanation data\n",
    "global_explanation = client.download_model_explanation(raw=False)\n",
    "# TODO fix error here, it seems the engineered explanation cannot be found yet\n",
    "engineered_global_explanation = client.download_model_explanation(raw=False)\n",
    "# or only get the top k (e.g., 4) most important features with their importance values\n",
    "# explanation = client.download_model_explanation(top_k=4)\n",
    "\n",
    "global_importance_values = global_explanation.get_ranked_global_values()\n",
    "global_importance_names = global_explanation.get_ranked_global_names()\n",
    "print('global importance values: {}'.format(global_importance_values))\n",
    "print('global importance names: {}'.format(global_importance_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Model from AzureML and Visualize Explanations with it\n",
    "from raiwidgets import ExplanationDashboard\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "import jinja2\n",
    "\n",
    "# print('Model.list(ws)', Model.list(ws))\n",
    "\n",
    "# Download the Model from Azure\n",
    "\n",
    "# Try 1: just Model constructor and joblib.load()\n",
    "# downloaded_model = Model(ws, out_model_file_name)\n",
    "# joblib.load(downloaded_model)\n",
    "\n",
    "# Try 2: Use Model.get_model_path and joblib.load()\n",
    "# remote_model_path = Model.get_model_path(out_model_file_name, _workspace=ws)\n",
    "# downloaded_model = joblib.load(remote_model_path)\n",
    "\n",
    "# Try 3: Use Model.download and joblib.load()\n",
    "remote_model_obj = Model(ws, out_model_file_name)\n",
    "print('Name:', remote_model_obj.name)\n",
    "print('Version:', remote_model_obj.version)\n",
    "remote_model_path = remote_model_obj.download(exist_ok = True)\n",
    "downloaded_model = joblib.load(remote_model_path)\n",
    "\n",
    "# BEGIN Access \"Local Explanations\", uncomment these lines if you want to do that here...\n",
    "# (Local Explanation meaning \"of individual predictions\") \n",
    "# from interpret.ext.blackbox import TabularExplainer\n",
    "# \"features\" and \"classes\" fields are optional\n",
    "# explainer = TabularExplainer(downloaded_model,\n",
    "                            #  X_train,\n",
    "                            #  features=features)\n",
    "#\n",
    "# Get explanation for the first few data points in the test set\n",
    "# local_explanation = explainer.explain_local(X_test[0:5])\n",
    "# Sorted feature importance values and feature names\n",
    "# sorted_local_importance_names = local_explanation.get_ranked_local_names()\n",
    "# print('sorted_local_importance_names: ', sorted_local_importance_names)\n",
    "# print('len(sorted_local_importance_names): ', len(sorted_local_importance_names))\n",
    "# sorted_local_importance_values = local_explanation.get_ranked_local_values()\n",
    "# print('sorted_local_importance_values: ', sorted_local_importance_values)\n",
    "# print('len(sorted_local_importance_values): ', len(sorted_local_importance_values))\n",
    "# COOL THING TO DO: Sometime could get local explanation of specific data points uploaded, downloaded, and visualized as well...\n",
    "# END Access \"Local Explanations\"\n",
    "\n",
    "# Visualize explanations\n",
    "# Be sure to pass dataset=(test feature columns Dataframe) and true_y=(test predicted column Dataframe)\n",
    "# TODO get this working again, and getting base statistics, consider...\n",
    "#       1) getting the raiwidgets thing working\n",
    "#       2) see README at https://github.com/interpretml/interpret\n",
    "ExplanationDashboard(global_explanation, downloaded_model, dataset=X_test, true_y=y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46ae9456b2737841401e3d3c7acdf327031e52ebe64544d23b06b17169cc2049"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit ('3.7.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
